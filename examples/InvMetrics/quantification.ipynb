{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明：这个notebook演示了如何使用quantification方法（目前实现了4种方法进行隐私量化）\n",
    "1. dFIL (batchsize = 1)\n",
    "2. distance correlation (batchsize>=2)\n",
    "3. mutual information (batchsize>=8)\n",
    "4. ULoss (batchsize = 1)\n",
    "\n",
    "注意用不同方法的时候要重新设置 批大小 （即args['batch_size']的值）\n",
    "\n",
    "因为在整个测试集上进行隐私量化，时间太长了（可能要跑好几天）所以这里设计了一个get_one_data()函数，取测试集的前k个数据作为一个数据集，batch_size=k,因此只需要迭代一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '48'\n",
    "\n",
    "\n",
    "# 导入各个指标\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.quantification.distance_correlation.distCor import distCorMetric\n",
    "from ppsplit.quantification.fisher_information.dFIL_inverse import dFILInverseMetric\n",
    "from ppsplit.quantification.shannon_information.mutual_information import MuInfoMetric\n",
    "from ppsplit.quantification.shannon_information.ULoss import ULossMetric\n",
    "from ppsplit.quantification.rep_reading.rep_reader import PCA_Reader\n",
    "\n",
    "\n",
    "# 模型、数据集获取\n",
    "from target_model.task_select import get_dataloader_and_model,get_dataloader_and_model, get_dataloader,get_models\n",
    "\n",
    "# utils\n",
    "from ppsplit.utils.utils import create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# 基本参数：\n",
    "# 硬件\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 参数\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', type = str, default = 'CIFAR10')\n",
    "# parser.add_argument('--device', type = str, default = 'cuda:1')\n",
    "# parser.add_argument('--batch_size',type=int, default=1) # muinfo最小为8，# distcor最小为2\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args = {\n",
    "#         'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "#         # 'device':torch.device(\"cpu\"),\n",
    "#         'dataset':'CIFAR10',\n",
    "#         # 'dataset':'bank',\n",
    "#         # 'dataset':'credit',\n",
    "#         # 'dataset':'purchase',\n",
    "#         # 'result_dir': 'InvMetric-202403',\n",
    "#         'result_dir': '20240428-Rep-quantify/',\n",
    "#         'batch_size':2,\n",
    "#         'noise_scale':0, # 防护措施\n",
    "#         'num_pairs': 10000, # RepE\n",
    "#         }\n",
    "# print(args['device'])\n",
    "\n",
    "# 超参数\n",
    "args = {\n",
    "        'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        # 'device':torch.device(\"cpu\"),\n",
    "        # 'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        'dataset':'Iris',\n",
    "        # 'result_dir': 'inverse-model-results-20240414/',\n",
    "        # 'result_dir': 'inverse-model-results-20240414/',\n",
    "        'result_dir': 'InvMetric-202403/',\n",
    "        'oneData_bs': 30,\n",
    "        'test_bs': 1,\n",
    "        'train_bs':1,\n",
    "        'noise_scale':0, # 防护措施\n",
    "        'split_layer': 1,\n",
    "        # 'num_pairs': 10000, # RepE # 这个要另外准备\n",
    "        }\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集及其模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n",
      "X_train.shape: (120, 4)\n",
      "X_test.shape: (30, 4)\n",
      "y_train.shape: (120,)\n",
      "y_test.shape: (30,) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "train decoder model...\n",
      "['Tanh', ('D', 4, 128)]\n",
      "results_dir: ../../results/InvMetric-202403//Iris/1/\n",
      "inverse_dir: ../../results/InvMetric-202403//Iris/1/layer1/\n",
      "decoder_route: ../../results/InvMetric-202403//Iris/1//Decoder-layer1.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IrisNet(\n",
       "  (linear1): Linear(in_features=4, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_msg = get_dataloader(args)\n",
    "model_msg = get_models(args)\n",
    "msg = {**model_msg,**data_msg}\n",
    "\n",
    "# 数据集\n",
    "one_data_loader,trainloader,testloader = data_msg['one_data_loader'],data_msg['trainloader'], data_msg['testloader']\n",
    "\n",
    "# 模型和路径\n",
    "client_net,decoder_net = model_msg['client_net'],model_msg['decoder_net']\n",
    "decoder_route = model_msg['decoder_route']\n",
    "image_deprocess = model_msg['image_deprocess']\n",
    "\n",
    "results_dir = model_msg['results_dir']\n",
    "inverse_dir = results_dir + 'layer'+str(args['split_layer'])+'/'\n",
    "data_type = 1 if args['dataset'] == 'CIFAR10' else 0\n",
    "\n",
    "print('results_dir:',results_dir)\n",
    "print('inverse_dir:',inverse_dir)\n",
    "print('decoder_route:',decoder_route)\n",
    "\n",
    "create_dir(results_dir)\n",
    "\n",
    "# client_net使用\n",
    "client_net = client_net.to(args['device'])\n",
    "client_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各种指标计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.dFIL-inverse\n",
    "注意：batchsize 需要等于1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [5, 5, 3, 32, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# 需要求导\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# inference\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mclient_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m eta \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mquantify(model\u001b[38;5;241m=\u001b[39mclient_net, inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs, with_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 打印\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(str(j)+\": \"+str(eta.item()))\u001b[39;00m\n",
      "File \u001b[0;32m/data/other/anaconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/dengruijun/FinTech/PP-Split/target_model/models/VGG.py:111\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    110\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 111\u001b[0m \t\tout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \t\u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m \t\tout \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m/data/other/anaconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/other/anaconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/data/other/anaconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/other/anaconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/other/anaconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [5, 5, 3, 32, 32]"
     ]
    }
   ],
   "source": [
    "# dFIL inverse指标计算\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric = dFILInverseMetric()\n",
    "# 对traingloader遍历计算所有 inverse dFIL\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)):\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    inputs.requires_grad_(True) # 需要求导\n",
    "    \n",
    "    # inference\n",
    "    outputs = client_net(inputs)\n",
    "\n",
    "    eta = metric.quantify(model=client_net, inputs=inputs, outputs=outputs, with_outputs=True)\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(save_img_dir + f'dFIL-1.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'dFIL.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. distance correlation\n",
    "注意：batchsize >=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 Avg distCorr: 0.9813618063926697\n",
      "../../results/InvMetric-202403//Iris/1/DLoss.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# distance correlation指标计算\n",
    "\n",
    "distCorr_diff_layer_list = []\n",
    "distCorr_same_layer_list = []\n",
    "metric = distCorMetric()\n",
    "\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    tab, labels = data\n",
    "    tab, labels = tab.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        pred = client_net(tab).cpu().detach()\n",
    "        inputs = tab.cpu().detach()\n",
    "\n",
    "        distCorr = metric.quantify(inputs=inputs,outputs=pred) # x,z\n",
    "        distCorr_same_layer_list.append(distCorr)\n",
    "\n",
    "\n",
    "print(f\"Layer {args['split_layer']} Avg distCorr: {sum(distCorr_same_layer_list)/len(distCorr_same_layer_list)}\")\n",
    "distCorr_diff_layer_list.append(distCorr_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(distCorr_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in range (len(split_layer_list))]).to_csv(save_img_dir + f'DLoss-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[args['split_layer']]).to_csv(results_dir + f'DLoss.csv',index=False)\n",
    "print(results_dir + f'DLoss.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. mutual information\n",
    "注意：batchsize>=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2 MI: -13.534394797710462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# mutual information指标计算\n",
    "\n",
    "MI_diff_layer_list = []\n",
    "MI_same_layer_list = []\n",
    "metric = MuInfoMetric()\n",
    "\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        inputs = images.cpu().detach()\n",
    "        mi = metric.quantify(inputs=inputs, outputs = outputs)\n",
    "        MI_same_layer_list.append(mi)\n",
    "        \n",
    "print(f\"Layer {args['split_layer']} MI: {sum(MI_same_layer_list)/len(MI_same_layer_list)}\")\n",
    "MI_diff_layer_list.append(MI_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(MI_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(results_dir + f'MI-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[args['split_layer']]).to_csv(results_dir + f'MILoss.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Uncertainty Loss\n",
    "注意：batchsize=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3 ULoss: 4.021282423374509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# mutual information指标计算\n",
    "\n",
    "ULoss_diff_layer_list = []\n",
    "ULoss_same_layer_list = []\n",
    "metric = ULossMetric()\n",
    "decoder_net = torch.load(decoder_route)\n",
    "decoder_net.to(args['device'])\n",
    "decoder_net.eval()\n",
    "\n",
    "\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        uloss = metric.quantify(output = outputs, decoder_net=decoder_net)\n",
    "        ULoss_same_layer_list.append(uloss)\n",
    "        \n",
    "print(f\"Layer {split_layer} ULoss: {sum(ULoss_same_layer_list)/len(ULoss_same_layer_list)}\")\n",
    "ULoss_diff_layer_list.append(ULoss_same_layer_list)\n",
    "\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(ULoss_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(results_dir + f'ULoss-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'ULoss.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RepE Reader\n",
    "无所谓bs\n",
    "\n",
    "1 格式化检查\n",
    "\n",
    "2 实例化一个finder\n",
    "\n",
    "3 模型推理数据得到hidden state（中途有些处理）\n",
    "\n",
    "4 训练pca得到direction\n",
    "\n",
    "5 转换，nparray转换成浮点数\n",
    "\n",
    "6 如果有train label，就get sign一下\n",
    "\n",
    "评估维度性，对label的贡献（偏泄漏隐私，还是偏不泄漏隐私）\n",
    "\n",
    "你directionality 反应的你对于泄漏方向的偏移，和真实的标签？   \n",
    "\n",
    "7 分析测试数据的隐私泄漏程度（在每一层的隐私泄漏程度）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "from target_model.data_preprocessing.dataset import pair_smashed_data,diff_pair_data\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize_two_train\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading paired dataset from ../results/20240428-Rep-quantify//VGG5/quantification/200pairs/\n",
      "[False, True]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 1. designing stimulus and test\n",
    "dataset_route = f\"../results/{args['result_dir']}/VGG5/quantification/{args['num_pairs']}pairs/\"\n",
    "if os.path.isfile(dataset_route+'train_feature.pkl'): # 直接加载预处理好的数据集\n",
    "    print(f\"=> loading paired dataset from {dataset_route}\")\n",
    "    with open(dataset_route+'train_feature.pkl','rb') as f:\n",
    "        train_feature = pickle.load(file=f)       \n",
    "    with open(dataset_route+'test_feature.pkl','rb') as f:\n",
    "        test_feature=pickle.load(file=f)   \n",
    "    with open(dataset_route+'train_label.pkl','rb') as f:                                                       \n",
    "        train_labels=pickle.load(file=f)      \n",
    "    with open(dataset_route+'test_label.pkl', 'rb') as f:                                                    \n",
    "        test_labels=pickle.load(file=f)     \n",
    "    train_loader= DataLoader(train_feature,shuffle=False,batch_size=1)\n",
    "    test_loader = DataLoader(test_feature,shuffle=False,batch_size=1)       \n",
    "# if False:\n",
    "#     pass\n",
    "else: # 进行预处理并存储\n",
    "    seen_loader,unseen_loader,_ = get_cifar10_normalize_two_train(batch_size=1)\n",
    "\n",
    "    train_loader,train_labels,test_loader,test_labels = pair_smashed_data(seen_loader,\n",
    "                                                                        unseen_loader,\n",
    "                                                                        num_pairs=args['num_pairs'])\n",
    "    create_dir(dataset_route)\n",
    "    with open(dataset_route+'train_feature.pkl','wb') as f:\n",
    "        pickle.dump(obj=train_loader.dataset,file=f)       \n",
    "    with open(dataset_route+'test_feature.pkl','wb') as f:\n",
    "        pickle.dump(obj=test_loader.dataset,file=f)   \n",
    "    with open(dataset_route+'train_label.pkl','wb') as f:                                                       \n",
    "        pickle.dump(obj=train_labels,file=f)      \n",
    "    with open(dataset_route+'test_label.pkl', 'wb') as f:                                                    \n",
    "        pickle.dump(obj=test_labels,file=f)                                                          \n",
    "\n",
    "print(train_labels[0])\n",
    "print(test_labels[0].index(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 1024.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9997, 0.9995, 0.9995], device='cuda:1')\n",
      "diff_data.shape:  torch.Size([200, 3])\n"
     ]
    }
   ],
   "source": [
    "# 2. collecting neural activity\n",
    "\n",
    "# #Picking the top X probabilities \n",
    "def clipDataTopX(dataToClip, top=3):\n",
    "    sorted_indices = torch.argsort(dataToClip,dim=1,descending=True)[:,:3]\n",
    "    new_data = torch.gather(dataToClip,1,sorted_indices)\n",
    "    \n",
    "\t# res = [sorted(s, reverse=True)[0:top] for s in dataToClip ]\n",
    "\t# return np.array(res)\n",
    "    # print(new_data[0])\n",
    "    return new_data\n",
    "\n",
    "# 收集所有smashed data\n",
    "train_smashed_data_list = []\n",
    "i = 1\n",
    "for j, data in enumerate(tqdm.tqdm(train_loader)): # 对trainloader遍历\n",
    "    # print(\"data: \", len(data))\n",
    "    features=data.to(args['device'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = client_net(features)\n",
    "        # pred_topk = sorted(pred, reverse=True)[0:5]\n",
    "        # train_smashed_data_list.append(pred)\n",
    "        train_smashed_data_list.append(pred)\n",
    "\n",
    "train_smashed_data_list=torch.stack(train_smashed_data_list).squeeze()\n",
    "# 拉成 [batchsize, vectorsize]的二维矩阵\n",
    "train_smashed_data_list=train_smashed_data_list.reshape(train_smashed_data_list.shape[0],-1)\n",
    "train_smashed_data_list = clipDataTopX(train_smashed_data_list,top=10)\n",
    "# 相对距离\n",
    "diff_data = diff_pair_data(train_smashed_data_list) # np.array\n",
    "print(\"diff_data.shape: \", diff_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in recenter: \n",
      "torch.Size([200, 3])\n",
      "torch.Size([1, 3])\n",
      "in recenter: \n",
      "torch.Size([400, 3])\n",
      "torch.Size([1, 3])\n",
      "direction shape of first layer:  (1, 3)\n",
      "signs of first layer:  [1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. constructing a linear model\n",
    "# 训练direction finder\n",
    "reader = PCA_Reader(n_components=1) # 要的是numpy数据？可以要tensor数据\n",
    "# diff_data = diff_data.reshape(diff_data.shape[0],-1)\n",
    "directions = reader.get_rep_direction(diff_data)\n",
    "signs = reader.get_sign(hidden_states=train_smashed_data_list,train_labels=train_labels)\n",
    "print('direction shape of first layer: ', reader.direction.shape)\n",
    "print('signs of first layer: ', reader.direction_signs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 998.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000], device='cuda:1')\n",
      "in recenter: \n",
      "torch.Size([400, 3])\n",
      "torch.Size([1, 3])\n",
      "[tensor(-1.6736), tensor(-1.6736)]\n",
      "quantified accuracy(privacy lekage): 0.495 \n"
     ]
    }
   ],
   "source": [
    "# 4. 测试\n",
    "test_smashed_data_list = []\n",
    "for j, data in enumerate(tqdm.tqdm(test_loader)): # 对trainloader遍历\n",
    "    features=data.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        pred = client_net(features)\n",
    "        test_smashed_data_list.append(pred)\n",
    "\n",
    "test_smashed_data_list=torch.stack(test_smashed_data_list).squeeze()\n",
    "test_smashed_data_list=test_smashed_data_list.reshape(test_smashed_data_list.shape[0],-1)\n",
    "test_smashed_data_list = clipDataTopX(test_smashed_data_list,top=10)\n",
    "acc = reader.quantify_acc(hidden_states=test_smashed_data_list,test_labels=test_labels)\n",
    "print(f\"quantified accuracy(privacy lekage): {acc} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([1., 2.]), tensor([3., 4.])], [tensor([5., 6.]), tensor([7., 8.])]]\n",
      "[tensor([[1., 2.],\n",
      "        [3., 4.]]), tensor([[5., 6.],\n",
      "        [7., 8.]])]\n",
      "[tensor([1., 2.]), tensor([3., 4.]), tensor([5., 6.]), tensor([7., 8.])]\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([False, False])\n",
      "tensor([[1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = [torch.Tensor([1,2]),torch.Tensor([3,4])]\n",
    "y = [torch.Tensor([5,6]),torch.Tensor([7,8])]\n",
    "l = [x,y]\n",
    "print(l)\n",
    "l1 = [torch.stack(i) for i in l]\n",
    "print(l1)\n",
    "l2 = [item for sublist in l for item in sublist]\n",
    "print(l2)\n",
    "\n",
    "# list[tensor]转tensor\n",
    "tensor_list = [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6]), torch.tensor([7, 8, 9])]\n",
    "tensor_stack = torch.stack(tensor_list)\n",
    "print(tensor_stack)\n",
    "\n",
    "# 看两个tensor比较\n",
    "x = torch.Tensor([1,2])\n",
    "y = torch.Tensor([3,4])\n",
    "print(x==y)\n",
    "x.reshape(1,-1)\n",
    "print(x.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drj-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
