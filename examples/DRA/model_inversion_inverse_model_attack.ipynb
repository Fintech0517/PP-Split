{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个notebook 介绍了 如何对split learning 发起 inverse-model attack攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.attacks.model_inversion.inverse_model import InverseModelAttack\n",
    "from ppsplit.utils.utils import create_dir\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 防护措施\n",
    "from ppsplit.defense.noise import Noise\n",
    "\n",
    "# 模型、数据集获取\n",
    "from target_model.task_select import get_dataloader_and_model, get_dataloader,get_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "{'device': device(type='cuda', index=0), 'dataset': 'CIFAR10', 'model': 'VGG5', 'result_dir': 'inverse-model-results-20240414/', 'oneData_bs': 1, 'test_bs': 1, 'train_bs': 32, 'noise_scale': 0.1, 'split_layer': 0, 'test_num': 'InverseModelAttack-defense0.1', 'no_dense': True}\n"
     ]
    }
   ],
   "source": [
    "# 超参数\n",
    "args = {\n",
    "        'device':torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        # 'device':torch.device(\"cpu\"),\n",
    "        'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        # 'dataset':'Iris',\n",
    "        # 'model': 'ResNet18',\n",
    "        'model': 'VGG5',\n",
    "        # 'result_dir': '20240702-FIL/',\n",
    "        'result_dir': 'inverse-model-results-20240414/',\n",
    "        'oneData_bs': 1,\n",
    "        'test_bs': 1,\n",
    "        'train_bs': 32,\n",
    "        'noise_scale': 0.1, # 防护措施\n",
    "        'split_layer': 0,\n",
    "        # 'test_num': 'invdFIL', # MI, invdFIL, distCor, ULoss,  # split layer [2,3,5,7,9,11] for ResNet18\n",
    "        'test_num': 'InverseModelAttack-defense0.1',\n",
    "        'no_dense':True,\n",
    "        }\n",
    "print(args['device'])\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n",
      "train decoder model...\n",
      "results_dir: ../../results/inverse-model-results-20240414//VGG5/InverseModelAttack-defense0.1/\n",
      "inverse_dir: ../../results/inverse-model-results-20240414//VGG5/InverseModelAttack-defense0.1/layer0/\n",
      "decoder_route: ../../results/inverse-model-results-20240414//VGG5/InverseModelAttack-defense0.1//Decoder-layer0.pth\n"
     ]
    }
   ],
   "source": [
    "# 获取模型和数据集\n",
    "# msg = get_dataloader_and_model(**args)\n",
    "\n",
    "model_msg = get_models(args)\n",
    "\n",
    "# one_data_loader,trainloader,testloader = model_msg['one_data_loader'],model_msg['trainloader'], model_msg['testloader']\n",
    "client_net,decoder_net = model_msg['client_net'], model_msg['decoder_net']\n",
    "decoder_route = model_msg['decoder_route']\n",
    "image_deprocess = model_msg['image_deprocess']\n",
    "\n",
    "results_dir = model_msg['results_dir']\n",
    "inverse_dir = results_dir + 'layer'+str(args['split_layer'])+'/'\n",
    "\n",
    "data_msg = get_dataloader(args)\n",
    "data_type = data_msg['data_type']\n",
    "\n",
    "print('results_dir:',results_dir)\n",
    "print('inverse_dir:',inverse_dir)\n",
    "print('decoder_route:',decoder_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----train decoder----\n",
      "client_net: \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n",
      "decoder_net: \n",
      "VGG5Decoder(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:14<00:00, 110.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 0, train_loss: [0.02780887857079506]\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:11<00:00, 140.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 1, train_loss: [0.02572711929678917]\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:11<00:00, 140.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 2, train_loss: [0.024962371215224266]\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:10<00:00, 142.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 3, train_loss: [0.02460484765470028]\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:10<00:00, 143.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 4, train_loss: [0.02444893680512905]\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:10<00:00, 154.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 5, train_loss: [0.024400930851697922]\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:10<00:00, 152.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 6, train_loss: [0.02441561222076416]\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:08<00:00, 177.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 7, train_loss: [0.02439156547188759]\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:10<00:00, 150.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 8, train_loss: [0.024390919134020805]\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:09<00:00, 158.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 9, train_loss: [0.02439962700009346]\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:08<00:00, 180.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 10, train_loss: [0.02440073899924755]\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:09<00:00, 164.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 11, train_loss: [0.02442345581948757]\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:08<00:00, 175.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 12, train_loss: [0.024396201595664024]\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:08<00:00, 183.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 13, train_loss: [0.02442195639014244]\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:09<00:00, 166.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 14, train_loss: [0.024362508207559586]\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:08<00:00, 178.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 15, train_loss: [0.024462684988975525]\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:08<00:00, 187.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 16, train_loss: [0.024398811161518097]\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:10<00:00, 149.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 17, train_loss: [0.024348866194486618]\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:09<00:00, 161.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 18, train_loss: [0.02446390688419342]\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:09<00:00, 168.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 19, train_loss: [0.024361256510019302]\n",
      "model saved\n",
      "VGG5Decoder(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_route,data_type=data_type,inverse_dir=inverse_dir,device=args['device'])\n",
    "\n",
    "# 加载decoder模型\n",
    "if not os.path.isfile(decoder_route): # 如果没有训练decoder\n",
    "    # 训练decoder\n",
    "    args['train_bs']=32\n",
    "    args['test_bs']=32\n",
    "    msg_data = get_dataloader(args)\n",
    "    # trainloader,testloader = get_cifar10_normalize(batch_size=32)\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=msg_data['trainloader'],test_loader=msg_data['testloader'],\n",
    "                            epochs=20)\n",
    "else:\n",
    "    print(\"Load decoder model from:\",decoder_route)\n",
    "\n",
    "print(decoder_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----train decoder----\n",
      "client_net: \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n",
      "decoder_net: \n",
      "VGG5Decoder(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:39<00:00, 251.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average euc: 1.457061591860652\n",
      "average mse: 0.09125959669062868\n",
      "average ssim: 0.8414376188203693\n",
      "average time: 0.0017640231609344483 avg infer time:0.0003156588792800903\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有图片\n",
    "# trainloader,testloader = get_cifar10_normalize(batch_size=1)\n",
    "args['train_bs']=1\n",
    "args['test_bs']=1\n",
    "msg_data = get_dataloader(args)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=msg_data['trainloader'],test_loader=msg_data['testloader'],\n",
    "                  deprocess=image_deprocess,\n",
    "                  save_fake=True,\n",
    "                  tab=msg_data['tabinfo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(msg_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtestloader\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mnext()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage.shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m,image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mclient_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout.shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m,out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/data/FinTech/PP-Split/target_model/models/VGG.py:115\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    114\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 115\u001b[0m \t\tout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \t\u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m \t\tout \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/miniconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)"
     ]
    }
   ],
   "source": [
    "# 查看中间输出的形状\n",
    "image = iter(msg_data['testloader']).next()[0]\n",
    "print(\"image.shape:\",image.shape)\n",
    "out = client_net(image)\n",
    "print(\"out.shape:\",out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar10 （图像多分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize,get_one_data,deprocess\n",
    "\n",
    "from target_model.models.ImageClassification.VGG5_9 import VGG,VGG5Decoder,model_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 2 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 2 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/VGG5/BN+Tanh/VGG5-params-20ep.pth'\n",
    "# unit_net_route = '/home/dengruijun/data/project/Inverse_efficacy/results/VGG5/BN+Tanh/2-20240101/VGG5-params-19ep.pth'\n",
    "results_dir = f'../results/inverse-model-results-20240414/VGG5/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/'\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "features.5.weight\n",
      "features.5.bias\n",
      "features.5.running_mean\n",
      "features.5.running_var\n",
      "features.5.num_batches_tracked\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 准备基本模型client net\n",
    "# split_layer_list = list(range(len(model_cfg['VGG5']))) # 可能的切割点\n",
    "\n",
    "# 创建对应文件夹\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "# 把unit模型切割成client-server 的模型pair\n",
    "client_net = VGG('Client','VGG5',split_layer,model_cfg,noise_scale=noise_scale)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(model_cfg['VGG5']):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n",
    "print(client_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading decoder model '../results/inverse-model-results-20240414/VGG5/2/Decoder-layer2.pth'\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=1,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = VGG5Decoder(split_layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = get_cifar10_normalize(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----train decoder----\n",
      "client_net: \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n",
      "decoder_net: \n",
      "VGG5Decoder(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:48<00:00, 204.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average euc: 1.4440752688854932\n",
      "average mse: 0.09039340291179251\n",
      "average ssim: 0.8552792906537652\n",
      "average time: 0.0020127411127090454 avg infer time:0.0003452569007873535\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有图片\n",
    "trainloader,testloader = get_cifar10_normalize(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  deprocess=deprocess,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank数据集 （表格数据二分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_bank import preprocess_bank,tabinfo_bank\n",
    "\n",
    "from target_model.models.TableClassification.BankNet import BankNet1,BankNetDecoder1,bank_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 2 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 2 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Bank/bank-20ep_params.pth'\n",
    "results_dir = f'../results/inverse-model-results-20240414/Bank/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/'\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "linear2.weight\n",
      "linear2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target model的 client net（对模型进行切割）\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "client_net = BankNet1(layer=split_layer,noise_scale=noise_scale)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(bank_cfg):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train decoder model...\n",
      "[('D', 128, 64), 'LR', ('D', 63, 128)]\n",
      "===============processing data===============\n",
      "X_train.shape: (32950, 63)\n",
      "X_test.shape: (8238, 63)\n",
      "y_train.shape: (32950, 1)\n",
      "y_test.shape: (8238, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n",
      "----train decoder----\n",
      "client_net: \n",
      "BankNet1(\n",
      "  (linear1): Linear(in_features=63, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      ")\n",
      "decoder_net: \n",
      "BankNetDecoder1(\n",
      "  (delinear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (delinear2): Linear(in_features=128, out_features=63, bias=True)\n",
      ")\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:08<00:00, 126.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 0, train_loss: [63.452796936035156]\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 189.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 1, train_loss: [2.55757737159729]\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 186.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 2, train_loss: [0.350360631942749]\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 187.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 3, train_loss: [0.1525724083185196]\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 184.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 4, train_loss: [0.20172497630119324]\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 186.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 5, train_loss: [0.2697388231754303]\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 186.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 6, train_loss: [1.9818490743637085]\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 183.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 7, train_loss: [1.5619789361953735]\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 184.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 8, train_loss: [1.095882773399353]\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 199.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 9, train_loss: [0.5650742650032043]\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 191.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 10, train_loss: [0.4056245982646942]\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 196.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 11, train_loss: [0.18588903546333313]\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 189.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 12, train_loss: [0.16741938889026642]\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 191.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 13, train_loss: [0.12515227496623993]\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 193.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 14, train_loss: [0.10246343910694122]\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 191.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 15, train_loss: [0.11320733278989792]\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 194.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 16, train_loss: [0.1283503770828247]\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 198.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 17, train_loss: [0.16170364618301392]\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 194.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 18, train_loss: [0.17891734838485718]\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [00:05<00:00, 195.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 19, train_loss: [1.9602798223495483]\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=0,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = BankNetDecoder1(layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = preprocess_bank(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n",
      "X_train.shape: (32950, 63)\n",
      "X_test.shape: (8238, 63)\n",
      "y_train.shape: (32950, 1)\n",
      "y_test.shape: (8238, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n",
      "----train decoder----\n",
      "client_net: \n",
      "BankNet1(\n",
      "  (linear1): Linear(in_features=63, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      ")\n",
      "decoder_net: \n",
      "BankNetDecoder1(\n",
      "  (delinear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (delinear2): Linear(in_features=128, out_features=63, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8238/8238 [00:27<00:00, 298.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average cos: 0.2624422825339745\n",
      "average euc: 9.768948042181458\n",
      "average mse: 1.862984648346539\n",
      "average acc: 0.21766205389657683\n",
      "average time: 0.0002857756053223486 avg infer time:0.00034716115411145793\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有表格数据行\n",
    "trainloader,testloader = preprocess_bank(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  save_fake=True,tab=tabinfo_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VFL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data+model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import sys\n",
    "sys.path.append('/home/yangjirui/data/drj/PP-Split')\n",
    "sys.path.append('/home/yangjirui/data/drj/PP-Split/ppsplit/')\n",
    "from ppsplit.attacks.model_inversion.UIFV.UIFV_model_data import *\n",
    "# from ppsplit.utils.utils import create_dir\n",
    "# import torch\n",
    "# import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tab = {\n",
    "    'boolList': [i for i in range(0, 22)],\n",
    "    'onehot': {\n",
    "        'marital': [0, 1, 2, 3], 'default': [4, 5, 6],\n",
    "        'loan': [7, 8, 9], 'month': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
    "        'poutcome': [20, 21, 22]\n",
    "    },\n",
    "    'numList': [i for i in range(23, 28)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "\n",
    "parser = argparse.ArgumentParser(\"TabRebuild\")\n",
    "parser.add_argument('--multiple', action='store_true', help='Whether to conduct multiple experiments')\n",
    "parser.add_argument('--name', type=str, default='decoder-rebuild', help='experiment name')\n",
    "parser.add_argument('--data_dir', default='/home/yangjirui/data/vfl-tab-reconstruction/dataset/bank/bank-additional/bank-additional-full.csv',\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='init learning rate')\n",
    "parser.add_argument('--eps', type=float, default=1e-3)\n",
    "parser.add_argument('--workers', type=int, default=6, help='num of workers')\n",
    "parser.add_argument('--k', type=int, default=2, help='num of client')\n",
    "parser.add_argument('--seed', type=int, default=1, help='random seed')\n",
    "parser.add_argument('--save',  default='/data/yangjirui/vfl-tab-reconstruction/rebuild-data/bank/v5/',\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--decoder_mode',\n",
    "                    default=\"/data/yangjirui/vfl-tab-reconstruction/model/bank/v5/attack-model/model+data+.pth.tar\",\n",
    "                    help='location of the decoder mode')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "# parser.add_argument('--inverse_data_output', default=\"/home/yangjirui/paper-code/data/adult/2layer-noloss/base-random/inverse_data.csv\",\n",
    "#                     help='location of the data corpus')\n",
    "parser.add_argument('--weight_decay', type=float, default=3e-5, help='weight decay')\n",
    "parser.add_argument('--gamma', type=float, default=0.97, help='weight decay')\n",
    "parser.add_argument('--epochs', type=int, default=120, help='num of epochs')\n",
    "parser.add_argument('--base_mode', default='/data/yangjirui/vfl-tab-reconstruction/model/bank/v5/best.pth.tar', type=str, metavar='PATH',\n",
    "                    help='path to latest base mode (default: none)')\n",
    "parser.add_argument('--grad_clip', type=float, default=5., help='gradient clipping')\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             41188 non-null  int64  \n",
      " 1   job             41188 non-null  object \n",
      " 2   marital         41188 non-null  object \n",
      " 3   education       41188 non-null  object \n",
      " 4   default         41188 non-null  object \n",
      " 5   housing         41188 non-null  object \n",
      " 6   loan            41188 non-null  object \n",
      " 7   contact         41188 non-null  object \n",
      " 8   month           41188 non-null  object \n",
      " 9   day_of_week     41188 non-null  object \n",
      " 10  duration        41188 non-null  int64  \n",
      " 11  campaign        41188 non-null  int64  \n",
      " 12  pdays           41188 non-null  int64  \n",
      " 13  previous        41188 non-null  int64  \n",
      " 14  poutcome        41188 non-null  object \n",
      " 15  emp.var.rate    41188 non-null  float64\n",
      " 16  cons.price.idx  41188 non-null  float64\n",
      " 17  cons.conf.idx   41188 non-null  float64\n",
      " 18  euribor3m       41188 non-null  float64\n",
      " 19  nr.employed     41188 non-null  float64\n",
      " 20  y               41188 non-null  object \n",
      "dtypes: float64(5), int64(5), object(11)\n",
      "memory usage: 6.6+ MB\n",
      "Xa_train.shape: (32950, 35)\n",
      "Xa_test.shape: (8238, 35)\n",
      "Xa_onehot_index: {'job': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'education': [12, 13, 14, 15, 16, 17, 18, 19], 'housing': [20, 21, 22], 'contact': [23, 24], 'day_of_week': [25, 26, 27, 28, 29]}\n",
      "Xb_train.shape: (32950, 28)\n",
      "Xb_test.shape: (8238, 28)\n",
      "Xb_onehot_index: {'marital': [0, 1, 2, 3], 'default': [4, 5, 6], 'loan': [7, 8, 9], 'month': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'poutcome': [20, 21, 22]}\n",
      "y_train.shape: (32950, 1)\n",
      "y_test.shape: (8238, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n",
      "################################ load Federated Models ############################\n",
      "=> loaded model '/data/yangjirui/vfl-tab-reconstruction/model/bank/v5/best.pth.tar' (epoch: 12 auc: 0.941743657418864)\n",
      "--- epoch: 12, test_loss: 44.79854965209961, test_acc: 0.9153395685877992, test_precison: 0.641940165777375, test_recall: 0.4889806790969582, test_f1: 0.5289009181100045, test_auc: 0.9411629101044713\n",
      "################################ Set Federated Models, optimizer, loss ############################\n",
      "=> loading decoder mode '/data/yangjirui/vfl-tab-reconstruction/model/bank/v5/attack-model/model+data+.pth.tar'\n",
      "################################ recovery data ############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515/515 [00:08<00:00, 60.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9878375584322187\n",
      "onehot_acc: 0.9970449029126213\n",
      "num_acc: 0.9786302139518158\n",
      "similarity 0.8835727652144364\n",
      "euclidean_dist 1.3748186748004654\n",
      "################################ save data ############################\n",
      "[[0.9878375584322187 0.9970449029126213 0.9786302139518158\n",
      "  0.8835727652144364 1.3748186748004654 False 'decoder-rebuild'\n",
      "  '/home/yangjirui/data/vfl-tab-reconstruction/dataset/bank/bank-additional/bank-additional-full.csv'\n",
      "  64 0.001 0.001 6 2 1\n",
      "  '/data/yangjirui/vfl-tab-reconstruction/rebuild-data/bank/v5/'\n",
      "  '/data/yangjirui/vfl-tab-reconstruction/model/bank/v5/attack-model/model+data+.pth.tar'\n",
      "  0.9 3e-05 0.97 120\n",
      "  '/data/yangjirui/vfl-tab-reconstruction/model/bank/v5/best.pth.tar' 5.0\n",
      "  12]]\n",
      "acc:0.9878375584322187, onehot_acc:0.9970449029126213, num_acc:0.9786302139518158, similarity:0.8835727652144364, euclidean_dist:1.3748186748004654\n"
     ]
    }
   ],
   "source": [
    "# 开始攻击\n",
    "train, test = preprocess(args.data_dir)\n",
    "\n",
    "acc, onehot_acc, num_acc, similarity, euclidean_dist = rebuild(train_data=train, test_data=test, tab=tab,\n",
    "                                                               device=device, args=args)\n",
    "\n",
    "print(f\"acc:{acc}, onehot_acc:{onehot_acc}, num_acc:{num_acc}, similarity:{similarity}, euclidean_dist:{euclidean_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit 数据集 （表格数据二分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_credit import preprocess_credit,tabinfo_credit\n",
    "\n",
    "from target_model.models.TableClassification.CreditNet import CreditNet1,CreditNetDecoder1,credit_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 2 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 3 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/credit/credit-20ep_params.pth'\n",
    "results_dir = f'../results/inverse-model-results-20240414/Credit/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/' # 储存\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "batch_norm1.weight\n",
      "batch_norm1.bias\n",
      "batch_norm1.running_mean\n",
      "batch_norm1.running_var\n",
      "batch_norm1.num_batches_tracked\n",
      "linear2.weight\n",
      "linear2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target model的 client net（对模型进行切割）\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "client_net = CreditNet1(layer=split_layer,noise_scale=noise_scale)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(credit_cfg):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train decoder model...\n",
      "[('D', 512, 128), ('LR',), ('BN', 512), ('D', 250, 512)]\n",
      "===============processing data===============\n",
      "X_train.shape: (246008, 250)\n",
      "X_test.shape: (61503, 250)\n",
      "y_train.shape: (246008, 1)\n",
      "y_test.shape: (61503, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n",
      "----train decoder----\n",
      "client_net: \n",
      "CreditNet1(\n",
      "  (linear1): Linear(in_features=250, out_features=512, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ReLU1): LeakyReLU(negative_slope=0.01)\n",
      "  (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "decoder_net: \n",
      "CreditNetDecoder1(\n",
      "  (delinear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (delinear2): Linear(in_features=512, out_features=250, bias=True)\n",
      ")\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:41<00:00, 185.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 0, train_loss: [1.4743270874023438]\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:43<00:00, 174.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 1, train_loss: [2.492779016494751]\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:43<00:00, 178.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 2, train_loss: [1.773805856704712]\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:42<00:00, 178.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 3, train_loss: [0.16835318505764008]\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:42<00:00, 179.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 4, train_loss: [0.24668940901756287]\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:43<00:00, 176.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 5, train_loss: [0.8895053863525391]\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:42<00:00, 182.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 6, train_loss: [0.6065173149108887]\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:43<00:00, 178.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 7, train_loss: [0.16784319281578064]\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:43<00:00, 176.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 8, train_loss: [0.254721999168396]\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:43<00:00, 176.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 9, train_loss: [0.8115662336349487]\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:44<00:00, 173.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 10, train_loss: [1.8489830493927002]\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:44<00:00, 173.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 11, train_loss: [0.1518251895904541]\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:44<00:00, 173.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 12, train_loss: [0.2923305928707123]\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:42<00:00, 180.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 13, train_loss: [0.678328275680542]\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:43<00:00, 176.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 14, train_loss: [1.4882549047470093]\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:42<00:00, 182.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 15, train_loss: [0.2673455476760864]\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:42<00:00, 179.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 16, train_loss: [0.1500895917415619]\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:42<00:00, 180.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 17, train_loss: [0.2651415467262268]\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:43<00:00, 175.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 18, train_loss: [0.6400712132453918]\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:42<00:00, 179.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 19, train_loss: [1.0420310497283936]\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=0,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = CreditNetDecoder1(layer=split_layer)\n",
    "    # optimizer = torch.optim.SGD(decoder_net.parameters(), 1e-3)\n",
    "\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = preprocess_credit(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20,\n",
    "                            # optimizer=optimizer\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n",
      "X_train.shape: (246008, 250)\n",
      "X_test.shape: (61503, 250)\n",
      "y_train.shape: (246008, 1)\n",
      "y_test.shape: (61503, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n",
      "----train decoder----\n",
      "client_net: \n",
      "CreditNet1(\n",
      "  (linear1): Linear(in_features=250, out_features=512, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ReLU1): LeakyReLU(negative_slope=0.01)\n",
      "  (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "decoder_net: \n",
      "CreditNetDecoder1(\n",
      "  (delinear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (delinear2): Linear(in_features=512, out_features=250, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61503/61503 [10:44<00:00, 95.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average cos: 0.3584266871135943\n",
      "average euc: 19.05002367519168\n",
      "average mse: 1.6026163726057512\n",
      "average acc: 0.2982943921434727\n",
      "average time: 0.0002927957076459463 avg infer time:0.0004771134156478203\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有表格数据行\n",
    "trainloader,testloader = preprocess_credit(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  save_fake=True,tab=tabinfo_credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purchase100 数据集 （表格数据多分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase,tabinfo_purchase\n",
    "\n",
    "from target_model.models.TableClassification.PurchaseNet import PurchaseClassifier1,PurchaseDecoder1,purchase_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 2 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 3 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Purchase100/Purchase_bestmodel_param.pth'\n",
    "results_dir = f'../results/inverse-model-results-20240414/Purchase/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/'\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "linear2.weight\n",
      "linear2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target model的 client net（对模型进行切割）\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "client_net = PurchaseClassifier1(layer=split_layer,noise_scale=noise_scale)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(purchase_cfg):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading decoder model '../results/inverse-model-results-20240414/Purchase/2/Decoder-layer3.pth'\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=0,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = PurchaseDecoder1(layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = preprocess_purchase(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase100 dataset processing...\n",
      "datset route: /home/dengruijun/data/FinTech/DATASET/kaggle-dataset/Purchase100//data.npz\n",
      "original dataset shape:  (197324, 600)\n",
      "After random selection, dataset shape:  (197324, 600)\n",
      "After split between classifier and attack: \n",
      "training dataset shape:  (157859, 600)\n",
      "testing dataset shape:  (39465, 600)\n",
      "Data loading finished\n",
      "----train decoder----\n",
      "client_net: \n",
      "PurchaseClassifier1(\n",
      "  (linear1): Linear(in_features=600, out_features=1024, bias=True)\n",
      "  (Tanh1): Tanh()\n",
      "  (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (Tanh2): Tanh()\n",
      ")\n",
      "decoder_net: \n",
      "PurchaseDecoder1(\n",
      "  (delinear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (Tanh1): Tanh()\n",
      "  (delinear2): Linear(in_features=1024, out_features=600, bias=True)\n",
      "  (Tanh2): Tanh()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39465/39465 [32:26<00:00, 20.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average cos: 0.6405844313075421\n",
      "average euc: 11.187545064271033\n",
      "average mse: 0.2091979069004007\n",
      "average acc: 0.3298509649900756\n",
      "average time: 0.00045916630894636665 avg infer time:0.0008126230395732837\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有表格数据行\n",
    "trainloader,testloader = preprocess_purchase(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  save_fake=True,tab=tabinfo_purchase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drj-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
