{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个notebook 发起了 fsha 数据重构攻击\n",
    "# 注意clientnet和shadownet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.attacks.model_inversion.fsha import FSHA_Attack, discriminatorNet\n",
    "from ppsplit.utils.utils import create_dir\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 导入各个baseline模型及其数据集预处理方法\n",
    "# 模型\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "from target_model.models.ImageClassification.VGG5_9 import VGG,VGG5Decoder,model_cfg\n",
    "from target_model.models.TableClassification.BankNet import BankNet1,BankNetDecoder1,bank_cfg\n",
    "from target_model.models.TableClassification.CreditNet import CreditNet1,CreditNetDecoder1,credit_cfg\n",
    "from target_model.models.TableClassification.PurchaseNet import PurchaseClassifier1,PurchaseDecoder1,purchase_cfg\n",
    "# 数据预处理方法\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize,get_one_data,deprocess\n",
    "from target_model.data_preprocessing.preprocess_bank import bank_dataset,preprocess_bank\n",
    "from target_model.data_preprocessing.preprocess_credit import preprocess_credit\n",
    "from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "CIFAR10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 一些超参数\n",
    "args = {\n",
    "        'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        # 'device':torch.device(\"cpu\"),\n",
    "        'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        'batch_size':1\n",
    "        }\n",
    "print(args['device'])\n",
    "args['data_type']= 1 if args['dataset']=='CIFAR10' else 0 # 区分图像数据（1）和表格数据（0）\n",
    "print(args['dataset'])\n",
    "print(args['data_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "tensor([[0.8194, 0.9482, 2.1111, 3.0231, 3.8683],\n",
      "        [2.0133, 3.1347, 4.0841, 4.9805, 6.0956]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.Tensor([[1,1,2,3,4],\n",
    "                 [2,3,4,5,6]])\n",
    "noise = torch.distributions.Laplace(0.0,0.1)\n",
    "y = x+noise.sample(x.size()).to(x.device)\n",
    "print(x.size())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集和模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "features.5.weight\n",
      "features.5.bias\n",
      "features.5.running_mean\n",
      "features.5.running_var\n",
      "features.5.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "# 数据集和模型加载\n",
    "# 加载模型和数据集，并从unit模型中切割出client_model\n",
    "if args['dataset']=='CIFAR10':\n",
    "    # 超参数\n",
    "    testset_len = 10000 # 10000个数据一次 整个测试集合的长度\n",
    "    # split_layer_list = list(range(len(model_cfg['VGG5'])))\n",
    "    split_layer = 2 # 定成3吧？\n",
    "    test_num = 1 # 试验序号\n",
    "    \n",
    "    # 关键路径\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/VGG5/BN+Tanh/VGG5-params-20ep.pth' # VGG5-BN+Tanh # 存储的是模型参数，不包括模型结构\n",
    "    results_dir  = f'../results/FSHA-results-20240413/VGG5/{test_num}/'\n",
    "    decoder_route = f\"../results/VGG5/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    trainloader,testloader = get_cifar10_normalize(batch_size=1)\n",
    "    # one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "    one_data_loader = get_one_data(trainloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # 切割成client model\n",
    "    client_net = VGG('Client','VGG5',split_layer,model_cfg)\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(model_cfg['VGG5']):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "    \n",
    "    # 其他fsha要用到的网络\n",
    "    shadow_net = VGG('Client','VGG5',split_layer,model_cfg)\n",
    "    decoder_net = VGG5Decoder(split_layer=split_layer)\n",
    "\n",
    "elif args['dataset']=='bank':\n",
    "    # 超参数\n",
    "    test_num = 1 # 试验序号\n",
    "    testset_len=8238\n",
    "    # split_layer_list = ['linear1', 'linear2']\n",
    "    split_layer_list = [0,2,4,6]\n",
    "    split_layer = 2\n",
    "\n",
    "    # 关键路径\n",
    "    results_dir  = f'../results/FSHA-results-2024041/Bank/{test_num}/'\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Bank/bank-20ep_params.pth'\n",
    "    decoder_route = f\"../results/Bank/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    trainloader,testloader = preprocess_bank(batch_size=1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据 \n",
    "\n",
    "    # 模型加载\n",
    "    client_net = BankNet1(layer=split_layer)\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(bank_cfg):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "    # 其他fsha要用到的网络\n",
    "    shadow_net = BankNet1(layer=split_layer)\n",
    "    decoder_net = BankNetDecoder1(layer=split_layer)\n",
    "\n",
    "elif args['dataset']=='credit':\n",
    "    # 超参数\n",
    "    test_num = 1 # 试验序号\n",
    "    testset_len = 61503 # for the mutual information\n",
    "    split_layer_list = [0,3,6,9]\n",
    "    split_layer = 3\n",
    "\n",
    "    # 关键路径\n",
    "    results_dir  = f'../results/FSHA-results-2024041/Credit/{test_num}/'\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/credit/credit-20ep_params.pth'\n",
    "    decoder_route = f\"../results/Credit/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    trainloader,testloader = preprocess_credit(batch_size=1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # client模型切割加载\n",
    "    client_net = CreditNet1(layer=split_layer)\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(credit_cfg):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "    # 其他fsha要用到的网络\n",
    "    shadow_net = CreditNet1(layer=split_layer)\n",
    "    decoder_net = CreditNetDecoder1(layer=split_layer)\n",
    "\n",
    "elif args['dataset']=='purchase':\n",
    "    # 超参数\n",
    "    test_num = 1 # 试验序号\n",
    "    testset_len = 39465 # test len\n",
    "    # split_layer_list = [0,1,2,3,4,5,6,7,8]\n",
    "    split_layer = 3\n",
    "\n",
    "    # 关键路径\n",
    "    results_dir = f'../results/FSHA-results-2024041/Purchase/{test_num}/'\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Purchase100/Purchase_bestmodel_param.pth'\n",
    "    decoder_route = f\"../results/Purchase/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "    \n",
    "    # 数据集加载\n",
    "    trainloader,testloader = preprocess_purchase(batch_size=1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # 模型加载\n",
    "    client_net = PurchaseClassifier1(layer=split_layer)\n",
    "    # pweights = torch.load(unit_net_route,map_location=torch.device('cpu'))\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(purchase_cfg):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "    # 其他fsha要用到的网络\n",
    "    shadow_net = PurchaseClassifier1(layer=split_layer)\n",
    "    decoder_net = PurchaseDecoder1(layer=split_layer)\n",
    "\n",
    "else:\n",
    "    exit(-1)\n",
    "\n",
    "discriminator_net = discriminatorNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建储存结果的文件夹\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/' # 储存逆向结果的dir\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "# 准备好攻击所需的模型的路径\n",
    "shadow_net_route = results_dir+'/shadow_net.pth'\n",
    "# shadow_net_route = unit_net_route # 直接用client net的参数\n",
    "discriminator_net_route = results_dir+'discriminator_net.pth'\n",
    "decoder_net_route = results_dir+'decoder_net.pth'\n",
    "client_net_route = results_dir+'client_net.pth'\n",
    "\n",
    "# client_net调整模式\n",
    "client_net = client_net.to(args['device'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练攻击模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsha_attack = FSHA_Attack(gpu=True,\n",
    "                          data_type=args['data_type'],\n",
    "                          client_route=client_net_route,\n",
    "                          shadow_route=shadow_net_route,\n",
    "                          decoder_route=decoder_net_route,\n",
    "                          discriminator_route=discriminator_net_route,\n",
    "                          inverse_dir=inverse_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading decoder model '../results/FSHA-results-20240413/VGG5/1/decoder_net.pth'\n"
     ]
    }
   ],
   "source": [
    "# 训练攻击模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了 直接加载模型\n",
    "# if False: # 如果已经训练好了 直接加载模型\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    # shadow_net.load_state_dict(pweights) # 加载client_net 参数\n",
    "    client_net = torch.load(client_net_route)\n",
    "    shadow_net = torch.load(shadow_net_route)\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "    discriminator_net = torch.load(discriminator_net_route)\n",
    "\n",
    "\n",
    "else: # 如果没有, 就训练一个\n",
    "    print(\"train decoder model...\")\n",
    "    # 创建新的batch_size为1的DataLoader \n",
    "    # shadow_net.load_state_dict(client_net.state_dict())\n",
    "    client_net.load_state_dict(shadow_net.state_dict())\n",
    "    new_trainloader = DataLoader(trainloader.dataset, batch_size=128)\n",
    "    client_net, shadow_net, decoder_net = fsha_attack.train_decoder(client_net=client_net,\n",
    "                              shadow_net=shadow_net,\n",
    "                              decoder_net=decoder_net,\n",
    "                              discriminator_net=discriminator_net,\n",
    "                              private_loader=new_trainloader,public_loader=new_trainloader,\n",
    "                              epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行重构攻击并评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----train decoder----\n",
      "client_net: \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n",
      "decoder_net: \n",
      "VGG5Decoder(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (4): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average euc: 1.5015357732772827\n",
      "average mse: 0.08566772937774658\n",
      "average ssim: 0.8253371119499207\n",
      "average time: 0.0057735443115234375 avg infer time:0.00302886962890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fsha_attack.inverse(client_net=shadow_net,\n",
    "                    decoder_net=decoder_net,\n",
    "                    train_loader=trainloader,test_loader=one_data_loader,\n",
    "                    deprocess=None if args['data_type']==0 else deprocess,\n",
    "                    save_fake=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 另外评估ML_Efficacy指标\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drj-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
