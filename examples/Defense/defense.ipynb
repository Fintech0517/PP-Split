{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dengruijun/miniconda3/envs/drj-pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from torch.nn.functional import avg_pool2d\n",
    "\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '48'\n",
    "\n",
    "# 导入各个指标\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "\n",
    "# task select\n",
    "from target_model.task_select import get_dataloader_and_model,get_dataloader_and_model, \\\n",
    "    get_dataloader,get_models,get_infotopo_para\n",
    "\n",
    "# utils\n",
    "from ppsplit.utils import concat_weights, create_dir, load_json, save_json\n",
    "\n",
    "# defense:\n",
    "from ppsplit.defense.obfuscation.scheduler import Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'defense': {'method': 'uniform_noise', 'client': {'model_name': 'resnet18', 'split_layer': 6, 'pretrained': False, 'optimizer': 'adam', 'lr': 0.0003, 'distribution': 'gaussian', 'mean': 0, 'sigma': 1}, 'server': {'model_name': 'resnet18', 'split_layer': 6, 'logits': 2, 'pretrained': False, 'lr': 0.0003, 'optimizer': 'adam'}, 'learning_rate': 0.01, 'total_epochs': 1, 'training_batch_size': 128, 'dataset': 'fairface', 'protected_attribute': 'data', 'prediction_attribute': 'gender', 'img_size': 128, 'split': False, 'test_batch_size': 64, 'exp_id': '1', 'exp_keys': ['client.distribution', 'client.mean', 'client.sigma'], 'device': 'cuda:0'}, 'general': {'result_dir': '20241228-defense/', 'test_num': 'uniform_noise', 'device': 'cuda:0', 'dataset': 'CIFAR10', 'oneData_bs': 1, 'train_bs': 128, 'test_bs': 64, 'model': 'VGG5', 'split_layer': 2, 'ep': -1, 'no_dense': 0, 'noise_scale': 0}}\n"
     ]
    }
   ],
   "source": [
    "# config = load_json('./config/nopeek.json')\n",
    "# config = load_json('./config/shredder.json')\n",
    "# config = load_json('./config/cloak.json')\n",
    "config = load_json('./config/uniform_noise.json')\n",
    "\n",
    "print(config)\n",
    "args = config['general']\n",
    "config['defense']['device']=args['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit_net weights:  odict_keys(['features.0.weight', 'features.0.bias', 'features.1.weight', 'features.1.bias', 'features.1.running_mean', 'features.1.running_var', 'features.1.num_batches_tracked', 'features.4.weight', 'features.4.bias', 'features.5.weight', 'features.5.bias', 'features.5.running_mean', 'features.5.running_var', 'features.5.num_batches_tracked', 'features.8.weight', 'features.8.bias', 'features.9.weight', 'features.9.bias', 'features.9.running_mean', 'features.9.running_var', 'features.9.num_batches_tracked', 'denses.0.weight', 'denses.0.bias', 'denses.1.weight', 'denses.1.bias'])\n",
      "client_net cweights:  odict_keys(['features.0.weight', 'features.0.bias', 'features.1.weight', 'features.1.bias', 'features.1.running_mean', 'features.1.running_var', 'features.1.num_batches_tracked', 'features.4.weight', 'features.4.bias', 'features.5.weight', 'features.5.bias', 'features.5.running_mean', 'features.5.running_var', 'features.5.num_batches_tracked'])\n",
      "len unit_net weights:  25\n",
      "len client_net cweights:  14\n",
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "features.5.weight\n",
      "features.5.bias\n",
      "features.5.running_mean\n",
      "features.5.running_var\n",
      "features.5.num_batches_tracked\n",
      "client_net:  VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n",
      "server_net:  VGG(\n",
      "  (features): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Tanh()\n",
      "  )\n",
      "  (denses): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "unit_net weights:  odict_keys(['features.0.weight', 'features.0.bias', 'features.1.weight', 'features.1.bias', 'features.1.running_mean', 'features.1.running_var', 'features.1.num_batches_tracked', 'features.4.weight', 'features.4.bias', 'features.5.weight', 'features.5.bias', 'features.5.running_mean', 'features.5.running_var', 'features.5.num_batches_tracked', 'features.8.weight', 'features.8.bias', 'features.9.weight', 'features.9.bias', 'features.9.running_mean', 'features.9.running_var', 'features.9.num_batches_tracked', 'denses.0.weight', 'denses.0.bias', 'denses.1.weight', 'denses.1.bias'])\n",
      "server_net cweights:  odict_keys(['features.1.weight', 'features.1.bias', 'features.2.weight', 'features.2.bias', 'features.2.running_mean', 'features.2.running_var', 'features.2.num_batches_tracked', 'denses.0.weight', 'denses.0.bias', 'denses.1.weight', 'denses.1.bias'])\n",
      "len unit_net weights:  25\n",
      "len server_net cweights:  11\n",
      "train decoder model...\n",
      "unit_net_route: /home/dengruijun/data/FinTech/PP-Split/results/trained_models/ImageClassification/VGG5/BN+Tanh/VGG5-params-20ep.pth\n",
      "infotopo: nb_of_values:  36\n",
      "results_dir: ../../results/20241228-defense//VGG5/uniform_noise/\n",
      "inverse_dir: ../../results/20241228-defense//VGG5/uniform_noise/layer2/\n",
      "decoder_route: ../../results/20241228-defense//VGG5/uniform_noise//Decoder-layer2.pth\n"
     ]
    }
   ],
   "source": [
    "data_msg = get_dataloader(args)\n",
    "model_msg = get_models(args)\n",
    "infotopo_msg = get_infotopo_para(args)\n",
    "msg = {**model_msg,**data_msg,**infotopo_msg}\n",
    "\n",
    "# 数据集\n",
    "one_data_loader,trainloader,testloader = data_msg['one_data_loader'],data_msg['trainloader'], data_msg['testloader']\n",
    "data_interval = data_msg['data_interval']\n",
    "data_type = msg['data_type']\n",
    "\n",
    "# effectEntropy Infotopo参数\n",
    "nb_of_values = msg['nb_of_values']\n",
    "\n",
    "conv = msg['conv']\n",
    "pool_size = msg['pool_size']\n",
    "# conv = False\n",
    "print(\"infotopo: nb_of_values: \",nb_of_values)\n",
    "\n",
    "# 模型\n",
    "client_net,decoder_net = model_msg['client_net'],model_msg['decoder_net']\n",
    "server_net,unit_net = model_msg['server_net'], model_msg['unit_net']\n",
    "decoder_route = model_msg['decoder_route']\n",
    "image_deprocess = model_msg['image_deprocess']\n",
    "\n",
    "# 路径\n",
    "results_dir = model_msg['results_dir']\n",
    "inverse_dir = results_dir + 'layer' + str(args['split_layer'])+'/'\n",
    "# data_type = 1 if args['dataset'] == 'CIFAR10' else 0\n",
    "split_layer = args['split_layer']\n",
    "\n",
    "print('results_dir:', results_dir)\n",
    "print('inverse_dir:', inverse_dir)\n",
    "print('decoder_route:', decoder_route)\n",
    "\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "# net使用\n",
    "client_net = client_net.to(args['device'])\n",
    "server_net = server_net.to(args['device'])\n",
    "unit_net = unit_net.to(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 04:42:33,978 - wandb.jupyter - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server model device: cuda:0\n",
      "client model device: cuda:0\n",
      "algo_config {'model_name': 'resnet18', 'split_layer': 6, 'pretrained': False, 'optimizer': 'adam', 'lr': 0.0003, 'distribution': 'gaussian', 'mean': 0, 'sigma': 1, 'method': 'uniform_noise'}\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 29.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tran loss: 1.4083401422061592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 58.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 1.2039988352234952\n",
      "val acc: 0.5778264331210192\n",
      "model saved in  ../../results/20241228-defense//VGG5/uniform_noise/layer2/unit_net_defensed.pth\n"
     ]
    }
   ],
   "source": [
    "# 防御\n",
    "config['defense'][\"results_dir\"] = results_dir\n",
    "# config[\"1\"] = results_dir\n",
    "defense_scheduler = Scheduler(config)\n",
    "if config['defense']['method']=='cloak':\n",
    "    client_net = None\n",
    "    server_net = unit_net\n",
    "\n",
    "# 初始化和run存储模型\n",
    "defense_scheduler.initialize(train_loader = trainloader, test_loader = testloader, client_model = client_net, server_model = server_net)\n",
    "client_net,server_net = defense_scheduler.run_job()\n",
    "\n",
    "# 拼接模型并保存\n",
    "if client_net: # 如果有client_net，则拼接\n",
    "    new_weights_unit = concat_weights(unit_net.state_dict(),client_net.state_dict(),server_net.state_dict())\n",
    "else: # 如果没有client_net，则直接保存server_net\n",
    "    new_weights_unit = server_net.state_dict()\n",
    "\n",
    "unit_net.load_state_dict(new_weights_unit)\n",
    "torch.save(unit_net.state_dict(), inverse_dir + 'unit_net_defensed.pth')\n",
    "\n",
    "print(\"model saved in \",inverse_dir + 'unit_net_defensed.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drj-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
