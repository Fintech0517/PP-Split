{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Membership Inference Attack (MIA)\n",
    "对 （Split learning 或 VFL 框架下的）神经网络模型进行membership infernece attack，目前采用了5个攻击方法\n",
    "* Modified Entropy (Mentropy)\n",
    "* Entropy\n",
    "* Confidence\n",
    "* Correctness\n",
    "* ML Leaks\n",
    "类中已经对smashed data进行了softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Ruijun Deng\n",
    "Date: 2024-05-22 22:27:05\n",
    "LastEditTime: 2024-05-22 22:27:23\n",
    "LastEditors: Ruijun Deng\n",
    "FilePath: /PP-Split/examples/membership_inference_attack_and_quantification.ipynb\n",
    "Description: \n",
    "'''\n",
    "# 导包\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import urllib\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.attacks.membership_inference.Mentr_attack import MentrAttack # 包含了4种方法的攻击类\n",
    "from ppsplit.attacks.membership_inference.ML_Leaks_attack import MLLeaksAttack # 包含了ML Leaks攻击类\n",
    "\n",
    "# 模型\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "from target_model.models.ImageClassification.VGG5_9 import VGG,VGG5Decoder,model_cfg\n",
    "from target_model.models.TableClassification.BankNet import BankNet1,bank_cfg\n",
    "from target_model.models.TableClassification.CreditNet import CreditNet1,credit_cfg\n",
    "from target_model.models.TableClassification.PurchaseNet import PurchaseClassifier1,purchase_cfg\n",
    "# 数据预处理方法\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize,get_one_data,deprocess\n",
    "from target_model.data_preprocessing.preprocess_bank import bank_dataset,preprocess_bank\n",
    "from target_model.data_preprocessing.preprocess_credit import preprocess_credit\n",
    "from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase\n",
    "\n",
    "\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# utils\n",
    "from ppsplit.utils.utils import create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "        'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        # 'device':torch.device(\"cpu\"),\n",
    "        'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        # 'result_dir': 'InvMetric-202403',\n",
    "        'result_dir': 'MIA/',\n",
    "        'batch_size':32,\n",
    "        'noise_scale':0, # 防护措施 \n",
    "        'num_pairs': 200, # RepE\n",
    "        'topk':10, # smashed data的size\n",
    "        }\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "  )\n",
       "  (denses): Sequential()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型和数据集，并从unit模型中切割出client_model\n",
    "if args['dataset']=='CIFAR10':\n",
    "    # 超参数\n",
    "    testset_len = 10000 # 10000个数据一次 整个测试集合的长度\n",
    "    # split_layer_list = list(range(len(model_cfg['VGG5'])))\n",
    "    split_layer = 6 # 定成2吧？\n",
    "    test_num = 3 # 试验序号\n",
    "\n",
    "    # 关键路径\n",
    "    # 此时是为了repE\n",
    "    # unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/VGG5/BN+Tanh/VGG5-params-20ep.pth' # VGG5-BN+Tanh # 存储的是模型参数，不包括模型结构\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/VGG5/20240429-RepE/VGG5-params-19ep.pth' # VGG5-BN+Tanh # 存储的是模型参数，不包括模型结构\n",
    "    results_dir  = f\"../results/{args['result_dir']}/VGG5/layer{split_layer}/\"\n",
    "    decoder_route = f\"../results/{args['result_dir']}/VGG5/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    # trainloader,testloader = get_cifar10_normalize(batch_size = args['batch_size'])\n",
    "    # one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "    shadow_train_loader, shadow_test_loader = get_cifar10_normalize(batch_size = args['batch_size'])\n",
    "    target_train_loader, target_test_loader = get_cifar10_normalize(batch_size = args['batch_size'])\n",
    "\n",
    "    # 切割成client model\n",
    "    # vgg5_unit.load_state_dict(torch.load(unit_net_route,map_location=torch.device('cpu'))) # 完整的模型\n",
    "    client_net = VGG('Client','VGG5',split_layer,model_cfg,noise_scale=args['noise_scale'])\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(model_cfg['VGG5']):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "    class_num = 10\n",
    "\n",
    "elif args['dataset']=='purchase':\n",
    "    # 设置一些超参数\n",
    "    batch_size = 100 # 批大小\n",
    "    # unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Purchase100/Purchase_bestmodel_param.pth' # 待检测模型\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/VFL/MIAs/membership_inference_evaluation/adv_reg/training_code/models/purchase_undefended/model_best.pth.tar' # 待检测模型\n",
    "    split_layer = 8 # 切隔层\n",
    "    # purchase 数据集 和 模型 导入\n",
    "    from target_model.models.TableClassification.PurchaseNet import PurchaseClassifier1, purchase_cfg\n",
    "        \n",
    "    class PurchaseClassifier(nn.Module):\n",
    "        def __init__(self,num_classes=100):\n",
    "            super(PurchaseClassifier, self).__init__()\n",
    "\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Linear(600,1024),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(1024,512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512,256),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(256,128),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "            self.classifier = nn.Linear(128,num_classes)\n",
    "            \n",
    "        def forward(self,x):\n",
    "            hidden_out = self.features(x)\n",
    "            return self.classifier(hidden_out)\n",
    "\n",
    "    from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase_shadow\n",
    "    class_num = 100 # Purchase的分类类别数目 # 源论文默认100\n",
    "\n",
    "    # 模型加载并切割：\n",
    "    # client_net = PurchaseClassifier1(layer=split_layer)\n",
    "    # pweights = torch.load(unit_net_route,map_location=device)\n",
    "    # if split_layer < len(purchase_cfg):\n",
    "    #     pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    # client_net.load_state_dict(pweights)\n",
    "\n",
    "    client_net = PurchaseClassifier(num_classes=100)\n",
    "    client_net = torch.nn.DataParallel(client_net).cuda()\n",
    "    checkpoint = torch.load(unit_net_route)\n",
    "    client_net.load_state_dict(checkpoint['state_dict'])\n",
    "    client_net.eval()\n",
    "\n",
    "    # model = PurchaseClassifier1()\n",
    "    # model = torch.nn.DataParallel(model).cuda()\n",
    "    # checkpoint = torch.load(target_model_path)\n",
    "    # model.load_state_dict(checkpoint['state_dict'])\n",
    "    # model.eval()\n",
    "\n",
    "    # 加载数据集\n",
    "    shadow_train_loader, shadow_test_loader,\\\n",
    "        target_train_loader, target_test_loader = preprocess_purchase_shadow(batch_size=batch_size)\n",
    "    \n",
    "else:\n",
    "    exit(-1)\n",
    "\n",
    "# 创建文件夹\n",
    "create_dir(results_dir)\n",
    "\n",
    "# client_net使用\n",
    "client_net = client_net.to(args['device'])\n",
    "client_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 前四种攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 32)\n",
      "(10000, 32, 32, 32)\n",
      "(50000, 32, 32, 32)\n",
      "(10000, 32, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# 攻击 对象实例化\n",
    "# ('C', 3, 32, 3, 32*32*32, 32*32*32*3*3*3), # 0\n",
    "# ('M', 32, 32, 2, 32*16*16, 0),  # 1\n",
    "# ('C', 32, 64, 3, 64*16*16, 64*16*16*3*3*32), #2\n",
    "# ('M', 64, 64, 2, 64*8*8, 0), # 3\n",
    "# ('C', 64, 64, 3, 64*8*8, 64*8*8*3*3*64), # 4\n",
    "# ('D', 8*8*64, 128, 1, 64, 128*8*8*64), # 5 \n",
    "# ('D', 128, 10, 1, 10, 128*10)], # 6\n",
    "MIA = MentrAttack(num_classes = class_num)\n",
    "\n",
    "# 攻击初始化，推理模型\n",
    "# 这里 shadow model 和 target model 用的是相同的一个\n",
    "MIA.prepare_model_performance(client_net, shadow_train_loader, shadow_test_loader, \n",
    "                              client_net, target_train_loader, target_test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MentrAttack' object has no attribute 't_tr_corr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/dengruijun/data/FinTech/PP-Split/examples/membership_inference_attack_and_quantification.ipynb 单元格 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdrj-gpu-10_176_22_36/home/dengruijun/data/FinTech/PP-Split/examples/membership_inference_attack_and_quantification.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m benchmark_methods \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmodified entropy\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# 可以取子集\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdrj-gpu-10_176_22_36/home/dengruijun/data/FinTech/PP-Split/examples/membership_inference_attack_and_quantification.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# benchmark_methods = ['correctness','confidence','entropy','modified entropy'] # 可以取子集\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdrj-gpu-10_176_22_36/home/dengruijun/data/FinTech/PP-Split/examples/membership_inference_attack_and_quantification.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m MIA\u001b[39m.\u001b[39;49mmem_inf_benchmarks(all_methods\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/data/FinTech/PP-Split/ppsplit/attacks/membership_inference/Mentr_attack.py:133\u001b[0m, in \u001b[0;36mMentrAttack.mem_inf_benchmarks\u001b[0;34m(self, all_methods, benchmark_methods)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmem_inf_benchmarks\u001b[39m(\u001b[39mself\u001b[39m, all_methods\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, benchmark_methods\u001b[39m=\u001b[39m[]): \u001b[39m# 4大类攻击\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m (all_methods) \u001b[39mor\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mcorrectness\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m benchmark_methods):\n\u001b[0;32m--> 133\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mem_inf_via_corr()\n\u001b[1;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m (all_methods) \u001b[39mor\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m benchmark_methods):\n\u001b[1;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_inf_thre(\u001b[39m'\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms_tr_conf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms_te_conf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_tr_conf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_te_conf)\n",
      "File \u001b[0;32m~/data/FinTech/PP-Split/ppsplit/attacks/membership_inference/Mentr_attack.py:113\u001b[0m, in \u001b[0;36mMentrAttack._mem_inf_via_corr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mem_inf_via_corr\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39m# 基于correctness 进行攻击\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[39m# perform membership inference attack based on whether the input is correctly classified or not\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     t_tr_acc \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt_tr_corr)\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_tr_corr)\u001b[39m+\u001b[39m\u001b[39m0.0\u001b[39m)\n\u001b[1;32m    114\u001b[0m     t_te_acc \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_te_corr)\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_te_corr)\u001b[39m+\u001b[39m\u001b[39m0.0\u001b[39m)\n\u001b[1;32m    115\u001b[0m     mem_inf_acc \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\u001b[39m*\u001b[39m(t_tr_acc \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m t_te_acc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MentrAttack' object has no attribute 't_tr_corr'"
     ]
    }
   ],
   "source": [
    "# 实施MIA攻击\n",
    "# 设定要执行的攻击类型：默认是4种都执行\n",
    "\n",
    "benchmark_methods = ['correctness','confidence','entropy','modified entropy'] # 可以取子集\n",
    "MIA.mem_inf_benchmarks(all_methods=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For membership inference attack via confidence, the attack acc is 514.044\n",
      "For membership inference attack via entropy, the attack acc is 513.084\n",
      "For membership inference attack via modified entropy, the attack acc is 514.009\n"
     ]
    }
   ],
   "source": [
    "# 实施MIA攻击，只执行指定的2种攻击\n",
    "# benchmark_methods = ['correctness','modified entropy'] # 可以取子集\n",
    "benchmark_methods = ['confidence','entropy','modified entropy'] # 可以取子集\n",
    "MIA.mem_inf_benchmarks(all_methods=False, benchmark_methods = benchmark_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 ML Leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集\n",
    "from target_model.data_preprocessing.dataset import ListDataset\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize_two_train\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seen_loader,unseen_loader,test_loader = get_cifar10_normalize_two_train(batch_size=1)\n",
    "\n",
    "print(\"seen data length: \",len(seen_loader.dataset))\n",
    "print(\"unseen data length: \", len(unseen_loader.dataset))\n",
    "print(\"test data length: \", len(test_loader.dataset))\n",
    "\n",
    "x = iter(seen_loader).next()\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance准备 准备attacker的 input feature，处理完存储起来\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "dataset_route = f\"../results/{args['result_dir']}/VGG5/layer{split_layer}/\"\n",
    "\n",
    "MIA = MLLeaksAttack(smashed_data_size=args['topk']) # top10\n",
    "\n",
    "if os.path.isfile(dataset_route+'attack_train_member25000.pkl'):\n",
    "    print(f\"=> loading paired dataset from {dataset_route}\")\n",
    "    with open(dataset_route+'attack_train_member25000.pkl','rb') as f:\n",
    "        smashed_data_list = pickle.load(file=f)\n",
    "else:\n",
    "    print(f\"=> making paired dataset...\")\n",
    "    smashed_data_list = MIA.prepare_model_performance(client_net,seen_loader,unseen_loader,test_loader)\n",
    "    with open(dataset_route+'attack_train_member25000.pkl','wb') as f:\n",
    "        pickle.dump(obj=smashed_data_list, file=f)\n",
    "\n",
    "smashed_dataset = ListDataset(smashed_data_list)\n",
    "# print(len(smashed_dataset))\n",
    "attack_loader = DataLoader(smashed_dataset, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看attack training数据集：\n",
    "all_labels = [d[1] for d in smashed_data_list]\n",
    "all_features = [d[0] for d in smashed_data_list]\n",
    "print(\"第一个smashed data形状:\",all_features[0].shape)\n",
    "print(\"第一个smashed data取值:\",all_features[0])\n",
    "print(\"所有标签: \",all_labels)\n",
    "\n",
    "# 第一个数据的shape：\n",
    "print(\"第一个数据的shape: \")\n",
    "(x,y) = iter(attack_loader).next()\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练attack model\n",
    "attack_loader = DataLoader(smashed_dataset, batch_size=32)\n",
    "\n",
    "MIA.train_attack_model(attack_loader,optimizer=None,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 attack model 进行 MIA攻击\n",
    "data_loader = DataLoader(smashed_dataset, batch_size=1)\n",
    "\n",
    "MIA.MIA_test(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 F\n",
    "import torch.nn.functional as F\n",
    "x=torch.tensor([[1,1,1,],[4,4,4]],dtype=float)\n",
    "print(F.softmax(x,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1])\n",
    "y = x.squeeze()\n",
    "print(x)\n",
    "print([i for i in y.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Quantification for MIA\n",
    "用 privacy score 量化 MIA攻击隐私情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppsplit.quantification.privacy_risk_score.privacy_risk_score import calculate_risk_score # 导包\n",
    "# private risk score calculation\n",
    "risk_score = calculate_risk_score(MIA.s_tr_m_entr, MIA.s_te_m_entr,  # shadow model mentropy\n",
    "                                MIA.s_tr_labels, MIA.s_te_labels,  # shadow model labels\n",
    "                                MIA.t_tr_m_entr, MIA.t_tr_labels) # target model mentropy & labels\n",
    "print(f\"target model 的 训练数据 (共{len(risk_score)}）的MIA隐私泄漏 privacy score 为：\", risk_score)\n",
    "print(f\"平均privacy score为{risk_score.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drj-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
