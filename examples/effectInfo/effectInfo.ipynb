{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基础设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '48'\n",
    "\n",
    "# 导入各个指标\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.quantification.distance_correlation.distCor import distCorMetric\n",
    "from ppsplit.quantification.fisher_information.dFIL_inverse import dFILInverseMetric\n",
    "from ppsplit.quantification.shannon_information.mutual_information import MuInfoMetric\n",
    "from ppsplit.quantification.shannon_information.ULoss import ULossMetric\n",
    "from ppsplit.quantification.rep_reading.rep_reader import PCA_Reader\n",
    "from ppsplit.quantification.shannon_information.ITE_tools import Shannon_quantity\n",
    "\n",
    "from target_model.task_select import get_dataloader_and_model,get_dataloader_and_model, get_dataloader,get_models\n",
    "\n",
    "# utils\n",
    "from ppsplit.utils.utils import create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "        # 'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'device':torch.device(\"cpu\"),\n",
    "        # 'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        'dataset':'Iris',\n",
    "        # 'result_dir': '20240702-FIL/',\n",
    "        'result_dir': '20240702-effectiveInfo/',\n",
    "        'oneData_bs': 3,\n",
    "        'test_bs': 1,\n",
    "        'train_bs': 1,\n",
    "        'noise_scale': 0, # 防护措施\n",
    "        'OneData':True,\n",
    "        'split_layer': 5,\n",
    "        # 'test_num': 'invdFIL', # MI, invdFIL, distCor, ULoss, \n",
    "        'test_num': 'effectiveInfo'\n",
    "        }\n",
    "print(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n",
      "X_train.shape: (120, 4)\n",
      "X_test.shape: (30, 4)\n",
      "y_train.shape: (120,)\n",
      "y_test.shape: (30,) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "linear2.weight\n",
      "linear2.bias\n",
      "linear3.weight\n",
      "linear3.bias\n",
      "train decoder model...\n",
      "['Tanh', ('D', 64, 3), 'Tanh', ('D', 128, 64), 'Tanh', ('D', 4, 128)]\n",
      "results_dir: ../../results/20240702-effectiveInfo//Iris/effectiveInfo/\n",
      "inverse_dir: ../../results/20240702-effectiveInfo//Iris/effectiveInfo/layer5/\n",
      "decoder_route: ../../results/20240702-effectiveInfo//Iris/effectiveInfo//Decoder-layer5.pth\n",
      "linear1.weight torch.Size([128, 4])\n",
      "linear1.bias torch.Size([128])\n",
      "linear2.weight torch.Size([64, 128])\n",
      "linear2.bias torch.Size([64])\n",
      "linear3.weight torch.Size([3, 64])\n",
      "linear3.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "data_msg = get_dataloader(args)\n",
    "model_msg = get_models(args)\n",
    "msg = {**model_msg,**data_msg}\n",
    "\n",
    "# 数据集\n",
    "one_data_loader,trainloader,testloader = data_msg['one_data_loader'],data_msg['trainloader'], data_msg['testloader']\n",
    "\n",
    "# 模型和路径\n",
    "client_net,decoder_net = model_msg['client_net'],model_msg['decoder_net']\n",
    "decoder_route = model_msg['decoder_route']\n",
    "image_deprocess = model_msg['image_deprocess']\n",
    "\n",
    "results_dir = model_msg['results_dir']\n",
    "inverse_dir = results_dir + 'layer'+str(args['split_layer'])+'/'\n",
    "data_type = 1 if args['dataset'] == 'CIFAR10' else 0\n",
    "split_layer = args['split_layer']\n",
    "\n",
    "print('results_dir:',results_dir)\n",
    "print('inverse_dir:',inverse_dir)\n",
    "print('decoder_route:',decoder_route)\n",
    "\n",
    "create_dir(results_dir)\n",
    "\n",
    "# client_net使用\n",
    "client_net = client_net.to(args['device'])\n",
    "client_net.eval()\n",
    "\n",
    "for n, p in client_net.named_parameters():\n",
    "    print(n, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. effective information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effective fisher 计算函数\n",
    "import torch.autograd.functional as F\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# nips23\n",
    "from torch.autograd.functional import jvp\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 自己实现的、规规矩矩的\n",
    "def computing_det_with_outputs(model, inputs, outputs, sigmas): # sigma_square\n",
    "        # batchsize:\n",
    "        batch_size = inputs.shape[0] # 一个batch的样本数目\n",
    "        output_size = outputs[0].numel() # 一个样本的outputs长度\n",
    "        input_size = inputs[0].numel() # 一个样本的outputs长度\n",
    "        effect_fisher_sum = 0.0\n",
    "\n",
    "        # 遍历单个样本: 换数据\n",
    "        for i in range(batch_size):\n",
    "            input_i = inputs[i].unsqueeze(0)\n",
    "\n",
    "            # 计算jacobian\n",
    "            J = F.jacobian(model, input_i)\n",
    "            # J = J.reshape(J.shape[0],outputs.numel(),inputs.numel()) # (batch, out_size, in_size)\n",
    "            J = J.reshape(output_size, input_size) # (batch, out_size, in_size)\n",
    "            # print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "            # 计算eta\n",
    "            JtJ = torch.matmul(J.t(), J)\n",
    "            I = 1.0/(sigmas)*JtJ\n",
    "            # ddFIL  = I.trace().div(input_size*input_size)\n",
    "\n",
    "            # 储存I\n",
    "            # I_np = I.cpu().detach().numpy()\n",
    "            # df = pd.DataFrame(I_np)\n",
    "            # df.to_csv(f'{i}.csv',index=False,header=False)\n",
    "\n",
    "            # print(\"I: \", I)\n",
    "            # w = torch.det(I)\n",
    "            # print('det I: ', I.det().log())\n",
    "\n",
    "            f1 = input_size * torch.log(2*torch.pi*torch.exp(torch.tensor(1.0)))\n",
    "            f2 = torch.logdet(I)\n",
    "            # print('log det I: ',f2 )\n",
    "            print('f1: ',f1)\n",
    "            print('f2: ',f2)\n",
    "            effect_fisher = 0.5 * (f1 - f2)\n",
    "            effect_fisher_sum+=effect_fisher\n",
    "\n",
    "            print(\"effect_fisher: \",effect_fisher)\n",
    "\n",
    "        # print(\"Jt*J: \", JtJ)\n",
    "        # print(\"Jt*J: \", JtJ.shape, JtJ)\n",
    "        # print(\"I.shape: \", I.shape)\n",
    "        # eta = dFIL\n",
    "        # print(f\"eta: {eta}\")\n",
    "        # print('t2-t1=',t2-t1, 't3-t2', t3-t2)\n",
    "        effect_fisher_mean = effect_fisher_sum / batch_size\n",
    "        return effect_fisher_mean.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# 用diag 来化简\n",
    "def computing_diag_det_with_outputs(model, inputs, outputs, sigmas): # sigma_square\n",
    "        # batchsize:\n",
    "        batch_size = inputs.shape[0] # 一个batch的样本数目\n",
    "        output_size = outputs[0].numel() # 一个样本的outputs长度\n",
    "        input_size = inputs[0].numel() # 一个样本的outputs长度\n",
    "        effect_fisher_sum = 0.0\n",
    "\n",
    "        # avg\n",
    "        I_diagonal_batch_avg = torch.zeros(input_size).to(args['device']) # batch上做平均\n",
    "        print(\"I_diagonal_batch_avg: \",I_diagonal_batch_avg.shape)\n",
    "        f2_2_avg_outer = torch.tensor(0.0).to(args['device'])\n",
    "        \n",
    "        # effecti_fisher第一部分\n",
    "        f1 = input_size * torch.log(2*torch.pi*torch.exp(torch.tensor(1.0)))\n",
    "        # print('f1: ',f1)\n",
    "\n",
    "        # f2需要求平均？\n",
    "        # 遍历单个样本: 换数据\n",
    "        for i in range(batch_size): # 对每个batch\n",
    "            input_i = inputs[i].unsqueeze(0)\n",
    "\n",
    "            # 计算jacobian\n",
    "            J = F.jacobian(model, input_i)\n",
    "            # J = J.reshape(J.shape[0],outputs.numel(),inputs.numel()) # (batch, out_size, in_size)\n",
    "            J = J.reshape(output_size, input_size) # (batch, out_size, in_size)\n",
    "            # print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "            # 计算eta\n",
    "            JtJ = torch.matmul(J.t(), J)\n",
    "            I = 1.0/(sigmas)*JtJ\n",
    "            # print(\"I: \", I)\n",
    "            # diagonal fisher information matrix (approximation)\n",
    "            I_diagonal = torch.diagonal(I,dim1=0,dim2=1) # vector\n",
    "            # print(\"I_diagonal: \",I_diagonal.shape)\n",
    "\n",
    "            I_diag = torch.diag_embed(I_diagonal) # matrix\n",
    "\n",
    "            # batch的平均\n",
    "            I_diagonal_batch_avg += I_diagonal / (batch_size)\n",
    "\n",
    "            # 储存I\n",
    "            # I_np = I.cpu().detach().numpy()\n",
    "            # df = pd.DataFrame(I_np)\n",
    "            # df.to_csv(f'{i}.csv',index=False,header=False)\n",
    "\n",
    "            # print(\"I: \", I)\n",
    "            # w = torch.det(I)\n",
    "            # print('det I: ', I.det().log())\n",
    "\n",
    "            f2 = torch.logdet(I)\n",
    "            # f2_1 = torch.logdet(I_diag)\n",
    "            f2_2 = torch.sum(torch.log(I_diagonal+1e-10)) # /I_diagonal.numel()\n",
    "            f2_2_avg_outer += f2_2 / batch_size\n",
    "            # print('log det I: ',f2 )\n",
    "            # print('f1: ',f1)\n",
    "            print('f2: ',f2)\n",
    "            # print('f2_1: ',f2_1)\n",
    "            print('f2_2: ',f2_2)\n",
    "\n",
    "        f2_2_avg_inner = torch.sum(torch.log(I_diagonal_batch_avg+1e-10))\n",
    "\n",
    "        print('f2_2_avg_outer: ',f2_2_avg_outer)\n",
    "        print('f2_2_avg_inner: ',f2_2_avg_inner)\n",
    "\n",
    "        # effect_fisher = 0.5 * (f1 - f2_2_avg_inner)\n",
    "        effect_fisher = 0.5 * (f1 - f2_2_avg_outer)\n",
    "        # effect_fisher_sum+=effect_fisher\n",
    "\n",
    "        # print(\"effect_fisher: \",effect_fisher)\n",
    "        \n",
    "        # effect_fisher_mean = effect_fisher_sum / batch_size\n",
    "        return effect_fisher.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  tensor([1, 0, 2])\n",
      "I_diagonal_batch_avg:  torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f2:  tensor(nan, device='cuda:1')\n",
      "f2_2:  tensor(44.7148, device='cuda:1')\n",
      "f2:  tensor(nan, device='cuda:1')\n",
      "f2_2:  tensor(44.7148, device='cuda:1')\n",
      "f2:  tensor(nan, device='cuda:1')\n",
      "f2_2:  tensor(44.7148, device='cuda:1')\n",
      "f2_2_avg_outer:  tensor(44.7148, device='cuda:1')\n",
      "f2_2_avg_inner:  tensor(44.7148, device='cuda:1')\n",
      "effectfisher: -16.681655883789062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# effect fisher 指标\n",
    "\n",
    "effectFisher_same_layer_list = []\n",
    "Fishermetric = dFILInverseMetric() \n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    print('labels: ',labels)\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        # fisher\n",
    "        outputs = client_net(images)\n",
    "        # images = images.unsqueeze(0)\n",
    "        # effectFisher = Fishermetric._computing_det_with_outputs(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01)\n",
    "        # effectFisher = computing_det_with_outputs(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01)\n",
    "\n",
    "        effectFisher = computing_diag_det_with_outputs(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01)\n",
    "        effectFisher_same_layer_list.append(effectFisher)\n",
    "# print(f\"Layer {split_layer} effecInfo: {sum(effecInfo_same_la yer_list)/len(effecInfo_same_layer_list)}\")\n",
    "print(f\"effectfisher: {sum(effectFisher_same_layer_list)/len(effectFisher_same_layer_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect entropy 计算函数\n",
    "import math\n",
    "import numpy as np\n",
    "def shannon_entropy_pyent(time_series):\n",
    "    \"\"\"Calculate Shannon Entropy of the sample data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_series: np.ndarray | list[str]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ent: float\n",
    "        The Shannon Entropy as float value\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate frequency counts\n",
    "    _, counts = np.unique(time_series, return_counts=True)\n",
    "    total_count = len(time_series)\n",
    "    # print('counts: ', counts)\n",
    "    # print(\"total_count: \",total_count)\n",
    "\n",
    "    # Calculate frequencies and Shannon entropy\n",
    "    frequencies = counts / total_count\n",
    "    # print(\"freq: \",frequencies)\n",
    "    ent = -np.sum(frequencies * np.log(frequencies))\n",
    "\n",
    "    return ent\n",
    "\n",
    "import infotopo\n",
    "\n",
    "def shannon_entropy_infotopo(x):\n",
    "    information_top = infotopo.infotopo(dimension_max = x.shape[1],\n",
    "                                        dimension_tot = x.shape[1],\n",
    "                                        sample_size = x.shape[0],\n",
    "                                        nb_of_values = 9, # 不是很懂这个意思，为什么iris对应9？\n",
    "                                        )\n",
    "    # 计算联合分布的概率？（全排列）\n",
    "    joint_prob = information_top._compute_probability(x)\n",
    "    print('joint_prob: ',joint_prob)\n",
    "    # 计算联合熵（全排列的）\n",
    "    joint_prob_ent = information_top.simplicial_entropies_decomposition(x) # log2\n",
    "    new_joint_prob_ent = {key: value * np.log(2) for key, value in joint_prob_ent.items()} #ln 转2为底 成 e为底\n",
    "    print(\"joint_entropy: \",new_joint_prob_ent)\n",
    "    # ent = information_top._compute_forward_entropies(x)\n",
    "    # information_top.entropy_simplicial_lanscape(joint_prob) # 画图\n",
    "    # ent = _entropy(np.array(list(new_joint_prob_ent.values())))\n",
    "\n",
    "    joint_entropy_final = list(new_joint_prob_ent.values())[-1]\n",
    "    return joint_entropy_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effectEntro_pyent:  10.78568442565739\n",
      "joint_prob:  {'0000': 0.7666666666666667, '0100': 0.06666666666666667, '0011': 0.03333333333333333, '0001': 0.1, '1100': 0.03333333333333333}\n",
      "Percent of tuples processed : 0\n",
      "joint_entropy:  {(4,): 1.9819572275458939, (3,): 1.7745281734898692, (2,): 1.900864205924261, (1,): 1.945733436884763, (3, 4): 2.560998380349697, (2, 4): 3.0315188853635178, (1, 4): 2.7655866155925666, (2, 3): 2.904216051704555, (1, 3): 2.470886138576437, (1, 2): 3.0777286974008473, (2, 3, 4): 3.1701483214755073, (1, 3, 4): 2.979194070987063, (1, 2, 4): 3.2625679455501664, (1, 2, 3): 3.2625679455501664, (1, 2, 3, 4): 3.354987569624826}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# effective entorpy\n",
    "\n",
    "from pyentrp import entropy as ent\n",
    "\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): \n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    # print(images)\n",
    "    with torch.no_grad():\n",
    "        # ITE\n",
    "        # effectEntro = Shannon_quantity(images)\n",
    "        # print(\"effectEntro_ite: \",effectEntro)\n",
    "\n",
    "        # PyEntropy\n",
    "        effectEntro_pyent = 0.0\n",
    "        for i in range(len(images[0])): # 对每个维度\n",
    "            effectEntro_pyent  += shannon_entropy_pyent(images[:,i].flatten().detach().cpu().numpy())\n",
    "            # print('effectEntro_pyent: ',effectEntro_pyent)\n",
    "        # effectEntro_pyent = shannon_entropy(images.flatten(start_dim=1).detach().cpu().numpy())\n",
    "        print('effectEntro_pyent: ',effectEntro_pyent)\n",
    "\n",
    "        # infotopo\n",
    "        effectEntro_infotopo = shannon_entropy_infotopo(images.flatten(start_dim=1).detach().cpu().numpy())\n",
    "        # print('effectEntro_infotopo: ',effectEntro_infotopo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint_prob:  {'0000': 0.7666666666666667, '0100': 0.06666666666666667, '0011': 0.03333333333333333, '0001': 0.1, '1100': 0.03333333333333333}\n",
      "Percent of tuples processed : 0\n",
      "joint_entropy:  {(4,): 1.9819572275458939, (3,): 1.7745281734898692, (2,): 1.900864205924261, (1,): 1.945733436884763, (3, 4): 2.560998380349697, (2, 4): 3.0315188853635178, (1, 4): 2.7655866155925666, (2, 3): 2.904216051704555, (1, 3): 2.470886138576437, (1, 2): 3.0777286974008473, (2, 3, 4): 3.1701483214755073, (1, 3, 4): 2.979194070987063, (1, 2, 4): 3.2625679455501664, (1, 2, 3): 3.2625679455501664, (1, 2, 3, 4): 3.354987569624826}\n",
      "inverse_dFIL:  2.0611473e-05\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(37.4480, device='cuda:1')\n",
      "effect_fisher:  tensor(-13.0482, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3 effecInfo: 16.403208204085274\n",
      "Layer 3 InversedFIL: 2.0611472791642882e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# effect Information 指标计算\n",
    "\n",
    "effecInfo_diff_layer_list = []\n",
    "effecInfo_same_layer_list = []\n",
    "EntropyMetric = ULossMetric()\n",
    "Fishermetric = dFILInverseMetric()\n",
    "\n",
    "InversedFIL_same_layer_list = []\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        # effect entropy \n",
    "        # effecEntro= EntropyMetric._entropy_prob_batch(images) # H(x)\n",
    "        effectEntro = shannon_entropy_infotopo(images.flatten(start_dim=1).detach().cpu().numpy())\n",
    "        # print(\"effecEntro: \", effectEntro)\n",
    "\n",
    "        # effecit fisher\n",
    "        # outputs = client_net(images)\n",
    "        inverse_dFIL = Fishermetric.quantify(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01, with_outputs=True)\n",
    "        print(\"inverse_dFIL: \",inverse_dFIL)\n",
    "        effectFisher = computing_det_with_outputs(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01)\n",
    "\n",
    "        # 存储\n",
    "        effecInfo_same_layer_list.append(effectEntro-effectFisher)\n",
    "        InversedFIL_same_layer_list.append(inverse_dFIL)\n",
    "        \n",
    "print(f\"Layer {args['split_layer']} effecInfo: {sum(effecInfo_same_layer_list)/len(effecInfo_same_layer_list)}\")\n",
    "print(f\"Layer {args['split_layer']} InversedFIL: {sum(InversedFIL_same_layer_list)/len(InversedFIL_same_layer_list)}\")\n",
    "effecInfo_diff_layer_list.append(effecInfo_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(effecInfo_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(results_dir + f'effecInfo-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[args['split_layer']]).to_csv(results_dir + f'effecInfo.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# x = next(iter(one_data_loader))\n",
    "for i,_ in one_data_loader:\n",
    "        # print(i)\n",
    "        y = scaler.fit_transform(i)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174.3527964782727\n",
      "335.9202759013189\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(10,341) # 最大能接受341个特征 \n",
    "z = torch.randn(100,200)\n",
    "e1 = Shannon_quantity(y)\n",
    "e2 = Shannon_quantity(z)\n",
    "print(e1)\n",
    "print(e2)\n",
    "# print(np.log(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n",
      "X_train.shape: (32950, 63)\n",
      "X_test.shape: (8238, 63)\n",
      "y_train.shape: (32950, 1)\n",
      "y_test.shape: (8238, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.289148</td>\n",
       "      <td>0.157198</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.063608</td>\n",
       "      <td>0.088614</td>\n",
       "      <td>0.032168</td>\n",
       "      <td>0.078174</td>\n",
       "      <td>0.064093</td>\n",
       "      <td>0.141539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284123</td>\n",
       "      <td>0.054413</td>\n",
       "      <td>0.020963</td>\n",
       "      <td>0.838213</td>\n",
       "      <td>0.081903</td>\n",
       "      <td>0.251712</td>\n",
       "      <td>0.335361</td>\n",
       "      <td>0.453437</td>\n",
       "      <td>0.089942</td>\n",
       "      <td>0.344142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.453394</td>\n",
       "      <td>0.364010</td>\n",
       "      <td>0.152031</td>\n",
       "      <td>0.146202</td>\n",
       "      <td>0.244068</td>\n",
       "      <td>0.284203</td>\n",
       "      <td>0.176457</td>\n",
       "      <td>0.268462</td>\n",
       "      <td>0.244934</td>\n",
       "      <td>0.348598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171524</td>\n",
       "      <td>0.051057</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.366935</td>\n",
       "      <td>0.121449</td>\n",
       "      <td>0.146193</td>\n",
       "      <td>0.252907</td>\n",
       "      <td>0.286649</td>\n",
       "      <td>0.055046</td>\n",
       "      <td>0.179637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160494</td>\n",
       "      <td>0.022977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.199532</td>\n",
       "      <td>0.192469</td>\n",
       "      <td>0.036953</td>\n",
       "      <td>0.203781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234568</td>\n",
       "      <td>0.040057</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.269680</td>\n",
       "      <td>0.418410</td>\n",
       "      <td>0.093403</td>\n",
       "      <td>0.425709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382716</td>\n",
       "      <td>0.068524</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.296960</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.143278</td>\n",
       "      <td>0.512287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769622</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150759</td>\n",
       "      <td>0.512287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  8238.000000  8238.000000  8238.000000  8238.000000  8238.000000   \n",
       "mean      0.289148     0.157198     0.023671     0.021850     0.063608   \n",
       "std       0.453394     0.364010     0.152031     0.146202     0.244068   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  8238.000000  8238.000000  8238.000000  8238.000000  8238.000000  ...   \n",
       "mean      0.088614     0.032168     0.078174     0.064093     0.141539  ...   \n",
       "std       0.284203     0.176457     0.268462     0.244934     0.348598  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "                53           54           55           56           57  \\\n",
       "count  8238.000000  8238.000000  8238.000000  8238.000000  8238.000000   \n",
       "mean      0.284123     0.054413     0.020963     0.838213     0.081903   \n",
       "std       0.171524     0.051057     0.030769     0.366935     0.121449   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.160494     0.022977     0.000000     1.000000     0.000000   \n",
       "50%       0.234568     0.040057     0.018182     1.000000     0.000000   \n",
       "75%       0.382716     0.068524     0.036364     1.000000     0.142857   \n",
       "max       1.000000     0.769622     0.272727     1.000000     1.000000   \n",
       "\n",
       "                58           59           60           61           62  \n",
       "count  8238.000000  8238.000000  8238.000000  8238.000000  8238.000000  \n",
       "mean      0.251712     0.335361     0.453437     0.089942     0.344142  \n",
       "std       0.146193     0.252907     0.286649     0.055046     0.179637  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.104167     0.199532     0.192469     0.036953     0.203781  \n",
       "50%       0.333333     0.269680     0.418410     0.093403     0.425709  \n",
       "75%       0.333333     0.296960     0.719665     0.143278     0.512287  \n",
       "max       0.479167     1.000000     1.000000     0.150759     0.512287  \n",
       "\n",
       "[8 rows x 63 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataPath = '/home/dengruijun/data/FinTech/DATASET/kaggle-dataset/bank/bank-additional-full.csv'\n",
    "\n",
    "[X_train, y_train], [X_test, y_test] = preprocess_bank_dataset(dataPath)\n",
    "df = pd.DataFrame(X_test)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4883,  1.4026, -0.9285,  1.1072,  1.5569],\n",
      "        [ 0.0139,  1.4536, -0.4703, -0.8110, -1.1403],\n",
      "        [-0.8630,  1.2017, -0.5574,  0.4894,  0.1805],\n",
      "        [-1.2771,  0.4857, -0.0949, -0.3659, -0.1998],\n",
      "        [-1.7603, -1.2163,  1.2970,  0.3627,  0.1785],\n",
      "        [-0.0182, -1.6627,  0.2138,  1.2353, -0.1917]])\n",
      "tensor([-1.4883,  1.4536, -0.5574, -0.3659,  0.1785])\n",
      "tensor([[-1.4883,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  1.4536,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.5574,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.3659,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1785]])\n"
     ]
    }
   ],
   "source": [
    "# 测试diagonal\n",
    "import torch\n",
    "x = torch.randn(6,5)\n",
    "y = torch.diagonal(x)\n",
    "z = torch.diag_embed(y)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
