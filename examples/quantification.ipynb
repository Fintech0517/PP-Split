{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明：这个notebook演示了如何使用quantification方法（目前实现了4种方法进行隐私量化）\n",
    "1. dFIL (batchsize = 1)\n",
    "2. distance correlation (batchsize>=2)\n",
    "3. mutual information (batchsize>=8)\n",
    "4. ULoss (batchsize = 1)\n",
    "\n",
    "注意用不同方法的时候要重新设置 批大小 （即args['batch_size']的值）\n",
    "\n",
    "因为在整个测试集上进行隐私量化，时间太长了（可能要跑好几天）所以这里设计了一个get_one_data()函数，取测试集的前k个数据作为一个数据集，batch_size=k,因此只需要迭代一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '48'\n",
    "\n",
    "# 导入各个指标\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.quantification.distance_correlation.distCor import distCorMetric\n",
    "from ppsplit.quantification.fisher_information.dFIL_inverse import dFILInverseMetric\n",
    "from ppsplit.quantification.shannon_information.mutual_information import MuInfoMetric\n",
    "from ppsplit.quantification.shannon_information.ULoss import ULossMetric\n",
    "\n",
    "# 导入各个baseline模型及其数据集预处理方法\n",
    "# 模型\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "from target_model.models.VGG import VGG,VGG5Decoder,model_cfg\n",
    "from target_model.models.BankNet import BankNet1,bank_cfg\n",
    "from target_model.models.CreditNet import CreditNet1,credit_cfg\n",
    "from target_model.models.PurchaseNet import PurchaseClassifier1,purchase_cfg\n",
    "# 数据预处理方法\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize,get_one_data,deprocess\n",
    "from target_model.data_preprocessing.preprocess_bank import bank_dataset,preprocess_bank\n",
    "from target_model.data_preprocessing.preprocess_credit import preprocess_credit\n",
    "from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase\n",
    "\n",
    "# utils\n",
    "from ppsplit.utils.utils import create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# 基本参数：\n",
    "# 硬件\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 参数\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', type = str, default = 'CIFAR10')\n",
    "# parser.add_argument('--device', type = str, default = 'cuda:1')\n",
    "# parser.add_argument('--batch_size',type=int, default=1) # muinfo最小为8，# distcor最小为2\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = {\n",
    "        'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        # 'device':torch.device(\"cpu\"),\n",
    "        'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        'batch_size':5,\n",
    "        'noise_scale':1000, # 防护措施\n",
    "        }\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集及其模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "features.5.weight\n",
      "features.5.bias\n",
      "features.5.running_mean\n",
      "features.5.running_var\n",
      "features.5.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "# 加载模型和数据集，并从unit模型中切割出client_model\n",
    "if args['dataset']=='CIFAR10':\n",
    "    # 超参数\n",
    "    testset_len = 10000 # 10000个数据一次 整个测试集合的长度\n",
    "    # split_layer_list = list(range(len(model_cfg['VGG5'])))\n",
    "    split_layer = 2 # 定成3吧？\n",
    "    test_num = 2 # 试验序号\n",
    "\n",
    "    # 关键路径\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/VGG5/BN+Tanh/VGG5-params-20ep.pth' # VGG5-BN+Tanh # 存储的是模型参数，不包括模型结构\n",
    "    results_dir  = f'../results/InvMetric-202403/VGG5/quantification/{test_num}/'\n",
    "    decoder_route = f\"../results/InvMetric-202403/VGG5/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    trainloader,testloader = get_cifar10_normalize(batch_size = 5)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # 切割成client model\n",
    "    # vgg5_unit.load_state_dict(torch.load(unit_net_route,map_location=torch.device('cpu'))) # 完整的模型\n",
    "    client_net = VGG('Client','VGG5',split_layer,model_cfg,noise_scale=args['noise_scale'])\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(model_cfg['VGG5']):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "elif args['dataset']=='bank':\n",
    "    # 超参数\n",
    "    test_num = 1 # 试验序号\n",
    "    testset_len=8238\n",
    "    # split_layer_list = ['linear1', 'linear2']\n",
    "    split_layer_list = [0,2,4,6]\n",
    "    split_layer = 2\n",
    "\n",
    "    # 关键路径\n",
    "    results_dir  = f'../results/InvMetric-202403/Bank/quantification/{test_num}/'\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Bank/bank-20ep_params.pth'\n",
    "    decoder_route = f\"../results/InvMetric-202403/Bank/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    trainloader,testloader = preprocess_bank(batch_size=1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据 \n",
    "\n",
    "    # 模型加载\n",
    "    client_net = BankNet1(layer=split_layer,noise_scale=args['noise_scale'])\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(bank_cfg):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "elif args['dataset']=='credit':\n",
    "    # 超参数\n",
    "    test_num = 1 # 试验序号\n",
    "    testset_len = 61503 # for the mutual information\n",
    "    split_layer_list = [0,3,6,9]\n",
    "    split_layer = 3\n",
    "    # split_layer_list = ['linear1', 'linear2']\n",
    "\n",
    "    # 关键路径\n",
    "    results_dir  = f'../results/InvMetric-202403/Credit/quantification/{test_num}/'\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/credit/credit-20ep_params.pth'\n",
    "    decoder_route = f\"../results/InvMetric-202403/Credit/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    trainloader,testloader = preprocess_credit(batch_size=1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # client模型切割加载\n",
    "    client_net = CreditNet1(layer=split_layer,noise_scale=args['noise_scale'])\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(credit_cfg):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "elif args['dataset']=='purchase':\n",
    "    # 超参数\n",
    "    test_num = 1 # 试验序号\n",
    "    testset_len = 39465 # test len\n",
    "    # split_layer_list = [0,1,2,3,4,5,6,7,8]\n",
    "    split_layer = 3\n",
    "\n",
    "    # 关键路径\n",
    "    results_dir = f'../results/InvMetric-202403/Purchase/quantification/{test_num}/'\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Purchase100/Purchase_bestmodel_param.pth'\n",
    "    decoder_route = f\"../results/InvMetric-202403/Purchase/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "    \n",
    "    # 数据集加载\n",
    "    trainloader,testloader = preprocess_purchase(batch_size=1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # 模型加载\n",
    "    client_net = PurchaseClassifier1(layer=split_layer,noise_scale=args['noise_scale'])\n",
    "    # pweights = torch.load(unit_net_route,map_location=torch.device('cpu'))\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(purchase_cfg):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "else:\n",
    "    exit(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Tanh()\n",
       "  )\n",
       "  (denses): Sequential()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建文件夹\n",
    "create_dir(results_dir)\n",
    "\n",
    "# client_net使用\n",
    "client_net = client_net.to(args['device'])\n",
    "client_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各种指标计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.dFIL-inverse\n",
    "注意：batchsize 需要等于1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# dFIL inverse指标计算\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric = dFILInverseMetric()\n",
    "# 对traingloader遍历计算所有 inverse dFIL\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)):\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    inputs.requires_grad_(True) # 需要求导\n",
    "    \n",
    "    # inference\n",
    "    outputs = client_net(inputs)\n",
    "\n",
    "    eta = metric.quantify(model=client_net, inputs=inputs, outputs=outputs, with_outputs=True)\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(save_img_dir + f'dFIL-1.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'dFIL.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. distance correlation\n",
    "注意：batchsize >=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:11<00:00, 167.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2 Avg distCorr: 0.9745437641441822\n"
     ]
    }
   ],
   "source": [
    "# distance correlation指标计算\n",
    "\n",
    "\n",
    "distCorr_diff_layer_list = []\n",
    "distCorr_same_layer_list = []\n",
    "metric = distCorMetric()\n",
    "\n",
    "for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "# for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    tab, labels = data\n",
    "    tab, labels = tab.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        pred = client_net(tab).cpu().detach()\n",
    "        inputs = tab.cpu().detach()\n",
    "\n",
    "        distCorr = metric.quantify(inputs=inputs,outputs=pred) # x,z\n",
    "        distCorr_same_layer_list.append(distCorr)\n",
    "\n",
    "\n",
    "print(f\"Layer {split_layer} Avg distCorr: {sum(distCorr_same_layer_list)/len(distCorr_same_layer_list)}\")\n",
    "distCorr_diff_layer_list.append(distCorr_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(distCorr_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in range (len(split_layer_list))]).to_csv(save_img_dir + f'DLoss-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'DLoss.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. mutual information\n",
    "注意：batchsize>=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'one_data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m metric \u001b[38;5;241m=\u001b[39m MuInfoMetric()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[43mone_data_loader\u001b[49m)): \u001b[38;5;66;03m# 测试第一个testloader\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     10\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]), labels\u001b[38;5;241m.\u001b[39mto(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'one_data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# mutual information指标计算\n",
    "\n",
    "MI_diff_layer_list = []\n",
    "MI_same_layer_list = []\n",
    "metric = MuInfoMetric()\n",
    "\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        inputs = images.cpu().detach()\n",
    "        mi = metric.quantify(inputs=inputs, outputs = outputs)\n",
    "        MI_same_layer_list.append(mi)\n",
    "        \n",
    "print(f\"Layer {split_layer} MI: {sum(MI_same_layer_list)/len(MI_same_layer_list)}\")\n",
    "MI_diff_layer_list.append(MI_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(MI_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(results_dir + f'MI-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'MILoss.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Uncertainty Loss\n",
    "注意：batchsize=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3 ULoss: 4.021282423374509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# mutual information指标计算\n",
    "\n",
    "\n",
    "ULoss_diff_layer_list = []\n",
    "ULoss_same_layer_list = []\n",
    "metric = ULossMetric()\n",
    "decoder_net = torch.load(decoder_route)\n",
    "decoder_net.to(args['device'])\n",
    "decoder_net.eval()\n",
    "\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        uloss = metric.quantify(output = outputs, decoder_net=decoder_net)\n",
    "        ULoss_same_layer_list.append(uloss)\n",
    "        \n",
    "print(f\"Layer {split_layer} ULoss: {sum(ULoss_same_layer_list)/len(ULoss_same_layer_list)}\")\n",
    "ULoss_diff_layer_list.append(ULoss_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(ULoss_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(results_dir + f'ULoss-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'ULoss.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drj-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
