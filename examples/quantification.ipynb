{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：这个notebook演示了如何使用quantification方法（目前实现了3种方法进行隐私量化）\n",
    "# dFIL\n",
    "# distance correlation\n",
    "# mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '48'\n",
    "\n",
    "# 导入各个指标\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.quantification.distance_correlation.distCor import distCorMetric\n",
    "from ppsplit.quantification.fisher_information.dFIL_inverse import dFILInverseMetric\n",
    "from ppsplit.quantification.shannon_information.mutual_information import MuInfoMetric\n",
    "\n",
    "# 导入各个baseline模型及其数据集预处理方法\n",
    "# 模型\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "from target_model.models.VGG import VGG,VGG5Decoder,model_cfg\n",
    "from target_model.models.BankNet import BankNet1\n",
    "from target_model.models.CreditNet import CreditNet1\n",
    "from target_model.models.PurchaseNet import PurchaseClassifier1\n",
    "# 数据预处理方法\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize,get_one_data,deprocess\n",
    "from target_model.data_preprocessing.preprocess_bank import bank_dataset,preprocess_bank\n",
    "from target_model.data_preprocessing.preprocess_credit import preprocess_credit\n",
    "from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 基本参数：\n",
    "# 硬件\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 参数\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', type = str, default = 'CIFAR10')\n",
    "# parser.add_argument('--device', type = str, default = 'cuda:1')\n",
    "# parser.add_argument('--batch_size',type=int, default=1) # muinfo最小为8，# distcor最小为2\n",
    "# args = parser.parse_args()\n",
    "args = {'dataset':'CIFAR10',\n",
    "        # 'device':torch.device(\"cpu\"),\n",
    "        'device':torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'batch_size':1}\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集及其模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "features.5.weight\n",
      "features.5.bias\n",
      "features.5.running_mean\n",
      "features.5.running_var\n",
      "features.5.num_batches_tracked\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Tanh()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (denses): Sequential()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型和数据集，并从unit模型中切割出client_model\n",
    "if args['dataset']=='CIFAR10':\n",
    "    save_inverse_dir  = f'../results/VGG5/'\n",
    "    testset_len = 10000 # 10000个数据一次\n",
    "    trainloader,testloader = get_cifar10_normalize(batch_size = args['batch_size'])\n",
    "    one_data_loader = get_one_data(testloader,batch_size = 8) #拿到第一个测试数据\n",
    "\n",
    "    client_net_route = '/home/dengruijun/data/project/Inverse_efficacy/trained_models/VGG5/BN+Tanh/VGG5-20ep.pth' # VGG5-BN+Tanh # 存储的是模型参数，不包括模型结构\n",
    "    # VGG5 unit模型\n",
    "    # vgg5_unit = VGG('Unit', 'VGG5', len(model_cfg['VGG5'])-1, model_cfg) # 加载模型结构\n",
    "    # vgg5_unit.load_state_dict(torch.load(client_net_route)) # 加载模型参数\n",
    "    # vgg5_unit.load_state_dict(torch.load(client_net_route,map_location=torch.device('cpu'))) # 完整的模型\n",
    "    split_layer_list = list(range(len(model_cfg['VGG5'])))\n",
    "\n",
    "    # 切割成client model\n",
    "    split_layer = 3\n",
    "    client_net = VGG('Client','VGG5',split_layer,model_cfg)\n",
    "    pweights = torch.load(client_net_route)\n",
    "    if split_layer < len(model_cfg['VGG5']):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "elif args.dataset=='credit':\n",
    "    save_inverse_dir  = f'../results/Credit/'\n",
    "\n",
    "    testset_len = 61503 # for the mutual information\n",
    "\n",
    "    client_net_route = '../results/1-7/credit-20ep.pth'\n",
    "    dataPath = '/home/dengruijun/data/FinTech/DATASET/kaggle-dataset/home_credit/dataset/application_train.csv'\n",
    "    train_data, test_data = preprocess_credit(dataPath)\n",
    "    test_dataset = bank_dataset(test_data)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False,\n",
    "                                            num_workers=8, drop_last=False)\n",
    "    # one_data_loader = get_one_data(testloader,batch_size = batch_size) #拿到第一个测试数据\n",
    "    # split_layer_list = ['linear1', 'linear2']\n",
    "    split_layer_list = [0,3,6,9]\n",
    "    split_layer = 3\n",
    "\n",
    "    client_net = CreditNet1(layer=split_layer)\n",
    "    pweights = torch.load('../results/1-7/credit-20ep.pth').state_dict()\n",
    "    if split_layer < 9:\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "elif args['dataset']=='bank':\n",
    "    save_inverse_dir  = f'../results/Bank/MI/'\n",
    "\n",
    "    testset_len=8238\n",
    "\n",
    "    client_net_route = '../results/1-8/bank-20ep.pth'\n",
    "    dataPath = '/home/dengruijun/data/FinTech/DATASET/kaggle-dataset/bank/bank-additional-full.csv'\n",
    "    \n",
    "    train_data, test_data = preprocess_bank(dataPath)\n",
    "    test_dataset = bank_dataset(test_data)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False,\n",
    "                                            num_workers=8, drop_last=False)\n",
    "    # one_data_loader = get_one_data(testloader,batch_size = batch_size) #拿到第一个测试数据\n",
    "    # split_layer_list = ['linear1', 'linear2']\n",
    "    split_layer_list = [0,2,4,6]\n",
    "    split_layer = 2\n",
    "\n",
    "    client_net = BankNet1(layer=split_layer)\n",
    "    pweights = torch.load('../results/1-8/bank-20ep.pth').state_dict()\n",
    "    if split_layer < 6:\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "elif args['dataset']=='purchase':\n",
    "    save_inverse_dir = f'../results/Purchase100/MI/'\n",
    "\n",
    "    testset_len = 39465 # test len\n",
    "    client_net_route = '../results/1-9/epoch_train0.pth'\n",
    "    dataPath = '/home/dengruijun/data/FinTech/DATASET/kaggle-dataset/Purchase100/'\n",
    "\n",
    "    trainloader, testloader = preprocess_purchase(data_path=dataPath, batch_size=args['batch_size'])\n",
    "    # one_data_loader = get_one_data(testloader,batch_size = 1) #拿到第一个测试数据\n",
    "    split_layer_list = [0,1,2,3,4,5,6,7,8]\n",
    "    split_layer = 2\n",
    "\n",
    "    # 读取（load）模型\n",
    "    client_net = PurchaseClassifier1(layer=split_layer)\n",
    "    pweights  = torch.load(client_net_route)['state_dict']\n",
    "    if split_layer < 8: # \n",
    "        pweights = split_weights_client(pweights ,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "else:\n",
    "    exit(-1)\n",
    "\n",
    "client_net = client_net.to(args['device'])\n",
    "client_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各种指标计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.dFIL-inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8, 32768, 24576]' is invalid for input of size 805306368",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# inference\u001b[39;00m\n\u001b[1;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m client_net(inputs)\n\u001b[0;32m---> 19\u001b[0m eta \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 打印\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(str(j)+\": \"+str(eta.item()))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m eta_same_layer_list\u001b[38;5;241m.\u001b[39mappend(eta)\n",
      "File \u001b[0;32m~/data/FinTech/PP-Split/ppsplit/quantification/fisher_information/dFIL_inverse.py:26\u001b[0m, in \u001b[0;36mdFILInverseMetric.quantify\u001b[0;34m(self, model, inputs, outputs, sigmas, with_outputs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantify\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, sigmas\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, with_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m with_outputs:\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_computing_eta_with_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computing_eta_without_outputs(model, inputs,  sigmas)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/data/FinTech/PP-Split/ppsplit/quantification/fisher_information/dFIL_inverse.py:60\u001b[0m, in \u001b[0;36mdFILInverseMetric._computing_eta_with_outputs\u001b[0;34m(self, model, inputs, outputs, sigmas)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_computing_eta_with_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, outputs, sigmas): \u001b[38;5;66;03m# sigma_square\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# outputs = outputs + sigma * torch.randn_like(outputs) # 加噪声 (0,1] uniform\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# 计算jacobian\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     J \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mjacobian(model, inputs)\n\u001b[0;32m---> 60\u001b[0m     J \u001b[38;5;241m=\u001b[39m \u001b[43mJ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (batch, out_size, in_size)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# 计算eta\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     I \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m(sigmas)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmatmul(J[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mt(), J[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[8, 32768, 24576]' is invalid for input of size 805306368"
     ]
    }
   ],
   "source": [
    "# dFIL inverse指标计算\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric = dFILInverseMetric()\n",
    "# 对traingloader遍历计算所有 inverse dFIL\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)):\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    inputs.requires_grad_(True) # 需要求导\n",
    "    \n",
    "    # inference\n",
    "    outputs = client_net(inputs)\n",
    "\n",
    "    eta = metric.quantify(model=client_net, inputs=inputs, outputs=outputs, with_outputs=True)\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(save_img_dir + f'dFIL-1.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(save_inverse_dir + f'dFIL.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. distance correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3 Avg distCorr: 0.9707143902778625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# distance correlation指标计算\n",
    "\n",
    "\n",
    "distCorr_diff_layer_list = []\n",
    "distCorr_same_layer_list = []\n",
    "metric = distCorMetric()\n",
    "\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    tab, labels = data\n",
    "    tab, labels = tab.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        pred = client_net(tab).cpu().detach()\n",
    "        inputs = tab.cpu().detach()\n",
    "\n",
    "        distCorr = metric.quantify(inputs=inputs,outputs=pred) # x,z\n",
    "        distCorr_same_layer_list.append(distCorr)\n",
    "\n",
    "\n",
    "print(f\"Layer {split_layer} Avg distCorr: {sum(distCorr_same_layer_list)/len(distCorr_same_layer_list)}\")\n",
    "distCorr_diff_layer_list.append(distCorr_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(distCorr_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in range (len(split_layer_list))]).to_csv(save_img_dir + f'DLoss-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(save_inverse_dir + f'DLoss.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3 MI: -97.71402831489374\n"
     ]
    }
   ],
   "source": [
    "# mutual information指标计算\n",
    "\n",
    "MI_diff_layer_list = []\n",
    "MI_same_layer_list = []\n",
    "metric = MuInfoMetric()\n",
    "\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        inputs = images.cpu().detach()\n",
    "        mi = metric.quantify(inputs=inputs, outputs = outputs)\n",
    "        MI_same_layer_list.append(mi)\n",
    "        \n",
    "print(f\"Layer {split_layer} MI: {sum(MI_same_layer_list)/len(MI_same_layer_list)}\")\n",
    "MI_diff_layer_list.append(MI_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(MI_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(save_inverse_dir + f'MI-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(save_inverse_dir + f'MILoss.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drj-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
