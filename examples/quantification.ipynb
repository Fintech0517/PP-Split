{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明：这个notebook演示了如何使用quantification方法（目前实现了4种方法进行隐私量化）\n",
    "1. dFIL (batchsize = 1)\n",
    "2. distance correlation (batchsize>=2)\n",
    "3. mutual information (batchsize>=8)\n",
    "4. ULoss (batchsize = 1)\n",
    "\n",
    "注意用不同方法的时候要重新设置 批大小 （即args['batch_size']的值）\n",
    "\n",
    "因为在整个测试集上进行隐私量化，时间太长了（可能要跑好几天）所以这里设计了一个get_one_data()函数，取测试集的前k个数据作为一个数据集，batch_size=k,因此只需要迭代一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '48'\n",
    "\n",
    "# 导入各个指标\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.quantification.distance_correlation.distCor import distCorMetric\n",
    "from ppsplit.quantification.fisher_information.dFIL_inverse import dFILInverseMetric\n",
    "from ppsplit.quantification.shannon_information.mutual_information import MuInfoMetric\n",
    "from ppsplit.quantification.shannon_information.ULoss import ULossMetric\n",
    "from ppsplit.quantification.rep_reading.rep_reader import PCA_Reader\n",
    "\n",
    "# 导入各个baseline模型及其数据集预处理方法\n",
    "# 模型\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "from target_model.models.VGG import VGG,VGG5Decoder,model_cfg\n",
    "from target_model.models.BankNet import BankNet1,bank_cfg\n",
    "from target_model.models.CreditNet import CreditNet1,credit_cfg\n",
    "from target_model.models.PurchaseNet import PurchaseClassifier1,purchase_cfg\n",
    "# 数据预处理方法\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize,get_one_data,deprocess\n",
    "from target_model.data_preprocessing.preprocess_bank import bank_dataset,preprocess_bank\n",
    "from target_model.data_preprocessing.preprocess_credit import preprocess_credit\n",
    "from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase\n",
    "\n",
    "# utils\n",
    "from ppsplit.utils.utils import create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# 基本参数：\n",
    "# 硬件\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 参数\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', type = str, default = 'CIFAR10')\n",
    "# parser.add_argument('--device', type = str, default = 'cuda:1')\n",
    "# parser.add_argument('--batch_size',type=int, default=1) # muinfo最小为8，# distcor最小为2\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = {\n",
    "        'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        # 'device':torch.device(\"cpu\"),\n",
    "        'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        # 'result_dir': 'InvMetric-202403',\n",
    "        'result_dir': '20240428-Rep-quantify/',\n",
    "        'batch_size':5,\n",
    "        'noise_scale':0, # 防护措施\n",
    "        'num_pairs': 10000, # RepE\n",
    "        }\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集及其模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "features.5.weight\n",
      "features.5.bias\n",
      "features.5.running_mean\n",
      "features.5.running_var\n",
      "features.5.num_batches_tracked\n",
      "features.8.weight\n",
      "features.8.bias\n",
      "features.9.weight\n",
      "features.9.bias\n",
      "features.9.running_mean\n",
      "features.9.running_var\n",
      "features.9.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "# 加载模型和数据集，并从unit模型中切割出client_model\n",
    "if args['dataset']=='CIFAR10':\n",
    "    # 超参数\n",
    "    testset_len = 10000 # 10000个数据一次 整个测试集合的长度\n",
    "    # split_layer_list = list(range(len(model_cfg['VGG5'])))\n",
    "    split_layer = 6 # 定成3吧？\n",
    "    test_num = 1 # 试验序号\n",
    "\n",
    "    # 关键路径\n",
    "    # 此时是为了repE\n",
    "    # unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/VGG5/BN+Tanh/VGG5-params-20ep.pth' # VGG5-BN+Tanh # 存储的是模型参数，不包括模型结构\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/VGG5/20240429-RepE/VGG5-params-19ep.pth' # VGG5-BN+Tanh # 存储的是模型参数，不包括模型结构\n",
    "    results_dir  = f\"../results/{args['result_dir']}/VGG5/quantification/{test_num}/\"\n",
    "    decoder_route = f\"../results/{args['result_dir']}/VGG5/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    trainloader,testloader = get_cifar10_normalize(batch_size = 5)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # 切割成client model\n",
    "    # vgg5_unit.load_state_dict(torch.load(unit_net_route,map_location=torch.device('cpu'))) # 完整的模型\n",
    "    client_net = VGG('Client','VGG5',split_layer,model_cfg,noise_scale=args['noise_scale'])\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(model_cfg['VGG5']):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "elif args['dataset']=='bank':\n",
    "    # 超参数\n",
    "    test_num = 1 # 试验序号\n",
    "    testset_len=8238\n",
    "    # split_layer_list = ['linear1', 'linear2']\n",
    "    split_layer_list = [0,2,4,6]\n",
    "    split_layer = 2\n",
    "\n",
    "    # 关键路径\n",
    "    results_dir  = f\"../results/{args['result_dir']}/Bank/quantification/{test_num}/\"\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Bank/bank-20ep_params.pth'\n",
    "    decoder_route = f\"../results/{args['result_dir']}/Bank/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    trainloader,testloader = preprocess_bank(batch_size=1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据 \n",
    "\n",
    "    # 模型加载\n",
    "    client_net = BankNet1(layer=split_layer,noise_scale=args['noise_scale'])\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(bank_cfg):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "elif args['dataset']=='credit':\n",
    "    # 超参数\n",
    "    test_num = 1 # 试验序号\n",
    "    testset_len = 61503 # for the mutual information\n",
    "    split_layer_list = [0,3,6,9]\n",
    "    split_layer = 3\n",
    "    # split_layer_list = ['linear1', 'linear2']\n",
    "\n",
    "    # 关键路径\n",
    "    results_dir  = f\"../results/{args['result_dir']}/Credit/quantification/{test_num}/\"\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/credit/credit-20ep_params.pth'\n",
    "    decoder_route = f\"../results/{args['result_dir']}/Credit/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    trainloader,testloader = preprocess_credit(batch_size=1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # client模型切割加载\n",
    "    client_net = CreditNet1(layer=split_layer,noise_scale=args['noise_scale'])\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(credit_cfg):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "elif args['dataset']=='purchase':\n",
    "    # 超参数\n",
    "    test_num = 1 # 试验序号\n",
    "    testset_len = 39465 # test len\n",
    "    # split_layer_list = [0,1,2,3,4,5,6,7,8]\n",
    "    split_layer = 3\n",
    "\n",
    "    # 关键路径\n",
    "    results_dir = f\"../results/{args['result_dir']}/Purchase/quantification/{test_num}/\"\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Purchase100/Purchase_bestmodel_param.pth'\n",
    "    decoder_route = f\"../results/{args['result_dir']}/Purchase/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "    \n",
    "    # 数据集加载\n",
    "    trainloader,testloader = preprocess_purchase(batch_size=1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # 模型加载\n",
    "    client_net = PurchaseClassifier1(layer=split_layer,noise_scale=args['noise_scale'])\n",
    "    # pweights = torch.load(unit_net_route,map_location=torch.device('cpu'))\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(purchase_cfg):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "else:\n",
    "    exit(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Tanh()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Tanh()\n",
       "  )\n",
       "  (denses): Sequential()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建文件夹\n",
    "create_dir(results_dir)\n",
    "\n",
    "# client_net使用\n",
    "client_net = client_net.to(args['device'])\n",
    "client_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各种指标计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.dFIL-inverse\n",
    "注意：batchsize 需要等于1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [5, 5, 3, 32, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# 需要求导\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# inference\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mclient_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m eta \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mquantify(model\u001b[38;5;241m=\u001b[39mclient_net, inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs, with_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 打印\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(str(j)+\": \"+str(eta.item()))\u001b[39;00m\n",
      "File \u001b[0;32m/data/other/anaconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/dengruijun/FinTech/PP-Split/target_model/models/VGG.py:111\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    110\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 111\u001b[0m \t\tout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \t\u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m \t\tout \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m/data/other/anaconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/other/anaconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/data/other/anaconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/other/anaconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/other/anaconda3/envs/drj-pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [5, 5, 3, 32, 32]"
     ]
    }
   ],
   "source": [
    "# dFIL inverse指标计算\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric = dFILInverseMetric()\n",
    "# 对traingloader遍历计算所有 inverse dFIL\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)):\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    inputs.requires_grad_(True) # 需要求导\n",
    "    \n",
    "    # inference\n",
    "    outputs = client_net(inputs)\n",
    "\n",
    "    eta = metric.quantify(model=client_net, inputs=inputs, outputs=outputs, with_outputs=True)\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(save_img_dir + f'dFIL-1.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'dFIL.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. distance correlation\n",
    "注意：batchsize >=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:11<00:00, 167.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2 Avg distCorr: 0.9745437641441822\n"
     ]
    }
   ],
   "source": [
    "# distance correlation指标计算\n",
    "\n",
    "\n",
    "distCorr_diff_layer_list = []\n",
    "distCorr_same_layer_list = []\n",
    "metric = distCorMetric()\n",
    "\n",
    "for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "# for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    tab, labels = data\n",
    "    tab, labels = tab.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        pred = client_net(tab).cpu().detach()\n",
    "        inputs = tab.cpu().detach()\n",
    "\n",
    "        distCorr = metric.quantify(inputs=inputs,outputs=pred) # x,z\n",
    "        distCorr_same_layer_list.append(distCorr)\n",
    "\n",
    "\n",
    "print(f\"Layer {split_layer} Avg distCorr: {sum(distCorr_same_layer_list)/len(distCorr_same_layer_list)}\")\n",
    "distCorr_diff_layer_list.append(distCorr_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(distCorr_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in range (len(split_layer_list))]).to_csv(save_img_dir + f'DLoss-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'DLoss.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. mutual information\n",
    "注意：batchsize>=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'one_data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m metric \u001b[38;5;241m=\u001b[39m MuInfoMetric()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[43mone_data_loader\u001b[49m)): \u001b[38;5;66;03m# 测试第一个testloader\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     10\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]), labels\u001b[38;5;241m.\u001b[39mto(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'one_data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# mutual information指标计算\n",
    "\n",
    "MI_diff_layer_list = []\n",
    "MI_same_layer_list = []\n",
    "metric = MuInfoMetric()\n",
    "\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        inputs = images.cpu().detach()\n",
    "        mi = metric.quantify(inputs=inputs, outputs = outputs)\n",
    "        MI_same_layer_list.append(mi)\n",
    "        \n",
    "print(f\"Layer {split_layer} MI: {sum(MI_same_layer_list)/len(MI_same_layer_list)}\")\n",
    "MI_diff_layer_list.append(MI_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(MI_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(results_dir + f'MI-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'MILoss.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Uncertainty Loss\n",
    "注意：batchsize=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3 ULoss: 4.021282423374509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# mutual information指标计算\n",
    "\n",
    "\n",
    "ULoss_diff_layer_list = []\n",
    "ULoss_same_layer_list = []\n",
    "metric = ULossMetric()\n",
    "decoder_net = torch.load(decoder_route)\n",
    "decoder_net.to(args['device'])\n",
    "decoder_net.eval()\n",
    "\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        uloss = metric.quantify(output = outputs, decoder_net=decoder_net)\n",
    "        ULoss_same_layer_list.append(uloss)\n",
    "        \n",
    "print(f\"Layer {split_layer} ULoss: {sum(ULoss_same_layer_list)/len(ULoss_same_layer_list)}\")\n",
    "ULoss_diff_layer_list.append(ULoss_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(ULoss_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(results_dir + f'ULoss-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'ULoss.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RepE Reader\n",
    "无所谓bs\n",
    "\n",
    "1 格式化检查\n",
    "\n",
    "2 实例化一个finder\n",
    "\n",
    "3 模型推理数据得到hidden state（中途有些处理）\n",
    "\n",
    "4 训练pca得到direction\n",
    "\n",
    "5 转换，nparray转换成浮点数\n",
    "\n",
    "6 如果有train label，就get sign一下\n",
    "\n",
    "评估维度性，对label的贡献（偏泄漏隐私，还是偏不泄漏隐私）\n",
    "\n",
    "你directionality 反应的你对于泄漏方向的偏移，和真实的标签？   \n",
    "\n",
    "7 分析测试数据的隐私泄漏程度（在每一层的隐私泄漏程度）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "from target_model.data_preprocessing.dataset import pair_smashed_data,diff_pair_data\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize_two_train\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading paired dataset from ../results/20240428-Rep-quantify//VGG5/quantification/200pairs/\n",
      "[False, True]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 1. designing stimulus and test\n",
    "dataset_route = f\"../results/{args['result_dir']}/VGG5/quantification/{args['num_pairs']}pairs/\"\n",
    "if os.path.isfile(dataset_route+'train_feature.pkl'): # 直接加载预处理好的数据集\n",
    "    print(f\"=> loading paired dataset from {dataset_route}\")\n",
    "    with open(dataset_route+'train_feature.pkl','rb') as f:\n",
    "        train_feature = pickle.load(file=f)       \n",
    "    with open(dataset_route+'test_feature.pkl','rb') as f:\n",
    "        test_feature=pickle.load(file=f)   \n",
    "    with open(dataset_route+'train_label.pkl','rb') as f:                                                       \n",
    "        train_labels=pickle.load(file=f)      \n",
    "    with open(dataset_route+'test_label.pkl', 'rb') as f:                                                    \n",
    "        test_labels=pickle.load(file=f)     \n",
    "    train_loader= DataLoader(train_feature,shuffle=False,batch_size=1)\n",
    "    test_loader = DataLoader(test_feature,shuffle=False,batch_size=1)       \n",
    "# if False:\n",
    "#     pass\n",
    "else: # 进行预处理并存储\n",
    "    seen_loader,unseen_loader,_ = get_cifar10_normalize_two_train(batch_size=1)\n",
    "\n",
    "    train_loader,train_labels,test_loader,test_labels = pair_smashed_data(seen_loader,\n",
    "                                                                        unseen_loader,\n",
    "                                                                        num_pairs=args['num_pairs'])\n",
    "    create_dir(dataset_route)\n",
    "    with open(dataset_route+'train_feature.pkl','wb') as f:\n",
    "        pickle.dump(obj=train_loader.dataset,file=f)       \n",
    "    with open(dataset_route+'test_feature.pkl','wb') as f:\n",
    "        pickle.dump(obj=test_loader.dataset,file=f)   \n",
    "    with open(dataset_route+'train_label.pkl','wb') as f:                                                       \n",
    "        pickle.dump(obj=train_labels,file=f)      \n",
    "    with open(dataset_route+'test_label.pkl', 'wb') as f:                                                    \n",
    "        pickle.dump(obj=test_labels,file=f)                                                          \n",
    "\n",
    "print(train_labels[0])\n",
    "print(test_labels[0].index(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 1024.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9997, 0.9995, 0.9995], device='cuda:1')\n",
      "diff_data.shape:  torch.Size([200, 3])\n"
     ]
    }
   ],
   "source": [
    "# 2. collecting neural activity\n",
    "\n",
    "# #Picking the top X probabilities \n",
    "def clipDataTopX(dataToClip, top=3):\n",
    "    sorted_indices = torch.argsort(dataToClip,dim=1,descending=True)[:,:3]\n",
    "    new_data = torch.gather(dataToClip,1,sorted_indices)\n",
    "    \n",
    "\t# res = [sorted(s, reverse=True)[0:top] for s in dataToClip ]\n",
    "\t# return np.array(res)\n",
    "    # print(new_data[0])\n",
    "    return new_data\n",
    "\n",
    "# 收集所有smashed data\n",
    "train_smashed_data_list = []\n",
    "i = 1\n",
    "for j, data in enumerate(tqdm.tqdm(train_loader)): # 对trainloader遍历\n",
    "    # print(\"data: \", len(data))\n",
    "    features=data.to(args['device'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = client_net(features)\n",
    "        # pred_topk = sorted(pred, reverse=True)[0:5]\n",
    "        # train_smashed_data_list.append(pred)\n",
    "        train_smashed_data_list.append(pred)\n",
    "\n",
    "train_smashed_data_list=torch.stack(train_smashed_data_list).squeeze()\n",
    "# 拉成 [batchsize, vectorsize]的二维矩阵\n",
    "train_smashed_data_list=train_smashed_data_list.reshape(train_smashed_data_list.shape[0],-1)\n",
    "train_smashed_data_list = clipDataTopX(train_smashed_data_list,top=10)\n",
    "# 相对距离\n",
    "diff_data = diff_pair_data(train_smashed_data_list) # np.array\n",
    "print(\"diff_data.shape: \", diff_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in recenter: \n",
      "torch.Size([200, 3])\n",
      "torch.Size([1, 3])\n",
      "in recenter: \n",
      "torch.Size([400, 3])\n",
      "torch.Size([1, 3])\n",
      "direction shape of first layer:  (1, 3)\n",
      "signs of first layer:  [1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. constructing a linear model\n",
    "# 训练direction finder\n",
    "reader = PCA_Reader(n_components=1) # 要的是numpy数据？可以要tensor数据\n",
    "# diff_data = diff_data.reshape(diff_data.shape[0],-1)\n",
    "directions = reader.get_rep_direction(diff_data)\n",
    "signs = reader.get_sign(hidden_states=train_smashed_data_list,train_labels=train_labels)\n",
    "print('direction shape of first layer: ', reader.direction.shape)\n",
    "print('signs of first layer: ', reader.direction_signs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 998.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000], device='cuda:1')\n",
      "in recenter: \n",
      "torch.Size([400, 3])\n",
      "torch.Size([1, 3])\n",
      "[tensor(-1.6736), tensor(-1.6736)]\n",
      "quantified accuracy(privacy lekage): 0.495 \n"
     ]
    }
   ],
   "source": [
    "# 4. 测试\n",
    "test_smashed_data_list = []\n",
    "for j, data in enumerate(tqdm.tqdm(test_loader)): # 对trainloader遍历\n",
    "    features=data.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        pred = client_net(features)\n",
    "        test_smashed_data_list.append(pred)\n",
    "\n",
    "test_smashed_data_list=torch.stack(test_smashed_data_list).squeeze()\n",
    "test_smashed_data_list=test_smashed_data_list.reshape(test_smashed_data_list.shape[0],-1)\n",
    "test_smashed_data_list = clipDataTopX(test_smashed_data_list,top=10)\n",
    "acc = reader.quantify_acc(hidden_states=test_smashed_data_list,test_labels=test_labels)\n",
    "print(f\"quantified accuracy(privacy lekage): {acc} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([1., 2.]), tensor([3., 4.])], [tensor([5., 6.]), tensor([7., 8.])]]\n",
      "[tensor([[1., 2.],\n",
      "        [3., 4.]]), tensor([[5., 6.],\n",
      "        [7., 8.]])]\n",
      "[tensor([1., 2.]), tensor([3., 4.]), tensor([5., 6.]), tensor([7., 8.])]\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([False, False])\n",
      "tensor([[1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = [torch.Tensor([1,2]),torch.Tensor([3,4])]\n",
    "y = [torch.Tensor([5,6]),torch.Tensor([7,8])]\n",
    "l = [x,y]\n",
    "print(l)\n",
    "l1 = [torch.stack(i) for i in l]\n",
    "print(l1)\n",
    "l2 = [item for sublist in l for item in sublist]\n",
    "print(l2)\n",
    "\n",
    "# list[tensor]转tensor\n",
    "tensor_list = [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6]), torch.tensor([7, 8, 9])]\n",
    "tensor_stack = torch.stack(tensor_list)\n",
    "print(tensor_stack)\n",
    "\n",
    "# 看两个tensor比较\n",
    "x = torch.Tensor([1,2])\n",
    "y = torch.Tensor([3,4])\n",
    "print(x==y)\n",
    "x.reshape(1,-1)\n",
    "print(x.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drj-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
