{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个notebook 介绍了 如何对split learning 发起 inverse-model attack攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.attacks.model_inversion.inverse_model import InverseModelAttack\n",
    "from ppsplit.utils.utils import create_dir\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar10 （图像多分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize,get_one_data,deprocess\n",
    "\n",
    "from target_model.models.VGG import VGG,VGG5Decoder,model_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 2 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 2 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/project/Inverse_efficacy/results/VGG5/BN+Tanh/2-20240101/VGG5-params-19ep.pth'\n",
    "results_dir = f'../results/VGG5/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/'\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "features.5.weight\n",
      "features.5.bias\n",
      "features.5.running_mean\n",
      "features.5.running_var\n",
      "features.5.num_batches_tracked\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备基本模型client net\n",
    "# split_layer_list = list(range(len(model_cfg['VGG5']))) # 可能的切割点\n",
    "\n",
    "# 创建对应文件夹\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "# 把unit模型切割成client-server 的模型pair\n",
    "client_net = VGG('Client','VGG5',split_layer,model_cfg)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(model_cfg['VGG5']):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading decoder model '../results/VGG5/2/Decoder-layer2.pth'\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=1,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = VGG5Decoder(split_layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = get_cifar10_normalize(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----train decoder----\n",
      "client_net: \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n",
      "decoder_net: \n",
      "VGG5Decoder(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (4): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:46<00:00, 215.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.8241773731291294,              MSE:0.0903856434754096\n",
      "average time: 0.0016465278148651123 avg infer time:0.0011764520883560182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有图片\n",
    "trainloader,testloader = get_cifar10_normalize(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  deprocess=deprocess,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank数据集 （表格数据二分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_bank import preprocess_bank\n",
    "\n",
    "from target_model.models.BankNet import BankNet1,BankNetDecoder1,bank_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 6 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 2 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Bank/bank-20ep_params.pth'\n",
    "results_dir = f'../results/Bank/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/'\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "linear2.weight\n",
      "linear2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target model的 client net（对模型进行切割）\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "client_net = BankNet1(layer=split_layer)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(bank_cfg):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading decoder model '../results/Bank/6/Decoder-layer2.pth'\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=0,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = BankNetDecoder1(layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = preprocess_bank(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8238 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (32950, 63)\n",
      "X_test.shape: (8238, 63)\n",
      "y_train.shape: (32950, 1)\n",
      "y_test.shape: (8238, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n",
      "----train decoder----\n",
      "client_net: \n",
      "BankNet1(\n",
      "  (linear1): Linear(in_features=63, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      ")\n",
      "decoder_net: \n",
      "BankNetDecoder1(\n",
      "  (delinear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (delinear2): Linear(in_features=128, out_features=63, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8238/8238 [00:20<00:00, 397.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine: 0.9167903578593969,               Euclidean: 1.294159001821137,              MSE:0.042359481014795926\n",
      "average time: 0.0006233145525571819 avg infer time:0.0003061840388924788\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有表格数据行\n",
    "trainloader,testloader = preprocess_bank(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit 数据集 （表格数据二分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_credit import preprocess_credit\n",
    "\n",
    "from target_model.models.CreditNet import CreditNet1,CreditNetDecoder1,credit_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 10 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 3 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/credit/credit-20ep_params.pth'\n",
    "results_dir = f'../results/Credit/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/' # 储存\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "batch_norm1.weight\n",
      "batch_norm1.bias\n",
      "batch_norm1.running_mean\n",
      "batch_norm1.running_var\n",
      "batch_norm1.num_batches_tracked\n",
      "linear2.weight\n",
      "linear2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target model的 client net（对模型进行切割）\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "client_net = CreditNet1(layer=split_layer)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(credit_cfg):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading decoder model '../results/Credit/10/Decoder-layer3.pth'\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=0,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = CreditNetDecoder1(layer=split_layer)\n",
    "    # optimizer = torch.optim.SGD(decoder_net.parameters(), 1e-3)\n",
    "\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = preprocess_credit(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20,\n",
    "                            # optimizer=optimizer\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/61503 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (246008, 250)\n",
      "X_test.shape: (61503, 250)\n",
      "y_train.shape: (246008, 1)\n",
      "y_test.shape: (61503, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n",
      "----train decoder----\n",
      "client_net: \n",
      "CreditNet1(\n",
      "  (linear1): Linear(in_features=250, out_features=512, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ReLU1): LeakyReLU(negative_slope=0.01)\n",
      "  (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "decoder_net: \n",
      "CreditNetDecoder1(\n",
      "  (delinear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (delinear2): Linear(in_features=512, out_features=250, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61503/61503 [01:49<00:00, 562.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine: 0.9869267164826434,               Euclidean: 1.4236435810680315,              MSE:0.009183392138517262\n",
      "average time: 0.00035009122411578644 avg infer time:0.00031623337855465504\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有表格数据行\n",
    "trainloader,testloader = preprocess_credit(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purchase100 数据集 （表格数据多分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase\n",
    "\n",
    "from target_model.models.PurchaseNet import PurchaseClassifier1,PurchaseDecoder1,purchase_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 1.1 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 3 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Purchase100/Purchase_bestmodel_param.pth'\n",
    "results_dir = f'../results/Purchase/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/'\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "linear2.weight\n",
      "linear2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target model的 client net（对模型进行切割）\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "client_net = PurchaseClassifier1(layer=split_layer)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(purchase_cfg):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train decoder model...\n",
      "[('Tanh',), ('D', 1024, 512), ('Tanh',), ('D', 600, 1024)]\n",
      "purchase100 dataset processing...\n",
      "datset route: /home/dengruijun/data/FinTech/DATASET/kaggle-dataset/Purchase100//data.npz\n",
      "original dataset shape:  (197324, 600)\n",
      "After random selection, dataset shape:  (197324, 600)\n",
      "After split between classifier and attack: \n",
      "training dataset shape:  (157859, 600)\n",
      "testing dataset shape:  (39465, 600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading finished\n",
      "----train decoder----\n",
      "client_net: \n",
      "PurchaseClassifier1(\n",
      "  (linear1): Linear(in_features=600, out_features=1024, bias=True)\n",
      "  (Tanh1): Tanh()\n",
      "  (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (Tanh2): Tanh()\n",
      ")\n",
      "decoder_net: \n",
      "PurchaseDecoder1(\n",
      "  (delinear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (Tanh1): Tanh()\n",
      "  (delinear2): Linear(in_features=1024, out_features=600, bias=True)\n",
      "  (Tanh2): Tanh()\n",
      ")\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:26<00:00, 184.74it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 0, train_loss: [0.036627549678087234]\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 203.88it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 1, train_loss: [0.03575607389211655]\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:23<00:00, 208.19it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 2, train_loss: [0.03371845558285713]\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 204.38it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 3, train_loss: [0.03246120363473892]\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 204.07it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 4, train_loss: [0.032881733030080795]\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 202.60it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 5, train_loss: [0.032808125019073486]\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 202.64it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 6, train_loss: [0.032848045229911804]\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 205.49it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 7, train_loss: [0.032901644706726074]\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 203.39it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 8, train_loss: [0.03116638958454132]\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 202.96it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 9, train_loss: [0.03199167922139168]\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 205.30it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 10, train_loss: [0.03230536729097366]\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 203.96it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 11, train_loss: [0.03261202201247215]\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 202.58it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 12, train_loss: [0.033292222768068314]\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 199.35it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 13, train_loss: [0.0322897732257843]\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 197.59it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 14, train_loss: [0.03349592909216881]\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:25<00:00, 197.22it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 15, train_loss: [0.03360217809677124]\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:23<00:00, 206.64it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 16, train_loss: [0.03368714824318886]\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 202.28it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 17, train_loss: [0.032218173146247864]\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:23<00:00, 205.82it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 18, train_loss: [0.03365800902247429]\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:24<00:00, 203.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 19, train_loss: [0.033522527664899826]\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=0,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = PurchaseDecoder1(layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = preprocess_purchase(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase100 dataset processing...\n",
      "datset route: /home/dengruijun/data/FinTech/DATASET/kaggle-dataset/Purchase100//data.npz\n",
      "original dataset shape:  (197324, 600)\n",
      "After random selection, dataset shape:  (197324, 600)\n",
      "After split between classifier and attack: \n",
      "training dataset shape:  (157859, 600)\n",
      "testing dataset shape:  (39465, 600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39465 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading finished\n",
      "----train decoder----\n",
      "client_net: \n",
      "PurchaseClassifier1(\n",
      "  (linear1): Linear(in_features=600, out_features=1024, bias=True)\n",
      "  (Tanh1): Tanh()\n",
      "  (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (Tanh2): Tanh()\n",
      ")\n",
      "decoder_net: \n",
      "PurchaseDecoder1(\n",
      "  (delinear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (Tanh1): Tanh()\n",
      "  (delinear2): Linear(in_features=1024, out_features=600, bias=True)\n",
      "  (Tanh2): Tanh()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39465/39465 [01:33<00:00, 420.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine: 0.9504989085429434,               Euclidean: 4.34348811342676,              MSE:0.03172383672123201\n",
      "average time: 0.000556240434597068 avg infer time:0.0003523296538405767\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有表格数据行\n",
    "trainloader,testloader = preprocess_purchase(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drj-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
