{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个notebook 介绍了 如何对split learning 发起 inverse-model attack攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.attacks.model_inversion.inverse_model import InverseModelAttack\n",
    "from ppsplit.utils.utils import create_dir\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar10 （图像多分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize,get_one_data,deprocess\n",
    "\n",
    "from target_model.models.VGG import VGG,VGG5Decoder,model_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 3 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 2 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/project/Inverse_efficacy/results/VGG5/BN+Tanh/2-20240101/VGG5-params-19ep.pth'\n",
    "results_dir = f'../results/VGG5/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/'\n",
    "decoder_net_route = results_dir + f'VGG5_layer{split_layer}_decoder.pth' # 攻击的decoder net存储位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "features.5.weight\n",
      "features.5.bias\n",
      "features.5.running_mean\n",
      "features.5.running_var\n",
      "features.5.num_batches_tracked\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备基本模型client net\n",
    "# split_layer_list = list(range(len(model_cfg['VGG5']))) # 可能的切割点\n",
    "\n",
    "# 创建对应文件夹\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "# 把unit模型切割成client-server 的模型pair\n",
    "client_net = VGG('Client','VGG5',split_layer,model_cfg)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(model_cfg['VGG5']):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----train decoder----\n",
      "client_net: \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n",
      "decoder_net: \n",
      "VGG5Decoder(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (4): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:18<00:00, 82.34it/s] \n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 0, train_loss: [0.03005491942167282]\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:17<00:00, 91.04it/s] \n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 1, train_loss: [0.02677375078201294]\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:17<00:00, 91.77it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 2, train_loss: [0.025707125663757324]\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:17<00:00, 90.61it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 3, train_loss: [0.025064490735530853]\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:16<00:00, 92.88it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 4, train_loss: [0.024576028808951378]\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:16<00:00, 94.75it/s] \n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 5, train_loss: [0.02423805370926857]\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:16<00:00, 92.59it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 6, train_loss: [0.02400543913245201]\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:16<00:00, 93.50it/s] \n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 7, train_loss: [0.02378014102578163]\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:16<00:00, 92.23it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 8, train_loss: [0.023601016029715538]\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:17<00:00, 91.81it/s] \n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 9, train_loss: [0.023442819714546204]\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:16<00:00, 92.67it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 10, train_loss: [0.023295387625694275]\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:17<00:00, 91.69it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 11, train_loss: [0.02316342107951641]\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:16<00:00, 91.96it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 12, train_loss: [0.023043181747198105]\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:16<00:00, 92.38it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 13, train_loss: [0.022930141538381577]\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:16<00:00, 92.16it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 14, train_loss: [0.0228277575224638]\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:16<00:00, 92.73it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 15, train_loss: [0.02273508533835411]\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:16<00:00, 96.24it/s] \n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 16, train_loss: [0.022652840241789818]\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:15<00:00, 103.90it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 17, train_loss: [0.022576212882995605]\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:15<00:00, 103.18it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 18, train_loss: [0.0225073155015707]\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:16<00:00, 93.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 19, train_loss: [0.022443225607275963]\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=1,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = VGG5Decoder(split_layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = get_cifar10_normalize(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----train decoder----\n",
      "client_net: \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n",
      "decoder_net: \n",
      "VGG5Decoder(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (4): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:00<00:00, 166.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.8241197123900056,              MSE:0.09035276736826636\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有图片\n",
    "trainloader,testloader = get_cifar10_normalize(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  deprocess=deprocess,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank数据集 （表格数据二分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_bank import preprocess_bank\n",
    "\n",
    "from target_model.models.BankNet import BankNet1,BankNetDecoder1,bank_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 1 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 2 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Bank/bank-20ep_params.pth'\n",
    "results_dir = f'../results/Bank/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/'\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "linear2.weight\n",
      "linear2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target model的 client net（对模型进行切割）\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "client_net = BankNet1(layer=split_layer)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(bank_cfg):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading decoder model '../results/Bank/1/Decoder-layer2.pth'\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=0,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = BankNetDecoder1(layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = preprocess_bank(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n",
      "X_train.shape: (32950, 63)\n",
      "X_test.shape: (8238, 63)\n",
      "y_train.shape: (32950, 1)\n",
      "y_test.shape: (8238, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n",
      "----train decoder----\n",
      "client_net: \n",
      "BankNet1(\n",
      "  (linear1): Linear(in_features=63, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      ")\n",
      "decoder_net: \n",
      "BankNetDecoder1(\n",
      "  (delinear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (delinear2): Linear(in_features=128, out_features=63, bias=True)\n",
      "  (ReLU2): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8238/8238 [00:14<00:00, 580.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine: 0.7169622892776604,               Euclidean: 2.7359826737279076,              MSE:0.12100472962056427\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有表格数据行\n",
    "trainloader,testloader = preprocess_bank(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit 数据集 （表格数据二分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_credit import preprocess_credit\n",
    "\n",
    "from target_model.models.CreditNet import CreditNet1,CreditNetDecoder1,credit_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 1 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 3 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/credit/credit-20ep_params.pth'\n",
    "results_dir = f'../results/Credit/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/' # 储存\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "batch_norm1.weight\n",
      "batch_norm1.bias\n",
      "batch_norm1.running_mean\n",
      "batch_norm1.running_var\n",
      "batch_norm1.num_batches_tracked\n",
      "linear2.weight\n",
      "linear2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CreditNet1(\n",
       "  (linear1): Linear(in_features=250, out_features=512, bias=True)\n",
       "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (ReLU1): LeakyReLU(negative_slope=0.01)\n",
       "  (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target model的 client net（对模型进行切割）\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "client_net = CreditNet1(layer=split_layer)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(credit_cfg):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading decoder model '../results/Credit/1/Decoder-layer3.pth'\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=0,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = CreditNetDecoder1(layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = preprocess_credit(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/61503 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (246008, 250)\n",
      "X_test.shape: (61503, 250)\n",
      "y_train.shape: (246008, 1)\n",
      "y_test.shape: (61503, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n",
      "----train decoder----\n",
      "client_net: \n",
      "CreditNet1(\n",
      "  (linear1): Linear(in_features=250, out_features=512, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ReLU1): LeakyReLU(negative_slope=0.01)\n",
      "  (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "decoder_net: \n",
      "CreditNetDecoder1(\n",
      "  (delinear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (delinear2): Linear(in_features=512, out_features=250, bias=True)\n",
      "  (ReLU2): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61503/61503 [09:38<00:00, 106.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine: 0.48356992033534546,               Euclidean: 8.41318228771874,              MSE:0.28819609253747236\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有表格数据行\n",
    "trainloader,testloader = preprocess_credit(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purchase100 数据集 （表格数据多分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase\n",
    "\n",
    "from target_model.models.PurchaseNet import PurchaseClassifier1,PurchaseDecoder1,purchase_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 1 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 3 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Purchase100/Purchase_bestmodel_param.pth'\n",
    "results_dir = f'../results/Purchase/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/'\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "linear2.weight\n",
      "linear2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target model的 client net（对模型进行切割）\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "client_net = PurchaseClassifier1(layer=split_layer)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(purchase_cfg):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train decoder model...\n",
      "[('Tanh',), ('D', 1024, 512), ('Tanh',), ('D', 600, 1024)]\n",
      "purchase100 dataset processing...\n",
      "datset route: /home/dengruijun/data/FinTech/DATASET/kaggle-dataset/Purchase100//data.npz\n",
      "original dataset shape:  (197324, 600)\n",
      "After random selection, dataset shape:  (197324, 600)\n",
      "After split between classifier and attack: \n",
      "training dataset shape:  (157859, 600)\n",
      "testing dataset shape:  (39465, 600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading finished\n",
      "----train decoder----\n",
      "client_net: \n",
      "PurchaseClassifier1(\n",
      "  (linear1): Linear(in_features=600, out_features=1024, bias=True)\n",
      "  (Tanh1): Tanh()\n",
      "  (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (Tanh2): Tanh()\n",
      ")\n",
      "decoder_net: \n",
      "PurchaseDecoder1(\n",
      "  (delinear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (Tanh1): Tanh()\n",
      "  (delinear2): Linear(in_features=1024, out_features=600, bias=True)\n",
      "  (Tanh2): Tanh()\n",
      ")\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:26<00:00, 183.74it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 0, train_loss: [0.03727872669696808]\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 182.05it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 1, train_loss: [0.032788753509521484]\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 179.82it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 2, train_loss: [0.03448701277375221]\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 178.91it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 3, train_loss: [0.03429969772696495]\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 179.83it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 4, train_loss: [0.03170054033398628]\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 180.58it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 5, train_loss: [0.031940482556819916]\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 181.36it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 6, train_loss: [0.03539854660630226]\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 176.69it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 7, train_loss: [0.03266086056828499]\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 181.18it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 8, train_loss: [0.03200650215148926]\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:26<00:00, 183.67it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 9, train_loss: [0.032836128026247025]\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:26<00:00, 183.73it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 10, train_loss: [0.032636966556310654]\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 179.50it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 11, train_loss: [0.033743083477020264]\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 179.57it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 12, train_loss: [0.03231579065322876]\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 180.70it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 13, train_loss: [0.03254451975226402]\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 178.61it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 14, train_loss: [0.030672360211610794]\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 177.69it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 15, train_loss: [0.02879592403769493]\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:28<00:00, 175.93it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 16, train_loss: [0.03290849179029465]\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 177.12it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 17, train_loss: [0.03248794376850128]\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:27<00:00, 181.59it/s]\n",
      "  0%|          | 0/4933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 18, train_loss: [0.03287642076611519]\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4933/4933 [00:26<00:00, 183.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 19, train_loss: [0.03166915476322174]\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=0,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = PurchaseDecoder1(layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = preprocess_purchase(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase100 dataset processing...\n",
      "datset route: /home/dengruijun/data/FinTech/DATASET/kaggle-dataset/Purchase100//data.npz\n",
      "original dataset shape:  (197324, 600)\n",
      "After random selection, dataset shape:  (197324, 600)\n",
      "After split between classifier and attack: \n",
      "training dataset shape:  (157859, 600)\n",
      "testing dataset shape:  (39465, 600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39465 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading finished\n",
      "----train decoder----\n",
      "client_net: \n",
      "PurchaseClassifier1(\n",
      "  (linear1): Linear(in_features=600, out_features=1024, bias=True)\n",
      "  (Tanh1): Tanh()\n",
      "  (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (Tanh2): Tanh()\n",
      ")\n",
      "decoder_net: \n",
      "PurchaseDecoder1(\n",
      "  (delinear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (Tanh1): Tanh()\n",
      "  (delinear2): Linear(in_features=1024, out_features=600, bias=True)\n",
      "  (Tanh2): Tanh()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39465/39465 [05:00<00:00, 131.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine: 0.9506615227503905,               Euclidean: 4.338581247002275,              MSE:0.031658947504995504\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有表格数据行\n",
    "trainloader,testloader = preprocess_purchase(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drj-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
