{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个notebook 介绍了 如何对split learning 发起 inverse-model attack攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.attacks.model_inversion.inverse_model import InverseModelAttack\n",
    "from ppsplit.utils.utils import create_dir\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar10 （图像多分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize,get_one_data,deprocess\n",
    "\n",
    "from target_model.models.VGG import VGG,VGG5Decoder,model_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 2 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 2 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/project/Inverse_efficacy/results/VGG5/BN+Tanh/2-20240101/VGG5-params-19ep.pth'\n",
    "results_dir = f'../results/VGG5/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/'\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "features.5.weight\n",
      "features.5.bias\n",
      "features.5.running_mean\n",
      "features.5.running_var\n",
      "features.5.num_batches_tracked\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备基本模型client net\n",
    "# split_layer_list = list(range(len(model_cfg['VGG5']))) # 可能的切割点\n",
    "\n",
    "# 创建对应文件夹\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "# 把unit模型切割成client-server 的模型pair\n",
    "client_net = VGG('Client','VGG5',split_layer,model_cfg)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(model_cfg['VGG5']):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading decoder model '../results/VGG5/2/Decoder-layer2.pth'\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=1,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = VGG5Decoder(split_layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = get_cifar10_normalize(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----train decoder----\n",
      "client_net: \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n",
      "decoder_net: \n",
      "VGG5Decoder(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (4): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (denses): Sequential()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:01<00:00, 162.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.8241773730516434,              MSE:0.09038564349152148\n",
      "average time: 0.002487421727180481\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有图片\n",
    "trainloader,testloader = get_cifar10_normalize(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  deprocess=deprocess,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank数据集 （表格数据二分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_bank import preprocess_bank\n",
    "\n",
    "from target_model.models.BankNet import BankNet1,BankNetDecoder1,bank_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 1 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 2 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Bank/bank-20ep_params.pth'\n",
    "results_dir = f'../results/Bank/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/'\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "linear2.weight\n",
      "linear2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target model的 client net（对模型进行切割）\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "client_net = BankNet1(layer=split_layer)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(bank_cfg):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading decoder model '../results/Bank/1/Decoder-layer2.pth'\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=0,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = BankNetDecoder1(layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = preprocess_bank(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8238 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (32950, 63)\n",
      "X_test.shape: (8238, 63)\n",
      "y_train.shape: (32950, 1)\n",
      "y_test.shape: (8238, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n",
      "----train decoder----\n",
      "client_net: \n",
      "BankNet1(\n",
      "  (linear1): Linear(in_features=63, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      ")\n",
      "decoder_net: \n",
      "BankNetDecoder1(\n",
      "  (delinear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (delinear2): Linear(in_features=128, out_features=63, bias=True)\n",
      "  (ReLU2): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8238/8238 [00:14<00:00, 582.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine: 0.7169622892776604,               Euclidean: 2.7359826737279076,              MSE:0.12100472962056427\n",
      "average time: 0.0003475469708240099\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有表格数据行\n",
    "trainloader,testloader = preprocess_bank(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit 数据集 （表格数据二分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_credit import preprocess_credit\n",
    "\n",
    "from target_model.models.CreditNet import CreditNet1,CreditNetDecoder1,credit_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 2 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 3 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/credit/credit-20ep_params.pth'\n",
    "results_dir = f'../results/Credit/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/' # 储存\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "batch_norm1.weight\n",
      "batch_norm1.bias\n",
      "batch_norm1.running_mean\n",
      "batch_norm1.running_var\n",
      "batch_norm1.num_batches_tracked\n",
      "linear2.weight\n",
      "linear2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target model的 client net（对模型进行切割）\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "client_net = CreditNet1(layer=split_layer)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(credit_cfg):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading decoder model '../results/Credit/2/Decoder-layer3.pth'\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=0,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = CreditNetDecoder1(layer=split_layer)\n",
    "    # optimizer = torch.optim.SGD(decoder_net.parameters(), 1e-3)\n",
    "\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = preprocess_credit(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=120,\n",
    "                            # optimizer=optimizer\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/61503 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (246008, 250)\n",
      "X_test.shape: (61503, 250)\n",
      "y_train.shape: (246008, 1)\n",
      "y_test.shape: (61503, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n",
      "----train decoder----\n",
      "client_net: \n",
      "CreditNet1(\n",
      "  (linear1): Linear(in_features=250, out_features=512, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ReLU1): LeakyReLU(negative_slope=0.01)\n",
      "  (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "decoder_net: \n",
      "CreditNetDecoder1(\n",
      "  (delinear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (delinear2): Linear(in_features=512, out_features=250, bias=True)\n",
      "  (ReLU2): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61503/61503 [02:31<00:00, 405.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine: 0.44282525866250244,               Euclidean: 8.634714468419837,              MSE:0.3023779114369925\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有表格数据行\n",
    "trainloader,testloader = preprocess_credit(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purchase100 数据集 （表格数据多分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包和超参数设置\n",
    "from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase\n",
    "\n",
    "from target_model.models.PurchaseNet import PurchaseClassifier1,PurchaseDecoder1,purchase_cfg\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "\n",
    "test_num = 1 # 测试编号（对应结果文件夹名称）\n",
    "split_layer = 3 # 模型切割点 （split point）在该层之前的层（含），作为client的模型，之后的层作为server的模型\n",
    "\n",
    "# 重要路径设置\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Purchase100/Purchase_bestmodel_param.pth'\n",
    "results_dir = f'../results/Purchase/{test_num}/'\n",
    "inverse_dir = results_dir + 'layer'+str(split_layer)+'/'\n",
    "decoder_net_route = results_dir + f'Decoder-layer{split_layer}.pth' # 攻击的decoder net存储位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "linear2.weight\n",
      "linear2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备target model的 client net（对模型进行切割）\n",
    "create_dir(results_dir)\n",
    "create_dir(inverse_dir)\n",
    "\n",
    "client_net = PurchaseClassifier1(layer=split_layer)\n",
    "pweights = torch.load(unit_net_route)\n",
    "if split_layer < len(purchase_cfg):\n",
    "    pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "client_net.load_state_dict(pweights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading decoder model '../results/Purchase/1/Decoder-layer3.pth'\n"
     ]
    }
   ],
   "source": [
    "# 准备inverse_model attack使用到的东西\n",
    "# 创建Inverse Model Attack对象\n",
    "im_attack = InverseModelAttack(decoder_route=decoder_net_route,data_type=0,inverse_dir=inverse_dir)\n",
    "\n",
    "# 加载decoder模型\n",
    "if os.path.isfile(decoder_net_route): # 如果已经训练好了\n",
    "    print(\"=> loading decoder model '{}'\".format(decoder_net_route))\n",
    "    decoder_net = torch.load(decoder_net_route)\n",
    "else: # 如果没有\n",
    "    print(\"train decoder model...\")\n",
    "    decoder_net = PurchaseDecoder1(layer=split_layer)\n",
    "    # 训练decoder\n",
    "    trainloader,testloader = preprocess_purchase(batch_size=32)\n",
    "\n",
    "    decoder_net= im_attack.train_decoder(client_net=client_net,decoder_net=decoder_net,\n",
    "                            train_loader=trainloader,test_loader=testloader,\n",
    "                            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase100 dataset processing...\n",
      "datset route: /home/dengruijun/data/FinTech/DATASET/kaggle-dataset/Purchase100//data.npz\n",
      "original dataset shape:  (197324, 600)\n",
      "After random selection, dataset shape:  (197324, 600)\n",
      "After split between classifier and attack: \n",
      "training dataset shape:  (157859, 600)\n",
      "testing dataset shape:  (39465, 600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39465 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading finished\n",
      "----train decoder----\n",
      "client_net: \n",
      "PurchaseClassifier1(\n",
      "  (linear1): Linear(in_features=600, out_features=1024, bias=True)\n",
      "  (Tanh1): Tanh()\n",
      "  (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (Tanh2): Tanh()\n",
      ")\n",
      "decoder_net: \n",
      "PurchaseDecoder1(\n",
      "  (delinear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (Tanh1): Tanh()\n",
      "  (delinear2): Linear(in_features=1024, out_features=600, bias=True)\n",
      "  (Tanh2): Tanh()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39465/39465 [01:29<00:00, 441.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine: 0.9506615227503905,               Euclidean: 4.338581247002275,              MSE:0.031658947504995504\n",
      "average time: 0.0005385350776265424\n"
     ]
    }
   ],
   "source": [
    "# 实现攻击,恢复testloader中所有表格数据行\n",
    "trainloader,testloader = preprocess_purchase(batch_size=1)\n",
    "\n",
    "im_attack.inverse(client_net=client_net,decoder_net=decoder_net,\n",
    "                  train_loader=trainloader,test_loader=testloader,\n",
    "                  save_fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drj-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
