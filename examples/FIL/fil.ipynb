{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基础设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '48'\n",
    "\n",
    "# 导入各个指标\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.quantification.distance_correlation.distCor import distCorMetric\n",
    "from ppsplit.quantification.fisher_information.dFIL_inverse import dFILInverseMetric\n",
    "from ppsplit.quantification.shannon_information.mutual_information import MuInfoMetric\n",
    "from ppsplit.quantification.shannon_information.ULoss import ULossMetric\n",
    "from ppsplit.quantification.rep_reading.rep_reader import PCA_Reader\n",
    "\n",
    "# 导入各个baseline模型及其数据集预处理方法\n",
    "# 模型\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "from target_model.models.VGG import VGG,VGG5Decoder,model_cfg\n",
    "from target_model.models.BankNet import BankNet1,bank_cfg\n",
    "from target_model.models.CreditNet import CreditNet1,credit_cfg\n",
    "from target_model.models.PurchaseNet import PurchaseClassifier1,purchase_cfg\n",
    "\n",
    "# 数据预处理方法\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize,deprocess\n",
    "from target_model.data_preprocessing.preprocess_bank import bank_dataset,preprocess_bank\n",
    "from target_model.data_preprocessing.preprocess_credit import preprocess_credit\n",
    "from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase\n",
    "from target_model.data_preprocessing.dataset import get_one_data\n",
    "\n",
    "# utils\n",
    "from ppsplit.utils.utils import create_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. pytorch的自动梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播\n",
    "import torch\n",
    "\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "# 信息打印\n",
    "print(x.shape)\n",
    "print(x.requires_grad)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向传播\n",
    "loss.backward()\n",
    "print(x.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad/backward 张量求梯度 sum\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.randn(5, 3)\n",
    "b = torch.randn(3)\n",
    "z = torch.matmul(x, w)+b\n",
    "\n",
    "# 计算sumed雅可比矩阵\n",
    "z.backward(torch.ones_like(z))\n",
    "print(x.grad.shape)\n",
    "print(x.grad)\n",
    "\n",
    "# 用grad\n",
    "from torch.autograd import grad\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.eye(5, 3)\n",
    "b = torch.randn(3)\n",
    "z = torch.matmul(x, w)+b\n",
    "xgrad = grad(z, x, grad_outputs=torch.ones_like(z))[0]\n",
    "print(xgrad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jacobian\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.autograd import grad\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "\n",
    "def forward(x):\n",
    "    y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "    w = torch.eye(5, 3)\n",
    "    b = torch.randn(3)\n",
    "    z = torch.matmul(x, w)+b\n",
    "    return z\n",
    "\n",
    "\n",
    "# 计算sumed雅可比矩阵\n",
    "jac = jacobian(forward, x)\n",
    "print(jac.shape)\n",
    "print(jac)\n",
    "print(jac.sum(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 试探FIL计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL计算：\n",
    "import torch.autograd.functional as F\n",
    "# 切割模型通讯量查看\n",
    "# for i in range(7):\n",
    "vgg5 = VGG('Client', 'VGG5', 1, model_cfg)\n",
    "\n",
    "client_outputs = vgg5(images)\n",
    "print('outputs.shape:',client_outputs.shape)\n",
    "jacs = F.jacobian(vgg5, images)\n",
    "print('jacobian: ', jacs)\n",
    "# print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))[1:]))\n",
    "print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))))\n",
    "\n",
    "# 0到6层每层的jacobians\n",
    "import torch.autograd.functional as F\n",
    "for i in range(7):\n",
    "    vgg5 = VGG('Client', 'VGG5', i, model_cfg)\n",
    "    client_outputs = vgg5(images)\n",
    "    print('outputs.shape:',client_outputs.shape)\n",
    "    jacs = F.jacobian(vgg5, images)\n",
    "    print('jacobian: ', jacs)\n",
    "    # print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))[1:]))\n",
    "    print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL 计算，摸索出来一条路\n",
    "import torch.autograd.functional as F\n",
    "# 参数：\n",
    "sigma = 0.01\n",
    "\n",
    "# 计算jacobian\n",
    "# 取一个batch的数据\n",
    "train_iter=iter(trainloader)\n",
    "inputs,labels = train_iter.next()6\n",
    "print(\"inputs.shape: \",inputs.shape)\n",
    "print(\"labels.shape: \",labels.shape)\n",
    "print(f\"input.requires_grad: {inputs.requires_grad}\")\n",
    "\n",
    "# 加载模型：\n",
    "# vgg5 = VGG('Client', 'VGG5', 1, model_cfg)\n",
    "\n",
    "# 进行前向传播：\n",
    "inputs.requires_grad_(True) # 需要求导\n",
    "outputs = vgg5(inputs)\n",
    "outputs = outputs + sigma * torch.randn_like(outputs) # 加噪声 (0,1] uniform\n",
    "print(\"outputs.shape: \",outputs.shape)\n",
    "\n",
    "# 1. 进行反向传播,计算jacobian\n",
    "# outputs.backward(torch.ones_like(outputs))\n",
    "# J = inputs.grad / sigma # 计算jacobian\n",
    "# print(f\"J1.shape: {J.shape}\")\n",
    "\n",
    "# 2. 重新计算jacobian（用torch.autograd.functional.jacobian函数）\n",
    "J = F.jacobian(vgg5, inputs)\n",
    "# print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "J = J.reshape(J.shape[0],outputs.numel(),inputs.numel())\n",
    "print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "\n",
    "# 计算eta 源论文\n",
    "# J = model.influence_jacobian(train_data)[:, :, :-1] / args.sigma  # 计算FIL（梯度）jacobian\n",
    "# etas = J.pow(2).sum(1).mean(1).sqrt() # 计算dFIL(这时候不是spectral norm了) \n",
    "\n",
    "# 计算eta：drj摸索：\n",
    "I = torch.matmul(J[0].t(), J[0])\n",
    "dFIL = I.trace().div(inputs.numel())\n",
    "eta = dFIL.sqrt()\n",
    "print(f\"eta: {eta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dFIL的两个要求: 可导 + unbiased\n",
    "# x = torch.rand_like(torch.Tensor([1,5]))\n",
    "x = torch.Tensor([0,0])\n",
    "x.requires_grad_(True)\n",
    "print(x.grad)\n",
    "y = torch.nn.ReLU()\n",
    "z = y(x).sum()\n",
    "# z = torch.autograd.functional.jacobian(y, x)\n",
    "z.backward()\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 现成函数调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "        'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        # 'device':torch.device(\"cpu\"),\n",
    "        'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        'result_dir': '20240702-FIL/',\n",
    "        'batch_size':1,\n",
    "        'noise_scale':0, # 防护措施\n",
    "        }\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "# 加载模型和数据集，并从unit模型中切割出client_model\n",
    "if args['dataset']=='CIFAR10':\n",
    "    # 超参数\n",
    "    testset_len = 10000 # 10000个数据一次 整个测试集合的长度\n",
    "    # split_layer_list = list(range(len(model_cfg['VGG5'])))\n",
    "    split_layer = 1 # 定成3吧？\n",
    "    test_num = 1 # 试验序号\n",
    "\n",
    "    # 关键路径\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/VGG5/BN+Tanh/VGG5-params-20ep.pth' # VGG5-BN+Tanh # 存储的是模型参数，不包括模型结构\n",
    "    results_dir  = f\"../../results/{args['result_dir']}/VGG5/{test_num}/\"\n",
    "    decoder_route = f\"../../results/{args['result_dir']}/VGG5/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    trainloader,testloader = get_cifar10_normalize(batch_size = 1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # 切割成client model\n",
    "    # vgg5_unit.load_state_dict(torch.load(unit_net_route,map_location=torch.device('cpu'))) # 完整的模型\n",
    "    client_net = VGG('Client','VGG5',split_layer,model_cfg,noise_scale=args['noise_scale'])\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(model_cfg['VGG5']):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "else:\n",
    "    exit(-1)\n",
    "\n",
    "client_net.to(args['device'])\n",
    "create_dir(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# 我实现的：\n",
    "# dFIL inverse指标计算\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric = dFILInverseMetric()\n",
    "# 对traingloader遍历计算所有 inverse dFIL\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)):\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    \n",
    "    # inference\n",
    "    outputs = client_net(inputs)\n",
    "\n",
    "    eta = metric.quantify(model=client_net, inputs=inputs, outputs=outputs, with_outputs=True)\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'inv_dFIL.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL 计算函数\n",
    "import torch.autograd.functional as F\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# nips23\n",
    "from torch.autograd.functional import jvp\n",
    "import random\n",
    "import math\n",
    "\n",
    "def calc_tr(net, x, device, subsample=-1, jvp_parallelism=1): # nips'23 源码\n",
    "    '''\n",
    "    calc_tr 函数利用雅可比向量积（JVP）来估计网络对于输入数据的迹，\n",
    "    这在分析网络的灵敏度或稳定性时非常有用。\n",
    "    此外，通过支持子采样和并行处理，该函数还提供了一种在保持计算效率的同时估计迹的方法。\n",
    "    '''\n",
    "    print(f'x.shape: {x.shape}')\n",
    "    \n",
    "    # 定义一个局部函数 jvp_func**：这个函数接受两个参数 x 和 tgt，并返回 net.forward_first 方法的雅可比向量积（JVP）。\n",
    "    # 这意味着 jvp_func 用于计算网络对于输入 x 在方向 tgt 上的一阶导数\n",
    "    # tgt 计算雅各比向量积的向量\n",
    "    def jvp_func(x, tgt):\n",
    "        # return jvp(net.forward_first, (x,), (tgt,)) #返回 outputs, jacobian product\n",
    "        return jvp(net.forward, (x,), (tgt,)) #返回 outputs, jacobian product\n",
    "\n",
    "    # 获取一个batch中第一个数据的维度？d代表的是批次中第一个数据点展平后的特征数量，即输入数据的维度。\n",
    "    d = x[0].flatten().shape[0] # 把一个batch的x展平，获取input dim\n",
    "\n",
    "    # 用于存储每个输入数据点的迹\n",
    "    tr = torch.zeros(x.shape[0], dtype=x.dtype).to(device)\n",
    "    #print(f'd: {d}, {x.shape}')\n",
    "\n",
    "    # 加速，矩阵降维，但是这个损伤精度，或许改成特征提取更好点？\n",
    "    # Randomly subsample pixels for faster execution\n",
    "    if subsample > 0:\n",
    "        samples = random.sample(range(d), min(d, subsample))\n",
    "    else:\n",
    "        samples = range(d)\n",
    "\n",
    "    #print(x.shape, d, samples)\n",
    "    # jvp parallelism是数据并行的粒度？\n",
    "    # 函数通过分批处理样本来计算迹，每批处理 jvp_parallelism 个样本\n",
    "    for j in range(math.ceil(len(samples) / jvp_parallelism)): # 对于每个数据块\n",
    "        tgts = []\n",
    "        # 遍历每个数据块中的每个维度\n",
    "        for k in samples[j*jvp_parallelism:(j+1)*jvp_parallelism]: # 提取整个batch中每个数据的特定维度\n",
    "            tgt = torch.zeros_like(x).reshape(x.shape[0], -1) # 按照batch 排列？# 雅各比向量积的\n",
    "            # 除了当前样本索引 k 对应的元素设置为 1。这相当于在计算迹时，每次只关注一个特征维度。\n",
    "            tgt[:, k] = 1. # 提取tgt所有的样本的k的特征 计算雅各比向量积的向量，可用于计算trace\n",
    "            tgt = tgt.reshape(x.shape) # 又变回x的形状\n",
    "            tgts.append(tgt)\n",
    "        tgts = torch.stack(tgts)\n",
    "\n",
    "        def helper(tgt):\n",
    "            batch_size = x.shape[0]\n",
    "            vals_list = []\n",
    "            grads_list = []\n",
    "            for i in range(batch_size):\n",
    "                val, grad = jvp_func(x[i], tgt[i])  # 对每个批次元素调用jvp_func\n",
    "                vals_list.append(val)\n",
    "                grads_list.append(grad)\n",
    "            # 将结果列表转换为张量\n",
    "            vals = torch.stack(vals_list)\n",
    "            grad = torch.stack(grads_list)\n",
    "\n",
    "\n",
    "            # vals, grad = vmap(jvp_func, randomness='same')(x, tgt)\n",
    "            #print('grad shape: ', grad.shape)\n",
    "            return torch.sum(grad * grad, dim=tuple(range(1, len(grad.shape)))), vals # 先求迹再求平方\n",
    "\n",
    "        # vmap被替换\n",
    "        trs,vals = [],[]\n",
    "        for item in tgts:\n",
    "            trs_, vals_ = helper(item)\n",
    "            trs.append(trs_)\n",
    "            vals.append(vals_)\n",
    "        trs,vals = torch.stack(trs),torch.stack(vals)\n",
    "\n",
    "        # trs, vals = vmap(helper, randomness='same')(tgts) # randomness for randomness control of dropout\n",
    "        \n",
    "        # vals are stacked results that are repeated by d (should be all the same)\n",
    "\n",
    "\n",
    "        tr += trs.sum(dim=0)\n",
    "\n",
    "    # Scale if subsampled\n",
    "    if subsample > 0:\n",
    "        tr *= d / len(samples)\n",
    "\n",
    "    tr = tr/(d*1.0)\n",
    "    tr = 1.0/tr\n",
    "\n",
    "    print('tr: ',tr.shape, tr)\n",
    "    return tr.cpu().item(), vals[0].squeeze(1)  # squeeze removes one dimension jvp puts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 1, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr:  torch.Size([1]) tensor([0.0691], device='cuda:1')\n",
      "matrix:  [[0.0691408]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# score matching的结果\n",
    "\n",
    "# sys.path.append('/home/dengruijun/data/FinTech/VFL/6_quantification/FIL_instance_encoding-nips23/sliced_score_matching/')\n",
    "# sys.path.append('/home/dengruijun/data/FinTech/VFL/6_quantification/FIL_instance_encoding-nips23/')\n",
    "# from util import calc_tr\n",
    "# from evaluate_scores import compute_scores, evaluate_scores\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "# metric_trace = dFILInverseMetric()\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    \n",
    "    # inference\n",
    "    # outputs = client_net(inputs)\n",
    "\n",
    "    # eta = metric_trace.calc_tr(net=client_net, x=inputs, device=args['device'])\n",
    "    inputs = inputs.unsqueeze(0)\n",
    "    eta,val = calc_tr(net=client_net, x=inputs, device=args['device'])\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大\n",
    "print(\"matrix: \",matrix)\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'inv_dFIL_maeng.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
