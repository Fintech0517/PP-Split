{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基础设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np1\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '48'\n",
    "\n",
    "# 导入各个指标\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.quantification.distance_correlation.distCor import distCorMetric\n",
    "from ppsplit.quantification.fisher_information.dFIL_inverse import dFILInverseMetric\n",
    "from ppsplit.quantification.shannon_information.mutual_information import MuInfoMetric\n",
    "from ppsplit.quantification.shannon_information.ULoss import ULossMetric\n",
    "from ppsplit.quantification.rep_reading.rep_reader import PCA_Reader\n",
    "from ppsplit.quantification.shannon_information.ITE_tools import Shannon_quantity\n",
    "\n",
    "from target_model.task_select import get_dataloader_and_model\n",
    "\n",
    "# utils\n",
    "from ppsplit.utils.utils import create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "        'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        # 'device':torch.device(\"cpu\"),\n",
    "        # 'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        'dataset':'Iris',\n",
    "        'result_dir': '20240702-FIL/',\n",
    "        'oneData_bs':30,\n",
    "        'noise_scale':0, # 防护措施\n",
    "        'OneData':True,\n",
    "        'split_layer': 1,\n",
    "        }\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n",
      "X_train.shape: (120, 4)\n",
      "X_test.shape: (30, 4)\n",
      "y_train.shape: (120,)\n",
      "y_test.shape: (30,) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "results_dir: ../results/20240702-FIL//Iris/1/\n"
     ]
    }
   ],
   "source": [
    "# 获取数据集和模型\n",
    "# if OneData:\n",
    "#     return one_data_loader,client_net,results_dir,decoder_route\n",
    "# else:\n",
    "#     return trainloader,testloader,client_net,results_dir,decoder_route\n",
    "\n",
    "# one_data_loader,client_net,results_dir,decoder_route = get_dataloader_and_model(dataset=args['dataset'], \n",
    "#                                                                                 loader_bs=1, \n",
    "#                                                                                 oneData_bs=args['batch_size'], \n",
    "#                                                                                 noise_scale=args['noise_scale'], \n",
    "#                                                                                 result_ws=args['result_dir'], \n",
    "#                                                                                 OneData=True)\n",
    "\n",
    "one_data_loader,client_net,results_dir,decoder_route = get_dataloader_and_model(**args)\n",
    "# client_net.to(args['device'])\n",
    "# create_dir(results_dir)\n",
    "print('results_dir:',results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. pytorch的自动梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "True\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "import torch\n",
    "\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "# 信息打印\n",
    "print(x.shape)\n",
    "print(x.requires_grad)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向传播\n",
    "loss.backward()\n",
    "print(x.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad/backward 张量求梯度 sum\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.randn(5, 3)\n",
    "b = torch.randn(3)\n",
    "z = torch.matmul(x, w)+b\n",
    "\n",
    "# 计算sumed雅可比矩阵\n",
    "z.backward(torch.ones_like(z))\n",
    "print(x.grad.shape)\n",
    "print(x.grad)\n",
    "\n",
    "# 用grad\n",
    "from torch.autograd import grad\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.eye(5, 3)\n",
    "b = torch.randn(3)\n",
    "z = torch.matmul(x, w)+b\n",
    "xgrad = grad(z, x, grad_outputs=torch.ones_like(z))[0]\n",
    "print(xgrad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jacobian\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.autograd import grad\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "\n",
    "def forward(x):\n",
    "    y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "    w = torch.eye(5, 3)\n",
    "    b = torch.randn(3)\n",
    "    z = torch.matmul(x, w)+b\n",
    "    return z\n",
    "\n",
    "\n",
    "# 计算sumed雅可比矩阵\n",
    "jac = jacobian(forward, x)\n",
    "print(jac.shape)\n",
    "print(jac)\n",
    "print(jac.sum(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 试探FIL计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL计算：\n",
    "import torch.autograd.functional as F\n",
    "# 切割模型通讯量查看\n",
    "# for i in range(7):\n",
    "vgg5 = VGG('Client', 'VGG5', 1, model_cfg)\n",
    "\n",
    "client_outputs = vgg5(images)\n",
    "print('outputs.shape:',client_outputs.shape)\n",
    "jacs = F.jacobian(vgg5, images)\n",
    "print('jacobian: ', jacs)\n",
    "# print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))[1:]))\n",
    "print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))))\n",
    "\n",
    "# 0到6层每层的jacobians\n",
    "import torch.autograd.functional as F\n",
    "for i in range(7):\n",
    "    vgg5 = VGG('Client', 'VGG5', i, model_cfg)\n",
    "    client_outputs = vgg5(images)\n",
    "    print('outputs.shape:',client_outputs.shape)\n",
    "    jacs = F.jacobian(vgg5, images)\n",
    "    print('jacobian: ', jacs)\n",
    "    # print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))[1:]))\n",
    "    print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL 计算，摸索出来一条路\n",
    "import torch.autograd.functional as F\n",
    "# 参数：\n",
    "sigma = 0.01\n",
    "\n",
    "# 计算jacobian\n",
    "# 取一个batch的数据\n",
    "train_iter=iter(trainloader)\n",
    "inputs,labels = train_iter.next()6\n",
    "print(\"inputs.shape: \",inputs.shape)\n",
    "print(\"labels.shape: \",labels.shape)\n",
    "print(f\"input.requires_grad: {inputs.requires_grad}\")\n",
    "\n",
    "# 加载模型：\n",
    "# vgg5 = VGG('Client', 'VGG5', 1, model_cfg)\n",
    "\n",
    "# 进行前向传播：\n",
    "inputs.requires_grad_(True) # 需要求导\n",
    "outputs = vgg5(inputs)\n",
    "outputs = outputs + sigma * torch.randn_like(outputs) # 加噪声 (0,1] uniform\n",
    "print(\"outputs.shape: \",outputs.shape)\n",
    "\n",
    "# 1. 进行反向传播,计算jacobian\n",
    "# outputs.backward(torch.ones_like(outputs))\n",
    "# J = inputs.grad / sigma # 计算jacobian\n",
    "# print(f\"J1.shape: {J.shape}\")\n",
    "\n",
    "# 2. 重新计算jacobian（用torch.autograd.functional.jacobian函数）\n",
    "J = F.jacobian(vgg5, inputs)\n",
    "# print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "J = J.reshape(J.shape[0],outputs.numel(),inputs.numel())\n",
    "print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "\n",
    "# 计算eta 源论文\n",
    "# J = model.influence_jacobian(train_data)[:, :, :-1] / args.sigma  # 计算FIL（梯度）jacobian\n",
    "# etas = J.pow(2).sum(1).mean(1).sqrt() # 计算dFIL(这时候不是spectral norm了) \n",
    "\n",
    "# 计算eta：drj摸索：\n",
    "I = torch.matmul(J[0].t(), J[0])\n",
    "dFIL = I.trace().div(inputs.numel())\n",
    "eta = dFIL.sqrt()\n",
    "print(f\"eta: {eta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dFIL的两个要求: 可导 + unbiased\n",
    "# x = torch.rand_like(torch.Tensor([1,5]))\n",
    "x = torch.Tensor([0,0])\n",
    "x.requires_grad_(True)\n",
    "print(x.grad)\n",
    "y = torch.nn.ReLU()\n",
    "z = y(x).sum()\n",
    "# z = torch.autograd.functional.jacobian(y, x)\n",
    "z.backward()\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 现成函数调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# 我实现的：\n",
    "# dFIL inverse指标计算\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric = dFILInverseMetric()\n",
    "# 对traingloader遍历计算所有 inverse dFIL\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)):\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    \n",
    "    # inference\n",
    "    outputs = client_net(inputs)\n",
    "\n",
    "    eta = metric.quantify(model=client_net, inputs=inputs, outputs=outputs, with_outputs=True)\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'inv_dFIL.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL 计算函数\n",
    "import torch.autograd.functional as F\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# nips23\n",
    "from torch.autograd.functional import jvp\n",
    "import random\n",
    "import math\n",
    "\n",
    "def calc_tr(net, x, device, subsample=-1, jvp_parallelism=1): # nips'23 源码\n",
    "    '''\n",
    "    calc_tr 函数利用雅可比向量积（JVP）来估计网络对于输入数据的迹，\n",
    "    这在分析网络的灵敏度或稳定性时非常有用。\n",
    "    此外，通过支持子采样和并行处理，该函数还提供了一种在保持计算效率的同时估计迹的方法。\n",
    "    '''\n",
    "    print(f'x.shape: {x.shape}')\n",
    "    \n",
    "    # 定义一个局部函数 jvp_func**：这个函数接受两个参数 x 和 tgt，并返回 net.forward_first 方法的雅可比向量积（JVP）。\n",
    "    # 这意味着 jvp_func 用于计算网络对于输入 x 在方向 tgt 上的一阶导数\n",
    "    # tgt 计算雅各比向量积的向量\n",
    "    def jvp_func(x, tgt):\n",
    "        # return jvp(net.forward_first, (x,), (tgt,)) #返回 outputs, jacobian product\n",
    "        return jvp(net.forward, (x,), (tgt,)) #返回 outputs, jacobian product\n",
    "\n",
    "    # 获取一个batch中第一个数据的维度？d代表的是批次中第一个数据点展平后的特征数量，即输入数据的维度。\n",
    "    d = x[0].flatten().shape[0] # 把一个batch的x展平，获取input dim\n",
    "\n",
    "    # 用于存储每个输入数据点的迹\n",
    "    tr = torch.zeros(x.shape[0], dtype=x.dtype).to(device)\n",
    "    #print(f'd: {d}, {x.shape}')\n",
    "\n",
    "    # 加速，矩阵降维，但是这个损伤精度，或许改成特征提取更好点？\n",
    "    # Randomly subsample pixels for faster execution\n",
    "    if subsample > 0:\n",
    "        samples = random.sample(range(d), min(d, subsample))\n",
    "    else:\n",
    "        samples = range(d)\n",
    "\n",
    "    #print(x.shape, d, samples)\n",
    "    # jvp parallelism是数据并行的粒度？\n",
    "    # 函数通过分批处理样本来计算迹，每批处理 jvp_parallelism 个样本\n",
    "    for j in range(math.ceil(len(samples) / jvp_parallelism)): # 对于每个数据块\n",
    "        tgts = []\n",
    "        # 遍历每个数据块中的每个维度\n",
    "        for k in samples[j*jvp_parallelism:(j+1)*jvp_parallelism]: # 提取整个batch中每个数据的特定维度\n",
    "            tgt = torch.zeros_like(x).reshape(x.shape[0], -1) # 按照batch 排列？# 雅各比向量积的\n",
    "            # 除了当前样本索引 k 对应的元素设置为 1。这相当于在计算迹时，每次只关注一个特征维度。\n",
    "            tgt[:, k] = 1. # 提取tgt所有的样本的k的特征 计算雅各比向量积的向量，可用于计算trace\n",
    "            tgt = tgt.reshape(x.shape) # 又变回x的形状\n",
    "            tgts.append(tgt)\n",
    "        tgts = torch.stack(tgts)\n",
    "\n",
    "        def helper(tgt):\n",
    "            batch_size = x.shape[0]\n",
    "            vals_list = []\n",
    "            grads_list = []\n",
    "            for i in range(batch_size):\n",
    "                val, grad = jvp_func(x[i], tgt[i])  # 对每个批次元素调用jvp_func\n",
    "                vals_list.append(val)\n",
    "                grads_list.append(grad)\n",
    "            # 将结果列表转换为张量\n",
    "            vals = torch.stack(vals_list)\n",
    "            grad = torch.stack(grads_list)\n",
    "\n",
    "\n",
    "            # vals, grad = vmap(jvp_func, randomness='same')(x, tgt)\n",
    "            #print('grad shape: ', grad.shape)\n",
    "            return torch.sum(grad * grad, dim=tuple(range(1, len(grad.shape)))), vals # 先求迹再求平方\n",
    "\n",
    "        # vmap被替换\n",
    "        trs,vals = [],[]\n",
    "        for item in tgts:\n",
    "            trs_, vals_ = helper(item)\n",
    "            trs.append(trs_)\n",
    "            vals.append(vals_)\n",
    "        trs,vals = torch.stack(trs),torch.stack(vals)\n",
    "\n",
    "        # trs, vals = vmap(helper, randomness='same')(tgts) # randomness for randomness control of dropout\n",
    "        \n",
    "        # vals are stacked results that are repeated by d (should be all the same)\n",
    "\n",
    "\n",
    "        tr += trs.sum(dim=0)\n",
    "\n",
    "    # Scale if subsampled\n",
    "    if subsample > 0:\n",
    "        tr *= d / len(samples)\n",
    "\n",
    "    tr = tr/(d*1.0)\n",
    "    tr = 1.0/tr\n",
    "\n",
    "    print('tr: ',tr.shape, tr)\n",
    "    return tr.cpu().item(), vals[0].squeeze(1)  # squeeze removes one dimension jvp puts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 2, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:39<00:00, 39.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix:  [[0.00075323]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# maeng fisher\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric_trace = dFILInverseMetric()\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    \n",
    "    # inference\n",
    "    # outputs = client_net(inputs)\n",
    "\n",
    "    inputs = inputs.unsqueeze(0)\n",
    "    eta,val = metric_trace.calc_tr(net=client_net, x=inputs, device=args['device'])\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大\n",
    "print(\"matrix: \",matrix)\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'inv_dFIL_maeng.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. effective information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effective fisher 计算函数\n",
    "import torch.autograd.functional as F\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# nips23\n",
    "from torch.autograd.functional import jvp\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def computing_det_with_outputs(model, inputs, outputs, sigmas): # sigma_square\n",
    "        # batchsize:\n",
    "        batch_size = inputs.shape[0] # 一个batch的样本数目\n",
    "        output_size = outputs[0].numel() # 一个样本的outputs长度\n",
    "        input_size = inputs[0].numel() # 一个样本的outputs长度\n",
    "        effect_fisher_sum = 0.0\n",
    "\n",
    "        # 遍历单个样本: 换数据\n",
    "        for i in range(batch_size):\n",
    "            input_i = inputs[i].unsqueeze(0)\n",
    "\n",
    "            # 计算jacobian\n",
    "            J = F.jacobian(model, input_i)\n",
    "            # J = J.reshape(J.shape[0],outputs.numel(),inputs.numel()) # (batch, out_size, in_size)\n",
    "            J = J.reshape(output_size, input_size) # (batch, out_size, in_size)\n",
    "            # print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "            # 计算eta\n",
    "            JtJ = torch.matmul(J.t(), J)\n",
    "            I = 1.0/(sigmas)*JtJ\n",
    "            # ddFIL  = I.trace().div(input_size*input_size)\n",
    "\n",
    "            # 储存I\n",
    "            # I_np = I.cpu().detach().numpy()\n",
    "            # df = pd.DataFrame(I_np)\n",
    "            # df.to_csv(f'{i}.csv',index=False,header=False)\n",
    "\n",
    "            # print(\"I: \", I)\n",
    "            # w = torch.det(I)\n",
    "            # print('det I: ', I.det().log())\n",
    "\n",
    "            f1 = input_size * torch.log(2*torch.pi*torch.exp(torch.tensor(1.0)))\n",
    "            f2 = torch.logdet(I)\n",
    "            # print('log det I: ',f2 )\n",
    "            print('f1: ',f1)\n",
    "            print('f2: ',f2)\n",
    "            effect_fisher = 0.5 * (f1 - f2)\n",
    "            effect_fisher_sum+=effect_fisher\n",
    "\n",
    "            print(\"effect_fisher: \",effect_fisher)\n",
    "\n",
    "        # print(\"Jt*J: \", JtJ)\n",
    "        # print(\"Jt*J: \", JtJ.shape, JtJ)\n",
    "        # print(\"I.shape: \", I.shape)\n",
    "        # eta = dFIL\n",
    "        # print(f\"eta: {eta}\")\n",
    "        # print('t2-t1=',t2-t1, 't3-t2', t3-t2)\n",
    "        effect_fisher_mean = effect_fisher_sum / batch_size\n",
    "        return effect_fisher_mean.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  tensor([1, 0, 2])\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(28.3999, device='cuda:1')\n",
      "effect_fisher:  tensor(-8.5242, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(28.3999, device='cuda:1')\n",
      "effect_fisher:  tensor(-8.5242, device='cuda:1')\n",
      "f1:  tensor(11.3515)\n",
      "f2:  tensor(28.3999, device='cuda:1')\n",
      "effect_fisher:  tensor(-8.5242, device='cuda:1')\n",
      " effectfisher: -8.524190902709961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# effect fisher 指标计算\n",
    "\n",
    "effectFisher_same_layer_list = []\n",
    "Fishermetric = dFILInverseMetric() \n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    print('labels: ',labels)\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        # fisher\n",
    "        outputs = client_net(images)\n",
    "        # images = images.unsqueeze(0)\n",
    "        # effectFisher = Fishermetric._computing_det_with_outputs(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01)\n",
    "        effectFisher = computing_det_with_outputs(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01)\n",
    "\n",
    "        effectFisher_same_layer_list.append(effectFisher)\n",
    "# print(f\"Layer {split_layer} effecInfo: {sum(effecInfo_same_la yer_list)/len(effecInfo_same_layer_list)}\")\n",
    "print(f\" effectfisher: {sum(effectFisher_same_layer_list)/len(effectFisher_same_layer_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect entropy 计算函数\n",
    "import math\n",
    "import numpy as np\n",
    "def shannon_entropy_pyent(time_series):\n",
    "    \"\"\"Calculate Shannon Entropy of the sample data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_series: np.ndarray | list[str]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ent: float\n",
    "        The Shannon Entropy as float value\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate frequency counts\n",
    "    _, counts = np.unique(time_series, return_counts=True)\n",
    "    total_count = len(time_series)\n",
    "    # print('counts: ', counts)\n",
    "    # print(\"total_count: \",total_count)\n",
    "\n",
    "    # Calculate frequencies and Shannon entropy\n",
    "    frequencies = counts / total_count\n",
    "    # print(\"freq: \",frequencies)\n",
    "    ent = -np.sum(frequencies * np.log(frequencies))\n",
    "\n",
    "    return ent\n",
    "\n",
    "import infotopo\n",
    "\n",
    "def shannon_entropy_infotopo(x):\n",
    "    def _entropy(x,base='e'): # 此时x应该是一个概率分布（离散的)\n",
    "        x.flatten()\n",
    "        if base=='e': \n",
    "            y = x * np.log(x+1e-9) # y<=0 # log是以e为底的\n",
    "        else:\n",
    "            y = x * self._lnyx(x+1e-9,base=base)\n",
    "        # print(\"x.shape: \",x.shape)\n",
    "        # print(\"y.shape: \",y.shape)\n",
    "        return -np.sum(y) # 熵\n",
    "\n",
    "    information_top = infotopo.infotopo(dimension_max = x.shape[1],\n",
    "                                        dimension_tot = x.shape[1],\n",
    "                                        sample_size = x.shape[0],\n",
    "                                        nb_of_values = 9, # 不是很懂这个意思，为什么iris对应9？\n",
    "                                        )\n",
    "    # 计算联合分布的概率？（全排列）\n",
    "    joint_prob = information_top._compute_probability(x)\n",
    "    print('joint_prob: ',joint_prob)\n",
    "    # 计算联合熵（全排列的）\n",
    "    joint_prob_ent = information_top.simplicial_entropies_decomposition(x) # log2\n",
    "    new_joint_prob_ent = {key: value * np.log(2) for key, value in joint_prob_ent.items()} #ln 转2为底 成 e为底\n",
    "    print(\"joint_entropy: \",new_joint_prob_ent)\n",
    "    # ent = information_top._compute_forward_entropies(x)\n",
    "    # information_top.entropy_simplicial_lanscape(joint_prob) # 画图\n",
    "    ent = _entropy(np.array(list(new_joint_prob_ent.values())))\n",
    "    return ent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effectEntro_pyent:  4.394449154672438\n",
      "joint_prob:  {'0000': 0.3333333333333333, '0100': 0.3333333333333333, '0011': 0.3333333333333333}\n",
      "Percent of tuples processed : 0\n",
      "joint_entropy:  {(4,): 1.0986122886681096, (3,): 1.0986122886681096, (2,): 1.0986122886681096, (1,): 1.0986122886681096, (3, 4): 1.0986122886681096, (2, 4): 1.0986122886681096, (1, 4): 1.0986122886681096, (2, 3): 1.0986122886681096, (1, 3): 1.0986122886681096, (1, 2): 1.0986122886681096, (2, 3, 4): 1.0986122886681096, (1, 3, 4): 1.0986122886681096, (1, 2, 4): 1.0986122886681096, (1, 2, 3): 1.0986122886681096, (1, 2, 3, 4): 1.0986122886681096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyentrp import entropy as ent\n",
    "\n",
    "# effective entorpy\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): \n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    # print(images)\n",
    "    with torch.no_grad():\n",
    "        # ITE\n",
    "        # effectEntro = Shannon_quantity(images)\n",
    "        # print(\"effectEntro_ite: \",effectEntro)\n",
    "\n",
    "        # PyEntropy\n",
    "        effectEntro_pyent = 0.0\n",
    "        for i in range(len(images[0])): # 对每个维度\n",
    "            effectEntro_pyent  += shannon_entropy_pyent(images[:,i].flatten().detach().cpu().numpy())\n",
    "            # print('effectEntro_pyent: ',effectEntro_pyent)\n",
    "        # effectEntro_pyent = shannon_entropy(images.flatten(start_dim=1).detach().cpu().numpy())\n",
    "        print('effectEntro_pyent: ',effectEntro_pyent)\n",
    "\n",
    "        # infotopo\n",
    "        effectEntro_infotopo = shannon_entropy_infotopo(images.flatten(start_dim=1).detach().cpu().numpy())\n",
    "        # print('effectEntro_infotopo: ',effectEntro_infotopo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# x = next(iter(one_data_loader))\n",
    "for i,_ in one_data_loader:\n",
    "        # print(i)\n",
    "        y = scaler.fit_transform(i)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170.22239461082756\n",
      "335.1999980582977\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(10,341) # 最大能接受341个特征 \n",
    "z = torch.randn(100,200)\n",
    "e1 = Shannon_quantity(y)\n",
    "e2 = Shannon_quantity(z)\n",
    "print(e1)\n",
    "print(e2)\n",
    "# print(np.log(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n",
      "X_train.shape: (32950, 63)\n",
      "X_test.shape: (8238, 63)\n",
      "y_train.shape: (32950, 1)\n",
      "y_test.shape: (8238, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.289148</td>\n",
       "      <td>0.157198</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.063608</td>\n",
       "      <td>0.088614</td>\n",
       "      <td>0.032168</td>\n",
       "      <td>0.078174</td>\n",
       "      <td>0.064093</td>\n",
       "      <td>0.141539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284123</td>\n",
       "      <td>0.054413</td>\n",
       "      <td>0.020963</td>\n",
       "      <td>0.838213</td>\n",
       "      <td>0.081903</td>\n",
       "      <td>0.251712</td>\n",
       "      <td>0.335361</td>\n",
       "      <td>0.453437</td>\n",
       "      <td>0.089942</td>\n",
       "      <td>0.344142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.453394</td>\n",
       "      <td>0.364010</td>\n",
       "      <td>0.152031</td>\n",
       "      <td>0.146202</td>\n",
       "      <td>0.244068</td>\n",
       "      <td>0.284203</td>\n",
       "      <td>0.176457</td>\n",
       "      <td>0.268462</td>\n",
       "      <td>0.244934</td>\n",
       "      <td>0.348598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171524</td>\n",
       "      <td>0.051057</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.366935</td>\n",
       "      <td>0.121449</td>\n",
       "      <td>0.146193</td>\n",
       "      <td>0.252907</td>\n",
       "      <td>0.286649</td>\n",
       "      <td>0.055046</td>\n",
       "      <td>0.179637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160494</td>\n",
       "      <td>0.022977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.199532</td>\n",
       "      <td>0.192469</td>\n",
       "      <td>0.036953</td>\n",
       "      <td>0.203781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234568</td>\n",
       "      <td>0.040057</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.269680</td>\n",
       "      <td>0.418410</td>\n",
       "      <td>0.093403</td>\n",
       "      <td>0.425709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382716</td>\n",
       "      <td>0.068524</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.296960</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.143278</td>\n",
       "      <td>0.512287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769622</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150759</td>\n",
       "      <td>0.512287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  8238.000000  8238.000000  8238.000000  8238.000000  8238.000000   \n",
       "mean      0.289148     0.157198     0.023671     0.021850     0.063608   \n",
       "std       0.453394     0.364010     0.152031     0.146202     0.244068   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  8238.000000  8238.000000  8238.000000  8238.000000  8238.000000  ...   \n",
       "mean      0.088614     0.032168     0.078174     0.064093     0.141539  ...   \n",
       "std       0.284203     0.176457     0.268462     0.244934     0.348598  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "                53           54           55           56           57  \\\n",
       "count  8238.000000  8238.000000  8238.000000  8238.000000  8238.000000   \n",
       "mean      0.284123     0.054413     0.020963     0.838213     0.081903   \n",
       "std       0.171524     0.051057     0.030769     0.366935     0.121449   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.160494     0.022977     0.000000     1.000000     0.000000   \n",
       "50%       0.234568     0.040057     0.018182     1.000000     0.000000   \n",
       "75%       0.382716     0.068524     0.036364     1.000000     0.142857   \n",
       "max       1.000000     0.769622     0.272727     1.000000     1.000000   \n",
       "\n",
       "                58           59           60           61           62  \n",
       "count  8238.000000  8238.000000  8238.000000  8238.000000  8238.000000  \n",
       "mean      0.251712     0.335361     0.453437     0.089942     0.344142  \n",
       "std       0.146193     0.252907     0.286649     0.055046     0.179637  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.104167     0.199532     0.192469     0.036953     0.203781  \n",
       "50%       0.333333     0.269680     0.418410     0.093403     0.425709  \n",
       "75%       0.333333     0.296960     0.719665     0.143278     0.512287  \n",
       "max       0.479167     1.000000     1.000000     0.150759     0.512287  \n",
       "\n",
       "[8 rows x 63 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataPath = '/home/dengruijun/data/FinTech/DATASET/kaggle-dataset/bank/bank-additional-full.csv'\n",
    "\n",
    "[X_train, y_train], [X_test, y_test] = preprocess_bank_dataset(dataPath)\n",
    "df = pd.DataFrame(X_test)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effecEntro:  0.587471032886238\n",
      "I:  tensor([[ 7.5991,  0.0214, -0.1178,  ...,  0.1394, -0.0462,  0.2453],\n",
      "        [ 0.0214,  9.0257, -0.1333,  ..., -0.0265, -0.7587, -0.6064],\n",
      "        [-0.1178, -0.1333,  7.9887,  ...,  0.1769,  0.4307,  0.1134],\n",
      "        ...,\n",
      "        [ 0.1394, -0.0265,  0.1769,  ...,  7.8924,  0.1008, -0.3792],\n",
      "        [-0.0462, -0.7587,  0.4307,  ...,  0.1008,  8.1875, -0.1230],\n",
      "        [ 0.2453, -0.6064,  0.1134,  ..., -0.3792, -0.1230,  7.2750]],\n",
      "       device='cuda:1')\n",
      "det I:  tensor(-inf, device='cuda:1')\n",
      "log det I:  tensor(-575.6401, device='cuda:1')\n",
      "effect_fisher:  tensor(1139.1831, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:  tensor([[ 6.5576e+00,  7.9808e-02, -1.4507e-01,  ...,  3.2295e-02,\n",
      "          4.8863e-02,  1.3413e-01],\n",
      "        [ 7.9808e-02,  7.9728e+00, -1.3367e-01,  ..., -1.0713e-01,\n",
      "         -6.0581e-01, -5.0328e-01],\n",
      "        [-1.4507e-01, -1.3367e-01,  7.2373e+00,  ...,  2.4361e-01,\n",
      "          2.0677e-01,  3.4368e-03],\n",
      "        ...,\n",
      "        [ 3.2295e-02, -1.0713e-01,  2.4361e-01,  ...,  7.0409e+00,\n",
      "          1.0706e-01, -3.2667e-01],\n",
      "        [ 4.8863e-02, -6.0581e-01,  2.0677e-01,  ...,  1.0706e-01,\n",
      "          7.1875e+00, -1.9418e-01],\n",
      "        [ 1.3413e-01, -5.0328e-01,  3.4368e-03,  ..., -3.2667e-01,\n",
      "         -1.9418e-01,  6.4056e+00]], device='cuda:1')\n",
      "det I:  tensor(-inf, device='cuda:1')\n",
      "log det I:  tensor(-674.2421, device='cuda:1')\n",
      "effect_fisher:  tensor(1188.4841, device='cuda:1')\n",
      "Layer 3 effecInfo: -1163.2461471311763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# effect Information 指标计算\n",
    "\n",
    "effecInfo_diff_layer_list = []\n",
    "effecInfo_same_layer_list = []\n",
    "EntropyMetric = ULossMetric()\n",
    "Fishermetric = dFILInverseMetric()\n",
    "\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        # entropy \n",
    "        effecEntro= EntropyMetric._entropy_prob_batch(images) # H(x)\n",
    "        print(\"effecEntro: \",effecEntro)\n",
    "\n",
    "        # fisher\n",
    "        outputs = client_net(images)\n",
    "        # effectFisher = Fishermetric._computing_det_with_outputs(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01)\n",
    "        effectFisher = computing_det_with_outputs(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01)\n",
    "\n",
    "        # 存储\n",
    "        effecInfo_same_layer_list.append(effecEntro-effectFisher)\n",
    "        \n",
    "print(f\"Layer {split_layer} effecInfo: {sum(effecInfo_same_layer_list)/len(effecInfo_same_layer_list)}\")\n",
    "effecInfo_diff_layer_list.append(effecInfo_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(effecInfo_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(results_dir + f'effecInfo-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'effecInfo.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
