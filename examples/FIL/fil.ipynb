{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基础设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '48'\n",
    "\n",
    "# 导入各个指标\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.quantification.distance_correlation.distCor import distCorMetric\n",
    "from ppsplit.quantification.fisher_information.dFIL_inverse import dFILInverseMetric\n",
    "from ppsplit.quantification.shannon_information.mutual_information import MuInfoMetric\n",
    "from ppsplit.quantification.shannon_information.ULoss import ULossMetric\n",
    "from ppsplit.quantification.rep_reading.rep_reader import PCA_Reader\n",
    "from ppsplit.quantification.shannon_information.ITE_tools import Shannon_quantity\n",
    "\n",
    "from target_model.task_select import get_dataloader_and_model,get_dataloader_and_model, get_dataloader,get_models\n",
    "\n",
    "# utils\n",
    "from ppsplit.utils.utils import create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "        'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        # 'device':torch.device(\"cpu\"),\n",
    "        # 'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        'dataset':'Iris',\n",
    "        # 'result_dir': '20240702-FIL/',\n",
    "        'result_dir': '20240702-effectiveInfo/',\n",
    "        'oneData_bs': 30,\n",
    "        'test_bs': 1,\n",
    "        'train_bs':1,\n",
    "        'noise_scale':0, # 防护措施\n",
    "        'OneData':True,\n",
    "        'split_layer': 3,\n",
    "        # 'test_num': 'invdFIL', # MI, invdFIL, distCor, ULoss, \n",
    "        'test_num': 'effectiveInfo'\n",
    "        }\n",
    "print(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n",
      "X_train.shape: (120, 4)\n",
      "X_test.shape: (30, 4)\n",
      "y_train.shape: (120,)\n",
      "y_test.shape: (30,) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n",
      "linear2.weight\n",
      "linear2.bias\n",
      "train decoder model...\n",
      "['Tanh', ('D', 128, 64), 'Tanh', ('D', 4, 128)]\n",
      "results_dir: ../../results/20240702-effectiveInfo//Iris/effectiveInfo/\n",
      "inverse_dir: ../../results/20240702-effectiveInfo//Iris/effectiveInfo/layer3/\n",
      "decoder_route: ../../results/20240702-effectiveInfo//Iris/effectiveInfo//Decoder-layer3.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IrisNet(\n",
       "  (linear1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_msg = get_dataloader(args)\n",
    "model_msg = get_models(args)\n",
    "msg = {**model_msg,**data_msg}\n",
    "\n",
    "# 数据集\n",
    "one_data_loader,trainloader,testloader = data_msg['one_data_loader'],data_msg['trainloader'], data_msg['testloader']\n",
    "\n",
    "# 模型和路径\n",
    "client_net,decoder_net = model_msg['client_net'],model_msg['decoder_net']\n",
    "decoder_route = model_msg['decoder_route']\n",
    "image_deprocess = model_msg['image_deprocess']\n",
    "\n",
    "results_dir = model_msg['results_dir']\n",
    "inverse_dir = results_dir + 'layer'+str(args['split_layer'])+'/'\n",
    "data_type = 1 if args['dataset'] == 'CIFAR10' else 0\n",
    "split_layer = args['split_layer']\n",
    "\n",
    "print('results_dir:',results_dir)\n",
    "print('inverse_dir:',inverse_dir)\n",
    "print('decoder_route:',decoder_route)\n",
    "\n",
    "create_dir(results_dir)\n",
    "\n",
    "# client_net使用\n",
    "client_net = client_net.to(args['device'])\n",
    "client_net.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. pytorch的自动梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "True\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "import torch\n",
    "\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "# 信息打印\n",
    "print(x.shape)\n",
    "print(x.requires_grad)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向传播\n",
    "loss.backward()\n",
    "print(x.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad/backward 张量求梯度 sum\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.randn(5, 3)\n",
    "b = torch.randn(3)\n",
    "z = torch.matmul(x, w)+b\n",
    "\n",
    "# 计算sumed雅可比矩阵\n",
    "z.backward(torch.ones_like(z))\n",
    "print(x.grad.shape)\n",
    "print(x.grad)\n",
    "\n",
    "# 用grad\n",
    "from torch.autograd import grad\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.eye(5, 3)\n",
    "b = torch.randn(3)\n",
    "z = torch.matmul(x, w)+b\n",
    "xgrad = grad(z, x, grad_outputs=torch.ones_like(z))[0]\n",
    "print(xgrad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jacobian\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.autograd import grad\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "\n",
    "def forward(x):\n",
    "    y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "    w = torch.eye(5, 3)\n",
    "    b = torch.randn(3)\n",
    "    z = torch.matmul(x, w)+b\n",
    "    return z\n",
    "\n",
    "\n",
    "# 计算sumed雅可比矩阵\n",
    "jac = jacobian(forward, x)\n",
    "print(jac.shape)\n",
    "print(jac)\n",
    "print(jac.sum(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 试探FIL计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL计算：\n",
    "import torch.autograd.functional as F\n",
    "# 切割模型通讯量查看\n",
    "# for i in range(7):\n",
    "vgg5 = VGG('Client', 'VGG5', 1, model_cfg)\n",
    "\n",
    "client_outputs = vgg5(images)\n",
    "print('outputs.shape:',client_outputs.shape)\n",
    "jacs = F.jacobian(vgg5, images)\n",
    "print('jacobian: ', jacs)\n",
    "# print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))[1:]))\n",
    "print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))))\n",
    "\n",
    "# 0到6层每层的jacobians\n",
    "import torch.autograd.functional as F\n",
    "for i in range(7):\n",
    "    vgg5 = VGG('Client', 'VGG5', i, model_cfg)\n",
    "    client_outputs = vgg5(images)\n",
    "    print('outputs.shape:',client_outputs.shape)\n",
    "    jacs = F.jacobian(vgg5, images)\n",
    "    print('jacobian: ', jacs)\n",
    "    # print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))[1:]))\n",
    "    print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL 计算，摸索出来一条路\n",
    "import torch.autograd.functional as F\n",
    "# 参数：\n",
    "sigma = 0.01\n",
    "\n",
    "# 计算jacobian\n",
    "# 取一个batch的数据\n",
    "train_iter=iter(trainloader)\n",
    "inputs,labels = train_iter.next()6\n",
    "print(\"inputs.shape: \",inputs.shape)\n",
    "print(\"labels.shape: \",labels.shape)\n",
    "print(f\"input.requires_grad: {inputs.requires_grad}\")\n",
    "\n",
    "# 加载模型：\n",
    "# vgg5 = VGG('Client', 'VGG5', 1, model_cfg)\n",
    "\n",
    "# 进行前向传播：\n",
    "inputs.requires_grad_(True) # 需要求导\n",
    "outputs = vgg5(inputs)\n",
    "outputs = outputs + sigma * torch.randn_like(outputs) # 加噪声 (0,1] uniform\n",
    "print(\"outputs.shape: \",outputs.shape)\n",
    "\n",
    "# 1. 进行反向传播,计算jacobian\n",
    "# outputs.backward(torch.ones_like(outputs))\n",
    "# J = inputs.grad / sigma # 计算jacobian\n",
    "# print(f\"J1.shape: {J.shape}\")\n",
    "\n",
    "# 2. 重新计算jacobian（用torch.autograd.functional.jacobian函数）\n",
    "J = F.jacobian(vgg5, inputs)\n",
    "# print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "J = J.reshape(J.shape[0],outputs.numel(),inputs.numel())\n",
    "print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "\n",
    "# 计算eta 源论文\n",
    "# J = model.influence_jacobian(train_data)[:, :, :-1] / args.sigma  # 计算FIL（梯度）jacobian\n",
    "# etas = J.pow(2).sum(1).mean(1).sqrt() # 计算dFIL(这时候不是spectral norm了) \n",
    "\n",
    "# 计算eta：drj摸索：\n",
    "I = torch.matmul(J[0].t(), J[0])\n",
    "dFIL = I.trace().div(inputs.numel())\n",
    "eta = dFIL.sqrt()\n",
    "print(f\"eta: {eta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dFIL的两个要求: 可导 + unbiased\n",
    "# x = torch.rand_like(torch.Tensor([1,5]))\n",
    "x = torch.Tensor([0,0])\n",
    "x.requires_grad_(True)\n",
    "print(x.grad)\n",
    "y = torch.nn.ReLU()\n",
    "z = y(x).sum()\n",
    "# z = torch.autograd.functional.jacobian(y, x)\n",
    "z.backward()\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 现成函数调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.0611473e-05]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 我实现的：\n",
    "# dFIL inverse指标计算\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric = dFILInverseMetric()\n",
    "# 对traingloader遍历计算所有 inverse dFIL\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)):\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    \n",
    "    # inference\n",
    "    outputs = client_net(inputs)\n",
    "\n",
    "    eta = metric.quantify(model=client_net, inputs=inputs, outputs=outputs, with_outputs=True)\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'inv_dFIL{split_layer}.csv',index=False)\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tr(net, x, device, sigmas=0.01, subsample=-1, jvp_parallelism=1): # nips'23 源码\n",
    "    '''\n",
    "    calc_tr 函数利用雅可比向量积（JVP）来估计网络对于输入数据的迹，\n",
    "    这在分析网络的灵敏度或稳定性时非常有用。\n",
    "    此外，通过支持子采样和并行处理，该函数还提供了一种在保持计算效率的同时估计迹的方法。\n",
    "    '''\n",
    "    print(f'x.shape: {x.shape}')\n",
    "    \n",
    "    # 定义一个局部函数 jvp_func**：这个函数接受两个参数 x 和 tgt，并返回 net.forward_first 方法的雅可比向量积（JVP）。\n",
    "    # 这意味着 jvp_func 用于计算网络对于输入 x 在方向 tgt 上的一阶导数\n",
    "    # tgt 计算雅各比向量积的向量\n",
    "    def jvp_func(x, tgt):\n",
    "        # return jvp(net.forward_first, (x,), (tgt,)) #返回 outputs, jacobian product\n",
    "        return jvp(net.forward, (x,), (tgt,)) #返回 outputs, jacobian product\n",
    "\n",
    "    # 获取一个batch中第一个数据的维度？d代表的是批次中第一个数据点展平后的特征数量，即输入数据的维度。\n",
    "    d = x[0].flatten().shape[0] # 把一个batch的x展平，获取input dim\n",
    "\n",
    "    # 用于存储每个输入数据点的迹，求迹的和。\n",
    "    tr = torch.zeros(x.shape[0], dtype=x.dtype).to(device)\n",
    "    #print(f'd: {d}, {x.shape}')\n",
    "\n",
    "    # 加速，矩阵降维，但是这个损伤精度，或许改成特征提取更好点？\n",
    "    # Randomly subsample pixels for faster execution\n",
    "    if subsample > 0:\n",
    "        samples = random.sample(range(d), min(d, subsample))\n",
    "    else:\n",
    "        samples = range(d)\n",
    "\n",
    "    #print(x.shape, d, samples)\n",
    "    # jvp parallelism是数据并行的粒度？\n",
    "    # 函数通过分批处理样本来计算迹，每批处理 jvp_parallelism 个样本\n",
    "    for j in range(math.ceil(len(samples) / jvp_parallelism)): # 对于每个数据块\n",
    "        tgts = []\n",
    "\n",
    "        # 遍历每个数据块中的每个维度\n",
    "        '''\n",
    "        在这个函数中，tgt 是用于计算雅可比向量积（JVP）的向量。具体来说，tgt 的作用如下：\n",
    "        构建雅可比向量积的向量：tgt 是一个与输入 x 形状相同的张量，但它的元素大部分为零，只有一个特定位置的元素为 1。这个特定位置对应于我们在计算迹时关注的特征维度。\n",
    "        计算 JVP：在 helper 函数中，tgt 被传递给 jvp_func，用于计算网络对于输入 x 在方向 tgt 上的一阶导数。具体来说，jvp_func 计算的是网络输出相对于输入 x 的雅可比矩阵与 tgt 的乘积。\n",
    "        估计迹：通过在不同的特征维度上重复上述过程，可以估计网络对于输入数据的迹。迹的计算涉及到对所有特征维度的导数进行求和，而 tgt 的作用就是在每次计算时只关注一个特征维度。\n",
    "        简而言之，tgt 是一个用于选择特定特征维度的向量，通过它可以逐个计算每个特征维度的导数，从而最终估计整个输入数据的迹。\n",
    "        '''\n",
    "        for k in samples[j*jvp_parallelism:(j+1)*jvp_parallelism]: # 提取整个batch中每个数据的特定维度\n",
    "            tgt = torch.zeros_like(x).reshape(x.shape[0], -1) # 按照batch 排列？# 雅各比向量积的\n",
    "            # 除了当前样本索引 k 对应的元素设置为 1。这相当于在计算迹时，每次只关注一个特征维度。\n",
    "            tgt[:, k] = 1. # 提取tgt所有的样本的k的特征 计算雅各比向量积的向量，可用于计算trace\n",
    "            tgt = tgt.reshape(x.shape) # 又变回x的形状\n",
    "            tgts.append(tgt)\n",
    "        tgts = torch.stack(tgts)\n",
    "\n",
    "        # 定义一个辅助函数 helper，该函数接受一个目标张量 tgt并返回一个迹的张量和一个值的张量。\n",
    "        # jvp wrapper，遍历每个batchsize\n",
    "        def helper(tgt):\n",
    "            batch_size = x.shape[0]\n",
    "            vals_list = []\n",
    "            grads_list = []\n",
    "            for i in range(batch_size):\n",
    "                val, grad = jvp_func(x[i], tgt[i])  # 对每个批次元素调用jvp_func\n",
    "                vals_list.append(val)\n",
    "                grads_list.append(grad)\n",
    "            # 将结果列表转换为张量, 多个batch的给stack起来\n",
    "            vals = torch.stack(vals_list)\n",
    "            grad = torch.stack(grads_list)\n",
    "\n",
    "\n",
    "            # vals, grad = vmap(jvp_func, randomness='same')(x, tgt)\n",
    "            #print('grad shape: ', grad.shape)\n",
    "            # 因此，矩阵平方的迹和迹的平方通常是不相等的。\n",
    "            # 先求平方再求迹\n",
    "            return torch.sum(grad * grad, dim=tuple(range(1, len(grad.shape)))), vals \n",
    "\n",
    "        # vmap被替换，\n",
    "        # 遍历每个数据块\n",
    "        trs,vals = [],[]\n",
    "        for item in tgts:\n",
    "            trs_, vals_ = helper(item)\n",
    "            trs.append(trs_) # 每个batch对应一个向量\n",
    "            vals.append(vals_)\n",
    "        trs,vals = torch.stack(trs),torch.stack(vals)\n",
    "        print(trs)\n",
    "        # trs, vals = vmap(helper, randomness='same')(tgts) # randomness for randomness control of dropout\n",
    "        \n",
    "        # vals are stacked results that are repeated by d (should be all the same)\n",
    "\n",
    "\n",
    "        tr += trs.sum(dim=0) # 对每个数据块的迹求和\n",
    "\n",
    "    # Scale if subsampled\n",
    "    if subsample > 0:\n",
    "        tr *= d / len(samples)\n",
    "\n",
    "    # 1/dFIL = d/tr(I)\n",
    "    tr = tr/(d*1.0)\n",
    "    tr = 1.0/tr*sigmas\n",
    "\n",
    "    # print('tr: ',tr.shape, tr)\n",
    "    return tr.cpu().item(), vals[0].squeeze(1)  # squeeze removes one dimension jvp puts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 30, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix:  [[2.06114892e-05]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# maeng fisher\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric_trace = dFILInverseMetric()\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    \n",
    "    # inference\n",
    "    # outputs = client_net(inputs)\n",
    "\n",
    "    inputs = inputs.unsqueeze(0)\n",
    "    eta,val = metric_trace.calc_tr(net=client_net, x=inputs, device=args['device'])\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大\n",
    "print(\"matrix: \",matrix)\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'inv_dFIL_maeng-{split_layer}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
