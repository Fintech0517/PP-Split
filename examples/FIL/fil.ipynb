{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基础设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '48'\n",
    "\n",
    "# 导入各个指标\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.quantification.distance_correlation.distCor import distCorMetric\n",
    "from ppsplit.quantification.fisher_information.dFIL_inverse import dFILInverseMetric\n",
    "from ppsplit.quantification.shannon_information.mutual_information import MuInfoMetric\n",
    "from ppsplit.quantification.shannon_information.ULoss import ULossMetric\n",
    "from ppsplit.quantification.rep_reading.rep_reader import PCA_Reader\n",
    "from ppsplit.quantification.shannon_information.ITE_tools import Shannon_quantity\n",
    "\n",
    "from target_model.task_select import get_dataloader_and_model,get_dataloader_and_model, \\\n",
    "    get_dataloader,get_models,get_infotopo_para\n",
    "# utils\n",
    "from ppsplit.utils.utils import create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "{'device': device(type='cpu'), 'dataset': 'CIFAR10', 'model': 'ResNet18', 'result_dir': '20240904-fisher/', 'oneData_bs': 1, 'test_bs': 1, 'train_bs': 1, 'noise_scale': 0, 'split_layer': 3, 'test_num': 'maeng', 'no_dense': True}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# nohup python -u effectInfo.py > ../../results/20240702-effectiveInfo/VGG5/effectiveInfo1.3/effectInfo1.3-pool4.log 2>&1 &\n",
    "args = {\n",
    "        # 'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'device':torch.device(\"cpu\"),\n",
    "        'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        # 'dataset':'Iris',\n",
    "        'model': 'ResNet18',\n",
    "        # 'model': 'VGG5',\n",
    "        # 'result_dir': '20240702-FIL/',\n",
    "        # 'result_dir': '20240702-effectiveInfo/',\n",
    "        'result_dir': '20240904-fisher/',\n",
    "        'oneData_bs': 1,\n",
    "        'test_bs': 1,\n",
    "        'train_bs': 1,\n",
    "        'noise_scale': 0, # 防护措施\n",
    "        'split_layer': 3,\n",
    "        # 'test_num': 'invdFIL', # MI, invdFIL, distCor, ULoss,  # split layer [2,3,5,7,9,11] for ResNet18\n",
    "        'test_num': 'maeng',\n",
    "        'no_dense':True,\n",
    "        }\n",
    "print(args['device'])\n",
    "print(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num unit layers: 14\n",
      "Split layer: 3\n",
      "client_net weights:  dict_keys(['selected_layers.0.weight', 'selected_layers.1.weight', 'selected_layers.1.bias', 'selected_layers.1.running_mean', 'selected_layers.1.running_var', 'selected_layers.1.num_batches_tracked', 'selected_layers.4.conv1.weight', 'selected_layers.4.bn1.weight', 'selected_layers.4.bn1.bias', 'selected_layers.4.bn1.running_mean', 'selected_layers.4.bn1.running_var', 'selected_layers.4.bn1.num_batches_tracked', 'selected_layers.4.conv2.weight', 'selected_layers.4.bn2.weight', 'selected_layers.4.bn2.bias', 'selected_layers.4.bn2.running_mean', 'selected_layers.4.bn2.running_var', 'selected_layers.4.bn2.num_batches_tracked', 'selected_layers.5.conv1.weight', 'selected_layers.5.bn1.weight', 'selected_layers.5.bn1.bias', 'selected_layers.5.bn1.running_mean', 'selected_layers.5.bn1.running_var', 'selected_layers.5.bn1.num_batches_tracked', 'selected_layers.5.conv2.weight', 'selected_layers.5.bn2.weight', 'selected_layers.5.bn2.bias', 'selected_layers.5.bn2.running_mean', 'selected_layers.5.bn2.running_var', 'selected_layers.5.bn2.num_batches_tracked', 'selected_layers.6.conv1.weight', 'selected_layers.6.bn1.weight', 'selected_layers.6.bn1.bias', 'selected_layers.6.bn1.running_mean', 'selected_layers.6.bn1.running_var', 'selected_layers.6.bn1.num_batches_tracked', 'selected_layers.6.conv2.weight', 'selected_layers.6.bn2.weight', 'selected_layers.6.bn2.bias', 'selected_layers.6.bn2.running_mean', 'selected_layers.6.bn2.running_var', 'selected_layers.6.bn2.num_batches_tracked', 'selected_layers.6.downsample.0.weight', 'selected_layers.6.downsample.1.weight', 'selected_layers.6.downsample.1.bias', 'selected_layers.6.downsample.1.running_mean', 'selected_layers.6.downsample.1.running_var', 'selected_layers.6.downsample.1.num_batches_tracked', 'selected_layers.7.conv1.weight', 'selected_layers.7.bn1.weight', 'selected_layers.7.bn1.bias', 'selected_layers.7.bn1.running_mean', 'selected_layers.7.bn1.running_var', 'selected_layers.7.bn1.num_batches_tracked', 'selected_layers.7.conv2.weight', 'selected_layers.7.bn2.weight', 'selected_layers.7.bn2.bias', 'selected_layers.7.bn2.running_mean', 'selected_layers.7.bn2.running_var', 'selected_layers.7.bn2.num_batches_tracked', 'selected_layers.8.conv1.weight', 'selected_layers.8.bn1.weight', 'selected_layers.8.bn1.bias', 'selected_layers.8.bn1.running_mean', 'selected_layers.8.bn1.running_var', 'selected_layers.8.bn1.num_batches_tracked', 'selected_layers.8.conv2.weight', 'selected_layers.8.bn2.weight', 'selected_layers.8.bn2.bias', 'selected_layers.8.bn2.running_mean', 'selected_layers.8.bn2.running_var', 'selected_layers.8.bn2.num_batches_tracked', 'selected_layers.8.downsample.0.weight', 'selected_layers.8.downsample.1.weight', 'selected_layers.8.downsample.1.bias', 'selected_layers.8.downsample.1.running_mean', 'selected_layers.8.downsample.1.running_var', 'selected_layers.8.downsample.1.num_batches_tracked', 'selected_layers.9.conv1.weight', 'selected_layers.9.bn1.weight', 'selected_layers.9.bn1.bias', 'selected_layers.9.bn1.running_mean', 'selected_layers.9.bn1.running_var', 'selected_layers.9.bn1.num_batches_tracked', 'selected_layers.9.conv2.weight', 'selected_layers.9.bn2.weight', 'selected_layers.9.bn2.bias', 'selected_layers.9.bn2.running_mean', 'selected_layers.9.bn2.running_var', 'selected_layers.9.bn2.num_batches_tracked', 'selected_layers.10.conv1.weight', 'selected_layers.10.bn1.weight', 'selected_layers.10.bn1.bias', 'selected_layers.10.bn1.running_mean', 'selected_layers.10.bn1.running_var', 'selected_layers.10.bn1.num_batches_tracked', 'selected_layers.10.conv2.weight', 'selected_layers.10.bn2.weight', 'selected_layers.10.bn2.bias', 'selected_layers.10.bn2.running_mean', 'selected_layers.10.bn2.running_var', 'selected_layers.10.bn2.num_batches_tracked', 'selected_layers.10.downsample.0.weight', 'selected_layers.10.downsample.1.weight', 'selected_layers.10.downsample.1.bias', 'selected_layers.10.downsample.1.running_mean', 'selected_layers.10.downsample.1.running_var', 'selected_layers.10.downsample.1.num_batches_tracked', 'selected_layers.11.conv1.weight', 'selected_layers.11.bn1.weight', 'selected_layers.11.bn1.bias', 'selected_layers.11.bn1.running_mean', 'selected_layers.11.bn1.running_var', 'selected_layers.11.bn1.num_batches_tracked', 'selected_layers.11.conv2.weight', 'selected_layers.11.bn2.weight', 'selected_layers.11.bn2.bias', 'selected_layers.11.bn2.running_mean', 'selected_layers.11.bn2.running_var', 'selected_layers.11.bn2.num_batches_tracked', 'selected_layers.13.weight', 'selected_layers.13.bias'])\n",
      "client_net cweights:  odict_keys(['selected_layers.0.weight', 'selected_layers.1.weight', 'selected_layers.1.bias', 'selected_layers.1.running_mean', 'selected_layers.1.running_var', 'selected_layers.1.num_batches_tracked'])\n",
      "len client_net weights:  122\n",
      "len client_net cweights:  6\n",
      "selected_layers.0.weight\n",
      "selected_layers.1.weight\n",
      "selected_layers.1.bias\n",
      "selected_layers.1.running_mean\n",
      "selected_layers.1.running_var\n",
      "selected_layers.1.num_batches_tracked\n",
      "train decoder model...\n",
      "infotopo: nb_of_values:  36\n",
      "results_dir: ../../results/20240904-fisher//Resnet18/maeng/\n",
      "inverse_dir: ../../results/20240904-fisher//Resnet18/maeng/layer3/\n",
      "decoder_route: ../../results/20240904-fisher//Resnet18/maeng//Decoder-layer3.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (selected_layers): ModuleList(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate=none)\n",
       "    (3): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_msg = get_dataloader(args)\n",
    "model_msg = get_models(args)\n",
    "infotopo_msg = get_infotopo_para(args)\n",
    "msg = {**model_msg,**data_msg,**infotopo_msg}\n",
    "\n",
    "# 数据集\n",
    "one_data_loader,trainloader,testloader = data_msg['one_data_loader'],data_msg['trainloader'], data_msg['testloader']\n",
    "\n",
    "# effectEntropy Infotopo参数\n",
    "nb_of_values = msg['nb_of_values']\n",
    "conv = msg['conv']\n",
    "print(\"infotopo: nb_of_values: \",nb_of_values)\n",
    "\n",
    "# 模型\n",
    "client_net,decoder_net = model_msg['client_net'],model_msg['decoder_net']\n",
    "decoder_route = model_msg['decoder_route']\n",
    "image_deprocess = model_msg['image_deprocess']\n",
    "\n",
    "# 路径\n",
    "results_dir = model_msg['results_dir']\n",
    "inverse_dir = results_dir + 'layer'+str(args['split_layer'])+'/'\n",
    "data_type = 1 if args['dataset'] == 'CIFAR10' else 0\n",
    "split_layer = args['split_layer']\n",
    "\n",
    "print('results_dir:' ,results_dir)\n",
    "print('inverse_dir:' ,inverse_dir)\n",
    "print('decoder_route:' ,decoder_route)\n",
    "\n",
    "create_dir(results_dir)\n",
    "\n",
    "# client_net使用\n",
    "client_net = client_net.to(args['device'])\n",
    "client_net.eval()\n",
    "\n",
    "# for n, p in client_net.named_parameters():\n",
    "#     print(n, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. pytorch的自动梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "True\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "import torch\n",
    "\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "# 信息打印\n",
    "print(x.shape)\n",
    "print(x.requires_grad)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# 反向传播\n",
    "loss.backward()\n",
    "print(x.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "tensor([-0.0555, -1.7560, -1.1999, -0.3189,  0.5489])\n",
      "tensor([1., 1., 1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# grad/backward 张量求梯度 sum\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.randn(5, 3)\n",
    "b = torch.randn(3)\n",
    "z = torch.matmul(x, w)+b\n",
    "\n",
    "# 计算sumed雅可比矩阵\n",
    "z.backward(torch.ones_like(z))\n",
    "print(x.grad.shape)\n",
    "print(x.grad)\n",
    "\n",
    "# 用grad\n",
    "from torch.autograd import grad\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.eye(5, 3)\n",
    "b = torch.randn(3)\n",
    "z = torch.matmul(x, w)+b\n",
    "xgrad = grad(z, x, grad_outputs=torch.ones_like(z))[0]\n",
    "print(xgrad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jacobian\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.autograd import grad\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "\n",
    "def forward(x):\n",
    "    y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "    w = torch.eye(5, 3)\n",
    "    b = torch.randn(3)\n",
    "    z = torch.matmul(x, w)+b\n",
    "    return z\n",
    "\n",
    "\n",
    "# 计算sumed雅可比矩阵\n",
    "jac = jacobian(forward, x)\n",
    "print(jac.shape)\n",
    "print(jac)\n",
    "print(jac.sum(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 试探FIL计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL计算：\n",
    "import torch.autograd.functional as F\n",
    "# 切割模型通讯量查看\n",
    "# for i in range(7):\n",
    "vgg5 = VGG('Client', 'VGG5', 1, model_cfg)\n",
    "\n",
    "client_outputs = vgg5(images)\n",
    "print('outputs.shape:',client_outputs.shape)\n",
    "jacs = F.jacobian(vgg5, images)\n",
    "print('jacobian: ', jacs)\n",
    "# print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))[1:]))\n",
    "print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))))\n",
    "\n",
    "# 0到6层每层的jacobians\n",
    "import torch.autograd.functional as F\n",
    "for i in range(7):\n",
    "    vgg5 = VGG('Client', 'VGG5', i, model_cfg)\n",
    "    client_outputs = vgg5(images)\n",
    "    print('outputs.shape:',client_outputs.shape)\n",
    "    jacs = F.jacobian(vgg5, images)\n",
    "    print('jacobian: ', jacs)\n",
    "    # print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))[1:]))\n",
    "    print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL 计算，摸索出来一条路\n",
    "import torch.autograd.functional as F\n",
    "# 参数：\n",
    "sigma = 0.01\n",
    "\n",
    "# 计算jacobian\n",
    "# 取一个batch的数据\n",
    "train_iter=iter(trainloader)\n",
    "inputs,labels = train_iter.next()6\n",
    "print(\"inputs.shape: \",inputs.shape)\n",
    "print(\"labels.shape: \",labels.shape)\n",
    "print(f\"input.requires_grad: {inputs.requires_grad}\")\n",
    "\n",
    "# 加载模型：\n",
    "# vgg5 = VGG('Client', 'VGG5', 1, model_cfg)\n",
    "\n",
    "# 进行前向传播：\n",
    "inputs.requires_grad_(True) # 需要求导\n",
    "outputs = vgg5(inputs)\n",
    "outputs = outputs + sigma * torch.randn_like(outputs) # 加噪声 (0,1] uniform\n",
    "print(\"outputs.shape: \",outputs.shape)\n",
    "\n",
    "# 1. 进行反向传播,计算jacobian\n",
    "# outputs.backward(torch.ones_like(outputs))\n",
    "# J = inputs.grad / sigma # 计算jacobian\n",
    "# print(f\"J1.shape: {J.shape}\")\n",
    "\n",
    "# 2. 重新计算jacobian（用torch.autograd.functional.jacobian函数）\n",
    "J = F.jacobian(vgg5, inputs)\n",
    "# print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "J = J.reshape(J.shape[0],outputs.numel(),inputs.numel())\n",
    "print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "\n",
    "# 计算eta 源论文\n",
    "# J = model.influence_jacobian(train_data)[:, :, :-1] / args.sigma  # 计算FIL（梯度）jacobian\n",
    "# etas = J.pow(2).sum(1).mean(1).sqrt() # 计算dFIL(这时候不是spectral norm了) \n",
    "\n",
    "# 计算eta：drj摸索：\n",
    "I = torch.matmul(J[0].t(), J[0])\n",
    "dFIL = I.trace().div(inputs.numel())\n",
    "eta = dFIL.sqrt()\n",
    "print(f\"eta: {eta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dFIL的两个要求: 可导 + unbiased\n",
    "# x = torch.rand_like(torch.Tensor([1,5]))\n",
    "x = torch.Tensor([0,0])\n",
    "x.requires_grad_(True)\n",
    "print(x.grad)\n",
    "y = torch.nn.ReLU()\n",
    "z = y(x).sum()\n",
    "# z = torch.autograd.functional.jacobian(y, x)\n",
    "z.backward()\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 现成函数调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effective fisher 计算函数\n",
    "import torch.autograd.functional as F\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# nips23\n",
    "from torch.autograd.functional import jvp\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calc_tr(net, x, device, sigmas=0.01, subsample=-1, jvp_parallelism=1): # nips'23 源码\n",
    "    '''\n",
    "    calc_tr 函数利用雅可比向量积（JVP）来估计网络对于输入数据的迹，\n",
    "    这在分析网络的灵敏度或稳定性时非常有用。\n",
    "    此外，通过支持子采样和并行处理，该函数还提供了一种在保持计算效率的同时估计迹的方法。\n",
    "    '''\n",
    "    print(f'x.shape: {x.shape}')\n",
    "    \n",
    "    # 定义一个局部函数 jvp_func**：这个函数接受两个参数 x 和 tgt，并返回 net.forward_first 方法的雅可比向量积（JVP）。\n",
    "    # 这意味着 jvp_func 用于计算网络对于输入 x 在方向 tgt 上的一阶导数\n",
    "    # tgt 计算雅各比向量积的向量\n",
    "    def jvp_func(x, tgt):\n",
    "        # return jvp(net.forward_first, (x,), (tgt,)) #返回 outputs, jacobian product\n",
    "        return jvp(net.forward_first, (x,), (tgt,)) #返回 outputs, jacobian product\n",
    "\n",
    "    # 获取一个batch中第一个数据的维度？d代表的是批次中第一个数据点展平后的特征数量，即输入数据的维度。\n",
    "    d = x[0].flatten().shape[0] # 把一个batch的x展平，获取input dim\n",
    "\n",
    "    # 用于存储每个输入数据点的迹，求迹的和。\n",
    "    tr = torch.zeros(x.shape[0], dtype=x.dtype).to(device)\n",
    "    #print(f'd: {d}, {x.shape}')\n",
    "\n",
    "    # 加速，矩阵降维，但是这个损伤精度，或许改成特征提取更好点？\n",
    "    # Randomly subsample pixels for faster execution\n",
    "    if subsample > 0:\n",
    "        samples = random.sample(range(d), min(d, subsample))\n",
    "    else:\n",
    "        samples = range(d)\n",
    "\n",
    "    #print(x.shape, d, samples)\n",
    "    # jvp parallelism是数据并行的粒度？\n",
    "    # 函数通过分批处理样本来计算迹，每批处理 jvp_parallelism 个样本\n",
    "    for j in range(math.ceil(len(samples) / jvp_parallelism)): # 对于每个数据块\n",
    "        tgts = []\n",
    "\n",
    "        # 遍历每个数据块中的每个维度\n",
    "        '''\n",
    "        在这个函数中，tgt 是用于计算雅可比向量积（JVP）的向量。具体来说，tgt 的作用如下：\n",
    "        构建雅可比向量积的向量：tgt 是一个与输入 x 形状相同的张量，但它的元素大部分为零，只有一个特定位置的元素为 1。这个特定位置对应于我们在计算迹时关注的特征维度。\n",
    "        计算 JVP：在 helper 函数中，tgt 被传递给 jvp_func，用于计算网络对于输入 x 在方向 tgt 上的一阶导数。具体来说，jvp_func 计算的是网络输出相对于输入 x 的雅可比矩阵与 tgt 的乘积。\n",
    "        估计迹：通过在不同的特征维度上重复上述过程，可以估计网络对于输入数据的迹。迹的计算涉及到对所有特征维度的导数进行求和，而 tgt 的作用就是在每次计算时只关注一个特征维度。\n",
    "        简而言之，tgt 是一个用于选择特定特征维度的向量，通过它可以逐个计算每个特征维度的导数，从而最终估计整个输入数据的迹。\n",
    "        '''\n",
    "        for k in samples[j*jvp_parallelism:(j+1)*jvp_parallelism]: # 提取整个batch中每个数据的特定维度\n",
    "            tgt = torch.zeros_like(x).reshape(x.shape[0], -1) # 按照batch 排列？# 雅各比向量积的\n",
    "            # 除了当前样本索引 k 对应的元素设置为 1。这相当于在计算迹时，每次只关注一个特征维度。\n",
    "            tgt[:, k] = 1. # 提取tgt所有的样本的k的特征 计算雅各比向量积的向量，可用于计算trace\n",
    "            tgt = tgt.reshape(x.shape) # 又变回x的形状\n",
    "            tgts.append(tgt)\n",
    "        tgts = torch.stack(tgts)\n",
    "\n",
    "        # 定义一个辅助函数 helper，该函数接受一个目标张量 tgt并返回一个迹的张量和一个值的张量。\n",
    "        # jvp wrapper，遍历每个batchsize\n",
    "        def helper(tgt):\n",
    "            batch_size = x.shape[0]\n",
    "            vals_list = []\n",
    "            grads_list = []\n",
    "            for i in range(batch_size):\n",
    "                val, grad = jvp_func(x[i], tgt[i])  # 对每个批次元素调用jvp_func\n",
    "                vals_list.append(val)\n",
    "                grads_list.append(grad)\n",
    "            # 将结果列表转换为张量, 多个batch的给stack起来\n",
    "            vals = torch.stack(vals_list)\n",
    "            grad = torch.stack(grads_list)\n",
    "\n",
    "\n",
    "            # vals, grad = vmap(jvp_func, randomness='same')(x, tgt)\n",
    "            #print('grad shape: ', grad.shape)\n",
    "            # 因此，矩阵平方的迹和迹的平方通常是不相等的。\n",
    "            # 先求平方再求迹\n",
    "            return torch.sum(grad * grad, dim=tuple(range(1, len(grad.shape)))), vals \n",
    "\n",
    "        # vmap被替换，\n",
    "        # 遍历每个数据块\n",
    "        trs,vals = [],[]\n",
    "        for item in tgts:\n",
    "            trs_, vals_ = helper(item)\n",
    "            trs.append(trs_) # 每个batch对应一个向量\n",
    "            vals.append(vals_)\n",
    "        trs,vals = torch.stack(trs),torch.stack(vals)\n",
    "        # print(trs)\n",
    "        # trs, vals = vmap(helper, randomness='same')(tgts) # randomness for randomness control of dropout\n",
    "        \n",
    "        # vals are stacked results that are repeated by d (should be all the same)\n",
    "\n",
    "\n",
    "        tr += trs.sum(dim=0) # 对每个数据块的迹求和\n",
    "\n",
    "    # Scale if subsampled\n",
    "    if subsample > 0:\n",
    "        tr *= d / len(samples)\n",
    "\n",
    "    # 1/dFIL = d/tr(I)\n",
    "    tr = tr/(d*1.0)\n",
    "    tr = 1.0/tr*sigmas\n",
    "\n",
    "    # print('tr: ',tr.shape, tr)\n",
    "    return tr.cpu().item(), vals[0].squeeze(1)  # squeeze removes one dimension jvp puts\n",
    "\n",
    "\n",
    "# nips'23 fisher trace 计算\n",
    "def calc_tr1(net, x, device, sigmas=0.01, subsample=-1, jvp_parallelism=1): # nips'23 源码\n",
    "    # 并行粒度=1 意思是，每次只处理一个维度\n",
    "    '''\n",
    "    calc_tr 函数利用雅可比向量积（JVP）来估计网络对于输入数据的迹，\n",
    "    这在分析网络的灵敏度或稳定性时非常有用。\n",
    "    此外，通过支持子采样和并行处理，该函数还提供了一种在保持计算效率的同时估计迹的方法。\n",
    "    '''\n",
    "    print(f'x.shape: {x.shape}')\n",
    "    \n",
    "    # 定义一个局部函数 jvp_func**：这个函数接受两个参数 x 和 tgt，并返回 net.forward_first 方法的雅可比向量积（JVP）。\n",
    "    # 这意味着 jvp_func 用于计算网络对于输入 x 在方向 tgt 上的一阶导数\n",
    "    # tgt 计算雅各比向量积的向量\n",
    "    def jvp_func(x, tgt): \n",
    "        # return jvp(net.forward_first, (x,), (tgt,)) #返回 outputs, jacobian product\n",
    "        return jvp(net.forward, (x,), (tgt,)) #返回 outputs, jacobian product\n",
    "\n",
    "    # 获取一个batch中第一个数据的维度？d代表的是批次中第一个数据点展平后的特征数量，即输入数据的维度。\n",
    "    d = x[0].flatten().shape[0] # 把一个batch的x展平，获取input dim\n",
    "\n",
    "    # 用于存储每个输入数据点的迹，求迹的和。\n",
    "    tr = torch.zeros(x.shape[0], dtype=x.dtype).to(device)\n",
    "    print(f'tr.shape: {tr.shape}')\n",
    "\n",
    "    # 加速，矩阵降维，但是这个损伤精度，或许改成特征提取更好点？ # 也是用了矩阵降维度\n",
    "    # Randomly subsample pixels for faster execution\n",
    "    # if subsample > 0: \n",
    "    #     samples = random.sample(range(d), min(d, subsample)) # 选取了部分维度\n",
    "    # else:\n",
    "    #     samples = range(d)\n",
    "    samples = range(d)\n",
    "    # print(x.shape, d, samples)\n",
    "    # jvp parallelism是数据并行的粒度？\n",
    "    # 函数通过分批处理样本来计算迹，每批处理 jvp_parallelism 个样本\n",
    "    # for j in range(math.ceil(len(samples) / jvp_parallelism)): # 对于每个数据块 # 每个数据块包含不同的维度\n",
    "    for j in range(math.ceil(d)): # 对于每个数据块 # 每个数据块包含不同的维度\n",
    "        tgts = []\n",
    "\n",
    "        # 遍历每个数据块中的每个维度\n",
    "        '''\n",
    "        在这个函数中，tgt 是用于计算雅可比向量积（JVP）的向量。具体来说，tgt 的作用如下：\n",
    "        构建雅可比向量积的向量：tgt 是一个与输入 x 形状相同的张量，但它的元素大部分为零，只有一个特定位置的元素为 1。这个特定位置对应于我们在计算迹时关注的特征维度。\n",
    "        计算 JVP：在 helper 函数中，tgt 被传递给 jvp_func，用于计算网络对于输入 x 在方向 tgt 上的一阶导数。具体来说，jvp_func 计算的是网络输出相对于输入 x 的雅可比矩阵与 tgt 的乘积。\n",
    "        估计迹：通过在不同的特征维度上重复上述过程，可以估计网络对于输入数据的迹。迹的计算涉及到对所有特征维度的导数进行求和，而 tgt 的作用就是在每次计算时只关注一个特征维度。\n",
    "        简而言之，tgt 是一个用于选择特定特征维度的向量，通过它可以逐个计算每个特征维度的导数，从而最终估计整个输入数据的迹。\n",
    "        '''\n",
    "        # 对于每一列，构建tgt， 形状和x一样，但是只有一列是1，其他是0\n",
    "        for k in samples[j:(j+1)]: # 提取整个batch中每个数据的特定维度\n",
    "            tgt = torch.zeros_like(x).reshape(x.shape[0], -1) # 按照batch 排列？# 雅各比向量积的\n",
    "            # 除了当前样本索引 k 对应的元素设置为 1。这相当于在计算迹时，每次只关注一个特征维度。\n",
    "            tgt[:, k] = 1. # 提取tgt所有的样本的k的特征 计算雅各比向量积的向量，可用于计算trace，所有行的特定几列有1值\n",
    "            tgt = tgt.reshape(x.shape) # 又变回x的形状\n",
    "            # print(f'tgt.shape: {tgt.shape}')\n",
    "            tgts.append(tgt)\n",
    "        tgts = torch.stack(tgts) # 把多个维度的tgt vstack，一行一行拼接起来\n",
    "        # print(f'tgts.shape: {tgts.shape}')\n",
    "\n",
    "        # 定义一个辅助函数 helper，该函数接受一个目标张量 tgt并返回一个迹的张量和一个值的张量。\n",
    "        # jvp wrapper，遍历每个batchsize\n",
    "        def helper(tgt):\n",
    "            batch_size = x.shape[0]\n",
    "            vals_list = []\n",
    "            grads_list = []\n",
    "            for i in range(batch_size): # 对每个样本\n",
    "                # 如果是一般的forward，要提前unsqueeze一下\n",
    "                val, grad = jvp_func(x[i].unsqueeze(0), tgt[i].unsqueeze(0))  # 对每个批次元素调用jvp_func\n",
    "                # 如果用forward_first，不需要unsqueeze\n",
    "                # val, grad = jvp_func(x[i].unsqueeze(0), tgt[i].unsqueeze(0))  # 对每个批次元素调用jvp_func\n",
    "                vals_list.append(val)\n",
    "                grads_list.append(grad)\n",
    "            # 将结果列表转换为张量, 多个batch的给stack起来\n",
    "            vals = torch.stack(vals_list)\n",
    "            grad = torch.stack(grads_list)\n",
    "\n",
    "            # print('grad.shape: ',grad.shape)\n",
    "            # print('grad: ',grad)\n",
    "\n",
    "            # grad.reshape(sum(list(x.shape)),-1)\n",
    "            # I_np = grad.cpu().detach().numpy()\n",
    "            # df = pd.DataFrame(I_np)\n",
    "            # df.to_csv(f'{time.time()}.csv',index=False,header=False)\n",
    "\n",
    "            # print('grad*grad: ',grad*grad)\n",
    "            # vals, grad = vmap(jvp_func, randomness='same')(x, tgt)\n",
    "            \n",
    "            # print('grad shape: ', grad.shape)\n",
    "            # 因此，矩阵平方的迹和迹的平方通常是不相等的。\n",
    "            # 先求平方再求迹\n",
    "            # range(1, len(grad.shape)) 生成一个从 1 到 len(grad.shape) - 1 的整数序列。\n",
    "            # torch.sum 函数对张量的指定维度进行求和。\n",
    "            # 这里，它对 grad * grad 沿着 tuple(range(1, len(grad.shape))) 指定的维度进行求和。\n",
    "            # ？为什么呢？--- 前面有个unsqueeze？\n",
    "            return torch.sum(grad * grad, dim=tuple(range(1, len(grad.shape)))), vals \n",
    "\n",
    "        # vmap被替换\n",
    "        # 遍历每个数据块\n",
    "        trs,vals = [],[]\n",
    "        for item in tgts: # 对于每一列？\n",
    "            trs_, vals_ = helper(item)\n",
    "            trs.append(trs_) # 每个batch对应一个向量\n",
    "            vals.append(vals_)\n",
    "            # print('trs_: ',trs_.shape)\n",
    "        trs,vals = torch.stack(trs),torch.stack(vals)\n",
    "        # print('trs: ',trs.shape, trs)\n",
    "        # trs, vals = vmap(helper, randomness='same')(tgts) # randomness for randomness control of dropout\n",
    "        # vals are stacked results that are repeated by d (should be all the same)\n",
    "\n",
    "        tr += trs.sum(dim=0) # 对每个数据块的迹求和\n",
    "\n",
    "    # Scale if subsampled\n",
    "    if subsample > 0: # 除以维度\n",
    "        tr *= d / len(samples)\n",
    "\n",
    "    # 1/dFIL = d/tr(I)\n",
    "    tr = tr/(d*1.0)\n",
    "    tr = 1.0/tr*sigmas\n",
    "\n",
    "    print('tr: ',tr.shape, tr)\n",
    "    return tr.cpu().item(), vals[0].squeeze(1)  # squeeze removes one dimension jvp puts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00244185]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 我实现的：\n",
    "# dFIL inverse指标计算\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric = dFILInverseMetric()\n",
    "# 对traingloader遍历计算所有 inverse dFIL\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)):\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    \n",
    "    # inference\n",
    "    outputs = client_net(inputs)\n",
    "\n",
    "    eta = metric.quantify(model=client_net, inputs=inputs, outputs=outputs, with_outputs=True)\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'inv_dFIL{split_layer}.csv',index=False)\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 3, 32, 32])\n",
      "tr.shape: torch.Size([1])\n",
      "tr:  torch.Size([1]) tensor([0.0024])\n",
      "eta(forward):  0.002441848861053586\n",
      "x.shape: torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta(forward_first):  0.002441848861053586\n",
      "matrix:  [[0.00244185]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# maeng fisher\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric_trace = dFILInverseMetric()\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    \n",
    "    # inference\n",
    "    # outputs = client_net(inputs)\n",
    "\n",
    "    # inputs = inputs.unsqueeze(0)\n",
    "    # eta,val = metric_trace.calc_tr(net=client_net, x=inputs, device=args['device'])\n",
    "    \n",
    "    # calc_tr forward\n",
    "    eta,val = calc_tr1(net=client_net, x=inputs, device=args['device'])\n",
    "    print('eta(forward): ',eta)\n",
    "\n",
    "    # calc_tr forward_first\n",
    "    client_net.set_bn_training(False)\n",
    "    eta,val = calc_tr(net=client_net, x=inputs, device=args['device'])\n",
    "    client_net.set_bn_training(True)\n",
    "    print('eta(forward_first): ',eta)\n",
    "\n",
    "    \n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大\n",
    "print(\"matrix: \",matrix)\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'inv_dFIL_maeng-{split_layer}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drj-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
