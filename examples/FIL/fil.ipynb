{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基础设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np1\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '48'\n",
    "\n",
    "# 导入各个指标\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "from ppsplit.quantification.distance_correlation.distCor import distCorMetric\n",
    "from ppsplit.quantification.fisher_information.dFIL_inverse import dFILInverseMetric\n",
    "from ppsplit.quantification.shannon_information.mutual_information import MuInfoMetric\n",
    "from ppsplit.quantification.shannon_information.ULoss import ULossMetric\n",
    "from ppsplit.quantification.rep_reading.rep_reader import PCA_Reader\n",
    "from ppsplit.quantification.shannon_information.ITE_tools import Shannon_quantity\n",
    "\n",
    "# 导入各个baseline模型及其数据集预处理方法\n",
    "# 模型\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "from target_model.models.VGG import VGG,VGG5Decoder,model_cfg\n",
    "from target_model.models.BankNet import BankNet1,bank_cfg\n",
    "from target_model.models.CreditNet import CreditNet1,credit_cfg\n",
    "from target_model.models.PurchaseNet import PurchaseClassifier1,purchase_cfg\n",
    "\n",
    "# 数据预处理方法\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize,deprocess\n",
    "from target_model.data_preprocessing.preprocess_bank import bank_dataset,preprocess_bank,preprocess_bank_dataset\n",
    "from target_model.data_preprocessing.preprocess_credit import preprocess_credit\n",
    "from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase\n",
    "from target_model.data_preprocessing.dataset import get_one_data\n",
    "\n",
    "# utils\n",
    "from ppsplit.utils.utils import create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "        'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        # 'device':torch.device(\"cpu\"),\n",
    "        'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        'result_dir': '20240702-FIL/',\n",
    "        'batch_size':2,\n",
    "        'noise_scale':0, # 防护措施\n",
    "        }\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.1.weight\n",
      "features.1.bias\n",
      "features.1.running_mean\n",
      "features.1.running_var\n",
      "features.1.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "# 加载模型和数据集，并从unit模型中切割出client_model\n",
    "if args['dataset']=='CIFAR10':\n",
    "    # 超参数\n",
    "    testset_len = 10000 # 10000个数据一次 整个测试集合的长度\n",
    "    # split_layer_list = list(range(len(model_cfg['VGG5'])))\n",
    "    split_layer = 1 # 定成3吧？\n",
    "    test_num = 2 # 试验序号\n",
    "\n",
    "    # 关键路径\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/VGG5/BN+Tanh/VGG5-params-20ep.pth' # VGG5-BN+Tanh # 存储的是模型参数，不包括模型结构\n",
    "    results_dir  = f\"../../results/{args['result_dir']}/VGG5/{test_num}/\"\n",
    "    decoder_route = f\"../../results/{args['result_dir']}/VGG5/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    trainloader,testloader = get_cifar10_normalize(batch_size = 1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # 切割成client model\n",
    "    # vgg5_unit.load_state_dict(torch.load(unit_net_route,map_location=torch.device('cpu'))) # 完整的模型\n",
    "    client_net = VGG('Client','VGG5',split_layer,model_cfg,noise_scale=args['noise_scale'])\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(model_cfg['VGG5']):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "elif args['dataset']=='credit':\n",
    "    # 超参数\n",
    "    test_num = 1 # 试验序号\n",
    "    testset_len = 61503 # for the mutual information\n",
    "    split_layer_list = [0,3,6,9]\n",
    "    split_layer = 3\n",
    "    # split_layer_list = ['linear1', 'linear2']\n",
    "\n",
    "    # 关键路径\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/credit/credit-20ep_params.pth'\n",
    "    results_dir  = f\"../results/{args['result_dir']}/Credit/{test_num}/\"\n",
    "    decoder_route = f\"../results/{args['result_dir']}/Credit/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "\n",
    "    # 数据集加载\n",
    "    trainloader,testloader = preprocess_credit(batch_size=1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # client模型切割加载\n",
    "    client_net = CreditNet1(layer=split_layer,noise_scale=args['noise_scale'])\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(credit_cfg):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "elif args['dataset']=='bank':\n",
    "    # 超参数\n",
    "    test_num = 1 # 试验序号\n",
    "    testset_len=8238\n",
    "    # split_layer_list = ['linear1', 'linear2']\n",
    "    split_layer_list = [0,2,4,6]\n",
    "    split_layer = 2\n",
    "\n",
    "    # 关键路径\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Bank/bank-20ep_params.pth'\n",
    "    results_dir  = f\"../results/{args['result_dir']}/Bank/{test_num}/\"\n",
    "    decoder_route = f\"../results/{args['result_dir']}/Bank/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    " \n",
    "    # 数据集加载\n",
    "    trainloader,testloader = preprocess_bank(batch_size=testset_len)\n",
    "    # one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据 \n",
    "\n",
    "    # 模型加载\n",
    "    client_net = BankNet1(layer=split_layer,noise_scale=args['noise_scale'])\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(bank_cfg):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)    \n",
    "elif args['dataset']=='purchase':\n",
    "    # 超参数\n",
    "    test_num = 1 # 试验序号\n",
    "    testset_len = 39465 # test len\n",
    "    # split_layer_list = [0,1,2,3,4,5,6,7,8]\n",
    "    split_layer = 3\n",
    "\n",
    "    # 关键路径\n",
    "    unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/Purchase100/Purchase_bestmodel_param.pth'\n",
    "    results_dir = f\"../../results/{args['result_dir']}/Purchase/{test_num}/\"\n",
    "    decoder_route = f\"../../results/{args['result_dir']}/Purchase/{test_num}/Decoder-layer{split_layer}.pth\"\n",
    "    \n",
    "    # 数据集加载\n",
    "    trainloader,testloader = preprocess_purchase(batch_size=1)\n",
    "    one_data_loader = get_one_data(testloader,batch_size = args['batch_size']) #拿到第一个测试数据\n",
    "\n",
    "    # 模型加载\n",
    "    client_net = PurchaseClassifier1(layer=split_layer,noise_scale=args['noise_scale'])\n",
    "    # pweights = torch.load(unit_net_route,map_location=torch.device('cpu'))\n",
    "    pweights = torch.load(unit_net_route)\n",
    "    if split_layer < len(purchase_cfg):\n",
    "        pweights = split_weights_client(pweights,client_net.state_dict())\n",
    "    client_net.load_state_dict(pweights)\n",
    "\n",
    "else:\n",
    "    exit(-1)\n",
    "\n",
    "client_net.to(args['device'])\n",
    "create_dir(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. pytorch的自动梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "True\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "import torch\n",
    "\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "# 信息打印\n",
    "print(x.shape)\n",
    "print(x.requires_grad)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向传播\n",
    "loss.backward()\n",
    "print(x.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad/backward 张量求梯度 sum\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.randn(5, 3)\n",
    "b = torch.randn(3)\n",
    "z = torch.matmul(x, w)+b\n",
    "\n",
    "# 计算sumed雅可比矩阵\n",
    "z.backward(torch.ones_like(z))\n",
    "print(x.grad.shape)\n",
    "print(x.grad)\n",
    "\n",
    "# 用grad\n",
    "from torch.autograd import grad\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "w = torch.eye(5, 3)\n",
    "b = torch.randn(3)\n",
    "z = torch.matmul(x, w)+b\n",
    "xgrad = grad(z, x, grad_outputs=torch.ones_like(z))[0]\n",
    "print(xgrad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jacobian\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.autograd import grad\n",
    "x = torch.ones(5, requires_grad=True)  # input tensor\n",
    "\n",
    "def forward(x):\n",
    "    y = torch.zeros(3, requires_grad=True)  # expected output\n",
    "    w = torch.eye(5, 3)\n",
    "    b = torch.randn(3)\n",
    "    z = torch.matmul(x, w)+b\n",
    "    return z\n",
    "\n",
    "\n",
    "# 计算sumed雅可比矩阵\n",
    "jac = jacobian(forward, x)\n",
    "print(jac.shape)\n",
    "print(jac)\n",
    "print(jac.sum(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 试探FIL计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL计算：\n",
    "import torch.autograd.functional as F\n",
    "# 切割模型通讯量查看\n",
    "# for i in range(7):\n",
    "vgg5 = VGG('Client', 'VGG5', 1, model_cfg)\n",
    "\n",
    "client_outputs = vgg5(images)\n",
    "print('outputs.shape:',client_outputs.shape)\n",
    "jacs = F.jacobian(vgg5, images)\n",
    "print('jacobian: ', jacs)\n",
    "# print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))[1:]))\n",
    "print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))))\n",
    "\n",
    "# 0到6层每层的jacobians\n",
    "import torch.autograd.functional as F\n",
    "for i in range(7):\n",
    "    vgg5 = VGG('Client', 'VGG5', i, model_cfg)\n",
    "    client_outputs = vgg5(images)\n",
    "    print('outputs.shape:',client_outputs.shape)\n",
    "    jacs = F.jacobian(vgg5, images)\n",
    "    print('jacobian: ', jacs)\n",
    "    # print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))[1:]))\n",
    "    print('output size:',torch.prod(torch.tensor(list(client_outputs.shape))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL 计算，摸索出来一条路\n",
    "import torch.autograd.functional as F\n",
    "# 参数：\n",
    "sigma = 0.01\n",
    "\n",
    "# 计算jacobian\n",
    "# 取一个batch的数据\n",
    "train_iter=iter(trainloader)\n",
    "inputs,labels = train_iter.next()6\n",
    "print(\"inputs.shape: \",inputs.shape)\n",
    "print(\"labels.shape: \",labels.shape)\n",
    "print(f\"input.requires_grad: {inputs.requires_grad}\")\n",
    "\n",
    "# 加载模型：\n",
    "# vgg5 = VGG('Client', 'VGG5', 1, model_cfg)\n",
    "\n",
    "# 进行前向传播：\n",
    "inputs.requires_grad_(True) # 需要求导\n",
    "outputs = vgg5(inputs)\n",
    "outputs = outputs + sigma * torch.randn_like(outputs) # 加噪声 (0,1] uniform\n",
    "print(\"outputs.shape: \",outputs.shape)\n",
    "\n",
    "# 1. 进行反向传播,计算jacobian\n",
    "# outputs.backward(torch.ones_like(outputs))\n",
    "# J = inputs.grad / sigma # 计算jacobian\n",
    "# print(f\"J1.shape: {J.shape}\")\n",
    "\n",
    "# 2. 重新计算jacobian（用torch.autograd.functional.jacobian函数）\n",
    "J = F.jacobian(vgg5, inputs)\n",
    "# print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "J = J.reshape(J.shape[0],outputs.numel(),inputs.numel())\n",
    "print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "\n",
    "# 计算eta 源论文\n",
    "# J = model.influence_jacobian(train_data)[:, :, :-1] / args.sigma  # 计算FIL（梯度）jacobian\n",
    "# etas = J.pow(2).sum(1).mean(1).sqrt() # 计算dFIL(这时候不是spectral norm了) \n",
    "\n",
    "# 计算eta：drj摸索：\n",
    "I = torch.matmul(J[0].t(), J[0])\n",
    "dFIL = I.trace().div(inputs.numel())\n",
    "eta = dFIL.sqrt()\n",
    "print(f\"eta: {eta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dFIL的两个要求: 可导 + unbiased\n",
    "# x = torch.rand_like(torch.Tensor([1,5]))\n",
    "x = torch.Tensor([0,0])\n",
    "x.requires_grad_(True)\n",
    "print(x.grad)\n",
    "y = torch.nn.ReLU()\n",
    "z = y(x).sum()\n",
    "# z = torch.autograd.functional.jacobian(y, x)\n",
    "z.backward()\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 现成函数调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# 我实现的：\n",
    "# dFIL inverse指标计算\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric = dFILInverseMetric()\n",
    "# 对traingloader遍历计算所有 inverse dFIL\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)):\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    \n",
    "    # inference\n",
    "    outputs = client_net(inputs)\n",
    "\n",
    "    eta = metric.quantify(model=client_net, inputs=inputs, outputs=outputs, with_outputs=True)\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'inv_dFIL.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL 计算函数\n",
    "import torch.autograd.functional as F\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# nips23\n",
    "from torch.autograd.functional import jvp\n",
    "import random\n",
    "import math\n",
    "\n",
    "def calc_tr(net, x, device, subsample=-1, jvp_parallelism=1): # nips'23 源码\n",
    "    '''\n",
    "    calc_tr 函数利用雅可比向量积（JVP）来估计网络对于输入数据的迹，\n",
    "    这在分析网络的灵敏度或稳定性时非常有用。\n",
    "    此外，通过支持子采样和并行处理，该函数还提供了一种在保持计算效率的同时估计迹的方法。\n",
    "    '''\n",
    "    print(f'x.shape: {x.shape}')\n",
    "    \n",
    "    # 定义一个局部函数 jvp_func**：这个函数接受两个参数 x 和 tgt，并返回 net.forward_first 方法的雅可比向量积（JVP）。\n",
    "    # 这意味着 jvp_func 用于计算网络对于输入 x 在方向 tgt 上的一阶导数\n",
    "    # tgt 计算雅各比向量积的向量\n",
    "    def jvp_func(x, tgt):\n",
    "        # return jvp(net.forward_first, (x,), (tgt,)) #返回 outputs, jacobian product\n",
    "        return jvp(net.forward, (x,), (tgt,)) #返回 outputs, jacobian product\n",
    "\n",
    "    # 获取一个batch中第一个数据的维度？d代表的是批次中第一个数据点展平后的特征数量，即输入数据的维度。\n",
    "    d = x[0].flatten().shape[0] # 把一个batch的x展平，获取input dim\n",
    "\n",
    "    # 用于存储每个输入数据点的迹\n",
    "    tr = torch.zeros(x.shape[0], dtype=x.dtype).to(device)\n",
    "    #print(f'd: {d}, {x.shape}')\n",
    "\n",
    "    # 加速，矩阵降维，但是这个损伤精度，或许改成特征提取更好点？\n",
    "    # Randomly subsample pixels for faster execution\n",
    "    if subsample > 0:\n",
    "        samples = random.sample(range(d), min(d, subsample))\n",
    "    else:\n",
    "        samples = range(d)\n",
    "\n",
    "    #print(x.shape, d, samples)\n",
    "    # jvp parallelism是数据并行的粒度？\n",
    "    # 函数通过分批处理样本来计算迹，每批处理 jvp_parallelism 个样本\n",
    "    for j in range(math.ceil(len(samples) / jvp_parallelism)): # 对于每个数据块\n",
    "        tgts = []\n",
    "        # 遍历每个数据块中的每个维度\n",
    "        for k in samples[j*jvp_parallelism:(j+1)*jvp_parallelism]: # 提取整个batch中每个数据的特定维度\n",
    "            tgt = torch.zeros_like(x).reshape(x.shape[0], -1) # 按照batch 排列？# 雅各比向量积的\n",
    "            # 除了当前样本索引 k 对应的元素设置为 1。这相当于在计算迹时，每次只关注一个特征维度。\n",
    "            tgt[:, k] = 1. # 提取tgt所有的样本的k的特征 计算雅各比向量积的向量，可用于计算trace\n",
    "            tgt = tgt.reshape(x.shape) # 又变回x的形状\n",
    "            tgts.append(tgt)\n",
    "        tgts = torch.stack(tgts)\n",
    "\n",
    "        def helper(tgt):\n",
    "            batch_size = x.shape[0]\n",
    "            vals_list = []\n",
    "            grads_list = []\n",
    "            for i in range(batch_size):\n",
    "                val, grad = jvp_func(x[i], tgt[i])  # 对每个批次元素调用jvp_func\n",
    "                vals_list.append(val)\n",
    "                grads_list.append(grad)\n",
    "            # 将结果列表转换为张量\n",
    "            vals = torch.stack(vals_list)\n",
    "            grad = torch.stack(grads_list)\n",
    "\n",
    "\n",
    "            # vals, grad = vmap(jvp_func, randomness='same')(x, tgt)\n",
    "            #print('grad shape: ', grad.shape)\n",
    "            return torch.sum(grad * grad, dim=tuple(range(1, len(grad.shape)))), vals # 先求迹再求平方\n",
    "\n",
    "        # vmap被替换\n",
    "        trs,vals = [],[]\n",
    "        for item in tgts:\n",
    "            trs_, vals_ = helper(item)\n",
    "            trs.append(trs_)\n",
    "            vals.append(vals_)\n",
    "        trs,vals = torch.stack(trs),torch.stack(vals)\n",
    "\n",
    "        # trs, vals = vmap(helper, randomness='same')(tgts) # randomness for randomness control of dropout\n",
    "        \n",
    "        # vals are stacked results that are repeated by d (should be all the same)\n",
    "\n",
    "\n",
    "        tr += trs.sum(dim=0)\n",
    "\n",
    "    # Scale if subsampled\n",
    "    if subsample > 0:\n",
    "        tr *= d / len(samples)\n",
    "\n",
    "    tr = tr/(d*1.0)\n",
    "    tr = 1.0/tr\n",
    "\n",
    "    print('tr: ',tr.shape, tr)\n",
    "    return tr.cpu().item(), vals[0].squeeze(1)  # squeeze removes one dimension jvp puts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 2, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:39<00:00, 39.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix:  [[0.00075323]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# maeng fisher\n",
    "\n",
    "eta_same_layer_list = []\n",
    "eta_diff_layer_list=[]\n",
    "\n",
    "metric_trace = dFILInverseMetric()\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    # if j < 31705:\n",
    "        # continue\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(args['device']), labels.to(args['device'])\n",
    "    \n",
    "    # inference\n",
    "    # outputs = client_net(inputs)\n",
    "\n",
    "    inputs = inputs.unsqueeze(0)\n",
    "    eta,val = metric_trace.calc_tr(net=client_net, x=inputs, device=args['device'])\n",
    "    # 打印\n",
    "    # print(str(j)+\": \"+str(eta.item()))\n",
    "    eta_same_layer_list.append(eta)\n",
    "eta_diff_layer_list.append(eta_same_layer_list)\n",
    "\n",
    "# 结果储存到csv中\n",
    "matrix = np.array(eta_diff_layer_list) # 有点大\n",
    "print(\"matrix: \",matrix)\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'inv_dFIL_maeng.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. effective information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effective fisher 计算函数\n",
    "import torch.autograd.functional as F\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# nips23\n",
    "from torch.autograd.functional import jvp\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def computing_det_with_outputs(model, inputs, outputs, sigmas): # sigma_square\n",
    "        # batchsize:\n",
    "        batch_size = inputs.shape[0] # 一个batch的样本数目\n",
    "        output_size = outputs[0].numel() # 一个样本的outputs长度\n",
    "        input_size = inputs[0].numel() # 一个样本的outputs长度\n",
    "        effect_fisher_sum = 0.0\n",
    "\n",
    "        # 遍历单个样本: 换数据\n",
    "        for i in range(batch_size):\n",
    "            input_i = inputs[i].unsqueeze(0)\n",
    "\n",
    "            # 计算jacobian\n",
    "            J = F.jacobian(model, input_i)\n",
    "            # J = J.reshape(J.shape[0],outputs.numel(),inputs.numel()) # (batch, out_size, in_size)\n",
    "            J = J.reshape(output_size, input_size) # (batch, out_size, in_size)\n",
    "            # print(f\"J2.shape: {J.shape}, J2.prod: {torch.prod(torch.tensor(list(J.shape)))}\")\n",
    "            # 计算eta\n",
    "            JtJ = torch.matmul(J.t(), J)\n",
    "            I = 1.0/(sigmas)*JtJ\n",
    "            I_np = I.cpu().detach().numpy()\n",
    "            df = pd.DataFrame(I_np)\n",
    "            df.to_csv(f'{i}.csv',index=False,header=False)\n",
    "\n",
    "\n",
    "            print(\"I: \", I)\n",
    "            # w = torch.det(I)\n",
    "            print('det I: ', I.det().log() )\n",
    "            k = torch.logdet(I)\n",
    "            print('log det I: ',k )\n",
    "            effect_fisher = 0.5 * (input_size * torch.log(2*torch.pi*torch.exp(torch.tensor(1.0))) - k)\n",
    "            effect_fisher_sum+=effect_fisher\n",
    "\n",
    "            print(\"effect_fisher: \",effect_fisher)\n",
    "\n",
    "        # print(\"Jt*J: \", JtJ)\n",
    "        # print(\"Jt*J: \", JtJ.shape, JtJ)\n",
    "        # print(\"I.shape: \", I.shape)\n",
    "        # eta = dFIL\n",
    "        # print(f\"eta: {eta}\")\n",
    "        # print('t2-t1=',t2-t1, 't3-t2', t3-t2)\n",
    "        effect_fisher_mean = effect_fisher_sum / batch_size\n",
    "        return effect_fisher_mean.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:  tensor([[ 4.7762e+02, -1.7653e+01, -3.7860e+01,  ..., -6.3836e-02,\n",
      "         -2.6662e-01,  5.2777e-02],\n",
      "        [-1.7653e+01,  6.7384e+02,  2.4491e+02,  ..., -4.7750e-01,\n",
      "         -7.4096e-01, -4.3659e-01],\n",
      "        [-3.7860e+01,  2.4491e+02,  6.6185e+02,  ..., -3.0760e-02,\n",
      "         -1.7476e-01, -2.6107e-01],\n",
      "        ...,\n",
      "        [-6.3836e-02, -4.7750e-01, -3.0760e-02,  ...,  7.6587e+02,\n",
      "          1.3968e+02, -4.9137e+01],\n",
      "        [-2.6662e-01, -7.4096e-01, -1.7476e-01,  ...,  1.3968e+02,\n",
      "          9.6689e+02,  3.5150e+01],\n",
      "        [ 5.2777e-02, -4.3659e-01, -2.6107e-01,  ..., -4.9137e+01,\n",
      "          3.5150e+01,  7.0952e+02]], device='cuda:1')\n",
      "det I:  tensor(nan, device='cuda:1')\n",
      "log det I:  tensor(nan, device='cuda:1')\n",
      "effect_fisher:  tensor(nan, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:32<00:00, 32.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:  tensor([[ 3.9688e+02, -1.0111e+02, -1.0212e+02,  ..., -2.9194e-01,\n",
      "         -8.9430e-01, -4.9021e-01],\n",
      "        [-1.0111e+02,  1.1254e+03, -9.1680e+01,  ..., -1.1800e+00,\n",
      "         -2.1911e+00, -1.5687e+00],\n",
      "        [-1.0212e+02, -9.1680e+01,  9.4364e+02,  ..., -7.5056e-01,\n",
      "         -1.7241e+00, -6.7552e-01],\n",
      "        ...,\n",
      "        [-2.9194e-01, -1.1800e+00, -7.5056e-01,  ...,  1.6392e+03,\n",
      "          1.3568e+01,  2.4505e+01],\n",
      "        [-8.9430e-01, -2.1911e+00, -1.7241e+00,  ...,  1.3568e+01,\n",
      "          1.2277e+03,  3.7108e+02],\n",
      "        [-4.9021e-01, -1.5687e+00, -6.7552e-01,  ...,  2.4505e+01,\n",
      "          3.7108e+02,  8.9802e+02]], device='cuda:1')\n",
      "det I:  tensor(nan, device='cuda:1')\n",
      "log det I:  tensor(nan, device='cuda:1')\n",
      "effect_fisher:  tensor(nan, device='cuda:1')\n",
      "Layer 1 effecInfo: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# effect fisher 指标计算\n",
    "\n",
    "effecInfo_same_layer_list = []\n",
    "Fishermetric = dFILInverseMetric()\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        # fisher\n",
    "        outputs = client_net(images)\n",
    "        # images = images.unsqueeze(0)\n",
    "        # effectFisher = Fishermetric._computing_det_with_outputs(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01)\n",
    "        effectFisher = computing_det_with_outputs(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01)\n",
    "\n",
    "        effecInfo_same_layer_list.append(effectFisher)\n",
    "print(f\"Layer {split_layer} effecInfo: {sum(effecInfo_same_layer_list)/len(effecInfo_same_layer_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect entropy 计算函数\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "def shannon_entropy(time_series):\n",
    "    \"\"\"Calculate Shannon Entropy of the sample data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_series: np.ndarray | list[str]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ent: float\n",
    "        The Shannon Entropy as float value\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate frequency counts\n",
    "    _, counts = np.unique(time_series, return_counts=True)\n",
    "    total_count = len(time_series)\n",
    "    # print(\"total_count: \",total_count)\n",
    "\n",
    "    # Calculate frequencies and Shannon entropy\n",
    "    frequencies = counts / total_count\n",
    "    # print(\"freq: \",frequencies)\n",
    "    ent = -np.sum(frequencies * np.log(frequencies))\n",
    "\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.2392,  0.2471,  0.2941,  ...,  0.0745, -0.0118, -0.0902],\n",
      "          [ 0.1922,  0.1843,  0.2471,  ...,  0.0667, -0.0196, -0.0667],\n",
      "          [ 0.1843,  0.1843,  0.2392,  ...,  0.0902,  0.0196, -0.0588],\n",
      "          ...,\n",
      "          [-0.4667, -0.6706, -0.7569,  ..., -0.7020, -0.8980, -0.6863],\n",
      "          [-0.5216, -0.6157, -0.7255,  ..., -0.7961, -0.7725, -0.8431],\n",
      "          [-0.5765, -0.5608, -0.6471,  ..., -0.8118, -0.7333, -0.8353]],\n",
      "\n",
      "         [[-0.1216, -0.1294, -0.0902,  ..., -0.2549, -0.2863, -0.3333],\n",
      "          [-0.1216, -0.1373, -0.1059,  ..., -0.2549, -0.2863, -0.3098],\n",
      "          [-0.1373, -0.1451, -0.1294,  ..., -0.2314, -0.2549, -0.3020],\n",
      "          ...,\n",
      "          [-0.0275, -0.2157, -0.3098,  ..., -0.2392, -0.4980, -0.3333],\n",
      "          [-0.0902, -0.2000, -0.3333,  ..., -0.3569, -0.3569, -0.4980],\n",
      "          [-0.1608, -0.1765, -0.3020,  ..., -0.3961, -0.3412, -0.4745]],\n",
      "\n",
      "         [[-0.6157, -0.6314, -0.6000,  ..., -0.7176, -0.7176, -0.7412],\n",
      "          [-0.6000, -0.6863, -0.6471,  ..., -0.7569, -0.7490, -0.7333],\n",
      "          [-0.6314, -0.7412, -0.7176,  ..., -0.7333, -0.7333, -0.7412],\n",
      "          ...,\n",
      "          [ 0.3882,  0.1608,  0.0745,  ...,  0.1451, -0.1529, -0.0039],\n",
      "          [ 0.3176,  0.1608,  0.0353,  ...,  0.0196, -0.0118, -0.1608],\n",
      "          [ 0.2549,  0.1686,  0.0353,  ..., -0.0275,  0.0118, -0.1373]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8431,  0.8118,  0.8196,  ...,  0.8275,  0.8275,  0.8196],\n",
      "          [ 0.8667,  0.8431,  0.8431,  ...,  0.8510,  0.8510,  0.8431],\n",
      "          [ 0.8588,  0.8353,  0.8353,  ...,  0.8431,  0.8431,  0.8353],\n",
      "          ...,\n",
      "          [-0.3176, -0.6627, -0.8510,  ...,  0.3255,  0.4275,  0.4745],\n",
      "          [-0.3569, -0.6392, -0.7176,  ...,  0.3647,  0.4510,  0.4667],\n",
      "          [-0.3333, -0.5137, -0.5451,  ...,  0.3176,  0.4118,  0.4588]],\n",
      "\n",
      "         [[ 0.8431,  0.8118,  0.8196,  ...,  0.8275,  0.8275,  0.8196],\n",
      "          [ 0.8667,  0.8431,  0.8431,  ...,  0.8510,  0.8510,  0.8431],\n",
      "          [ 0.8588,  0.8353,  0.8353,  ...,  0.8431,  0.8431,  0.8353],\n",
      "          ...,\n",
      "          [-0.2235, -0.6000, -0.8196,  ...,  0.4431,  0.5451,  0.5843],\n",
      "          [-0.2471, -0.5529, -0.6549,  ...,  0.4824,  0.5686,  0.5843],\n",
      "          [-0.2078, -0.4118, -0.4745,  ...,  0.4353,  0.5294,  0.5686]],\n",
      "\n",
      "         [[ 0.8431,  0.8118,  0.8196,  ...,  0.8275,  0.8275,  0.8196],\n",
      "          [ 0.8667,  0.8431,  0.8431,  ...,  0.8510,  0.8510,  0.8431],\n",
      "          [ 0.8588,  0.8353,  0.8353,  ...,  0.8431,  0.8431,  0.8353],\n",
      "          ...,\n",
      "          [-0.3020, -0.7098, -0.9137,  ...,  0.4039,  0.5137,  0.5765],\n",
      "          [-0.3569, -0.7176, -0.8275,  ...,  0.4353,  0.5373,  0.5686],\n",
      "          [-0.3490, -0.6235, -0.7020,  ...,  0.3961,  0.4980,  0.5608]]]],\n",
      "       device='cuda:1')\n",
      "effectEntro_pyent:  15.70461197683164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyentrp import entropy as ent\n",
    "\n",
    "# effective entorpy\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): \n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    print(images)\n",
    "    with torch.no_grad():\n",
    "        # effectEntro = Shannon_quantity(images)\n",
    "        # print(\"effectEntro_ite: \",effectEntro)\n",
    "\n",
    "        effectEntro_pyent = 0.0\n",
    "        for i in range(len(images[0])): # 对每个维度\n",
    "            effectEntro_pyent  += shannon_entropy(images[:,i].flatten().detach().cpu().numpy())\n",
    "            # print('effectEntro_pyent: ',effectEntro_pyent)\n",
    "        # effectEntro_pyent = shannon_entropy(images.flatten(start_dim=1).detach().cpu().numpy())\n",
    "        print('effectEntro_pyent: ',effectEntro_pyent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170.22239461082756\n",
      "335.1999980582977\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(10,341) # 最大能接受341个特征 \n",
    "z = torch.randn(100,200)\n",
    "e1 = Shannon_quantity(y)\n",
    "e2 = Shannon_quantity(z)\n",
    "print(e1)\n",
    "print(e2)\n",
    "# print(np.log(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============processing data===============\n",
      "X_train.shape: (32950, 63)\n",
      "X_test.shape: (8238, 63)\n",
      "y_train.shape: (32950, 1)\n",
      "y_test.shape: (8238, 1) <class 'numpy.ndarray'>\n",
      "===============processing data end===============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "      <td>8238.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.289148</td>\n",
       "      <td>0.157198</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.063608</td>\n",
       "      <td>0.088614</td>\n",
       "      <td>0.032168</td>\n",
       "      <td>0.078174</td>\n",
       "      <td>0.064093</td>\n",
       "      <td>0.141539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284123</td>\n",
       "      <td>0.054413</td>\n",
       "      <td>0.020963</td>\n",
       "      <td>0.838213</td>\n",
       "      <td>0.081903</td>\n",
       "      <td>0.251712</td>\n",
       "      <td>0.335361</td>\n",
       "      <td>0.453437</td>\n",
       "      <td>0.089942</td>\n",
       "      <td>0.344142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.453394</td>\n",
       "      <td>0.364010</td>\n",
       "      <td>0.152031</td>\n",
       "      <td>0.146202</td>\n",
       "      <td>0.244068</td>\n",
       "      <td>0.284203</td>\n",
       "      <td>0.176457</td>\n",
       "      <td>0.268462</td>\n",
       "      <td>0.244934</td>\n",
       "      <td>0.348598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171524</td>\n",
       "      <td>0.051057</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.366935</td>\n",
       "      <td>0.121449</td>\n",
       "      <td>0.146193</td>\n",
       "      <td>0.252907</td>\n",
       "      <td>0.286649</td>\n",
       "      <td>0.055046</td>\n",
       "      <td>0.179637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160494</td>\n",
       "      <td>0.022977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.199532</td>\n",
       "      <td>0.192469</td>\n",
       "      <td>0.036953</td>\n",
       "      <td>0.203781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234568</td>\n",
       "      <td>0.040057</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.269680</td>\n",
       "      <td>0.418410</td>\n",
       "      <td>0.093403</td>\n",
       "      <td>0.425709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382716</td>\n",
       "      <td>0.068524</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.296960</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.143278</td>\n",
       "      <td>0.512287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769622</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150759</td>\n",
       "      <td>0.512287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  8238.000000  8238.000000  8238.000000  8238.000000  8238.000000   \n",
       "mean      0.289148     0.157198     0.023671     0.021850     0.063608   \n",
       "std       0.453394     0.364010     0.152031     0.146202     0.244068   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  8238.000000  8238.000000  8238.000000  8238.000000  8238.000000  ...   \n",
       "mean      0.088614     0.032168     0.078174     0.064093     0.141539  ...   \n",
       "std       0.284203     0.176457     0.268462     0.244934     0.348598  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "                53           54           55           56           57  \\\n",
       "count  8238.000000  8238.000000  8238.000000  8238.000000  8238.000000   \n",
       "mean      0.284123     0.054413     0.020963     0.838213     0.081903   \n",
       "std       0.171524     0.051057     0.030769     0.366935     0.121449   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.160494     0.022977     0.000000     1.000000     0.000000   \n",
       "50%       0.234568     0.040057     0.018182     1.000000     0.000000   \n",
       "75%       0.382716     0.068524     0.036364     1.000000     0.142857   \n",
       "max       1.000000     0.769622     0.272727     1.000000     1.000000   \n",
       "\n",
       "                58           59           60           61           62  \n",
       "count  8238.000000  8238.000000  8238.000000  8238.000000  8238.000000  \n",
       "mean      0.251712     0.335361     0.453437     0.089942     0.344142  \n",
       "std       0.146193     0.252907     0.286649     0.055046     0.179637  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.104167     0.199532     0.192469     0.036953     0.203781  \n",
       "50%       0.333333     0.269680     0.418410     0.093403     0.425709  \n",
       "75%       0.333333     0.296960     0.719665     0.143278     0.512287  \n",
       "max       0.479167     1.000000     1.000000     0.150759     0.512287  \n",
       "\n",
       "[8 rows x 63 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataPath = '/home/dengruijun/data/FinTech/DATASET/kaggle-dataset/bank/bank-additional-full.csv'\n",
    "\n",
    "[X_train, y_train], [X_test, y_test] = preprocess_bank_dataset(dataPath)\n",
    "df = pd.DataFrame(X_test)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effecEntro:  0.587471032886238\n",
      "I:  tensor([[ 7.5991,  0.0214, -0.1178,  ...,  0.1394, -0.0462,  0.2453],\n",
      "        [ 0.0214,  9.0257, -0.1333,  ..., -0.0265, -0.7587, -0.6064],\n",
      "        [-0.1178, -0.1333,  7.9887,  ...,  0.1769,  0.4307,  0.1134],\n",
      "        ...,\n",
      "        [ 0.1394, -0.0265,  0.1769,  ...,  7.8924,  0.1008, -0.3792],\n",
      "        [-0.0462, -0.7587,  0.4307,  ...,  0.1008,  8.1875, -0.1230],\n",
      "        [ 0.2453, -0.6064,  0.1134,  ..., -0.3792, -0.1230,  7.2750]],\n",
      "       device='cuda:1')\n",
      "det I:  tensor(-inf, device='cuda:1')\n",
      "log det I:  tensor(-575.6401, device='cuda:1')\n",
      "effect_fisher:  tensor(1139.1831, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:  tensor([[ 6.5576e+00,  7.9808e-02, -1.4507e-01,  ...,  3.2295e-02,\n",
      "          4.8863e-02,  1.3413e-01],\n",
      "        [ 7.9808e-02,  7.9728e+00, -1.3367e-01,  ..., -1.0713e-01,\n",
      "         -6.0581e-01, -5.0328e-01],\n",
      "        [-1.4507e-01, -1.3367e-01,  7.2373e+00,  ...,  2.4361e-01,\n",
      "          2.0677e-01,  3.4368e-03],\n",
      "        ...,\n",
      "        [ 3.2295e-02, -1.0713e-01,  2.4361e-01,  ...,  7.0409e+00,\n",
      "          1.0706e-01, -3.2667e-01],\n",
      "        [ 4.8863e-02, -6.0581e-01,  2.0677e-01,  ...,  1.0706e-01,\n",
      "          7.1875e+00, -1.9418e-01],\n",
      "        [ 1.3413e-01, -5.0328e-01,  3.4368e-03,  ..., -3.2667e-01,\n",
      "         -1.9418e-01,  6.4056e+00]], device='cuda:1')\n",
      "det I:  tensor(-inf, device='cuda:1')\n",
      "log det I:  tensor(-674.2421, device='cuda:1')\n",
      "effect_fisher:  tensor(1188.4841, device='cuda:1')\n",
      "Layer 3 effecInfo: -1163.2461471311763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# effect Information 指标计算\n",
    "\n",
    "effecInfo_diff_layer_list = []\n",
    "effecInfo_same_layer_list = []\n",
    "EntropyMetric = ULossMetric()\n",
    "Fishermetric = dFILInverseMetric()\n",
    "\n",
    "# for j, data in enumerate(tqdm.tqdm(testloader)): # 对testloader遍历\n",
    "for j, data in enumerate(tqdm.tqdm(one_data_loader)): # 测试第一个testloader\n",
    "    images, labels = data\n",
    "    images, labels = images.to(args['device']), labels.to(args['device'])\n",
    "    with torch.no_grad():\n",
    "        # inference\n",
    "        outputs = client_net(images).clone().detach()\n",
    "        # entropy \n",
    "        effecEntro= EntropyMetric._entropy_prob_batch(images) # H(x)\n",
    "        print(\"effecEntro: \",effecEntro)\n",
    "\n",
    "        # fisher\n",
    "        outputs = client_net(images)\n",
    "        # effectFisher = Fishermetric._computing_det_with_outputs(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01)\n",
    "        effectFisher = computing_det_with_outputs(model=client_net, inputs=images, outputs=outputs,sigmas = 0.01)\n",
    "\n",
    "        # 存储\n",
    "        effecInfo_same_layer_list.append(effecEntro-effectFisher)\n",
    "        \n",
    "print(f\"Layer {split_layer} effecInfo: {sum(effecInfo_same_layer_list)/len(effecInfo_same_layer_list)}\")\n",
    "effecInfo_diff_layer_list.append(effecInfo_same_layer_list)\n",
    "\n",
    "# 保存到csv中\n",
    "matrix = np.array(effecInfo_diff_layer_list) # 有点大，x\n",
    "transpose = matrix.T # 一行一条数据，一列代表一个layer \n",
    "# pd.DataFrame(data=transpose, columns=[i for i in split_layer_list]).to_csv(results_dir + f'effecInfo-bs{batch_size}.csv',index=False)\n",
    "pd.DataFrame(data=transpose, columns=[split_layer]).to_csv(results_dir + f'effecInfo.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
