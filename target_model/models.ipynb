{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '48'\n",
    "# 导入各个指标\n",
    "import sys\n",
    "sys.path.append('/home/dengruijun/data/FinTech/PP-Split/')\n",
    "# utils\n",
    "from ppsplit.utils.utils import create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入各个baseline模型及其数据集预处理方法\n",
    "# 模型\n",
    "from target_model.models.splitnn_utils import split_weights_client\n",
    "from target_model.models.VGG import VGG,VGG5Decoder,model_cfg\n",
    "from target_model.models.BankNet import BankNet1,BankNetDecoder1,bank_cfg\n",
    "from target_model.models.CreditNet import CreditNet1,CreditNetDecoder1,credit_cfg\n",
    "from target_model.models.PurchaseNet import PurchaseClassifier1,PurchaseDecoder1,purchase_cfg\n",
    "from target_model.models.IrisNet import IrisNet,IrisNetDecoder,Iris_cfg\n",
    "from target_model.models.PyTorch_CIFAR10.cifar10_models.resnet import resnet18,InversionNet,resnet_model_cfg\n",
    "\n",
    "\n",
    "# 数据预处理方法\n",
    "from target_model.data_preprocessing.preprocess_cifar10 import get_cifar10_normalize,deprocess\n",
    "from target_model.data_preprocessing.preprocess_bank import bank_dataset,preprocess_bank,preprocess_bank_dataset,tabinfo_bank\n",
    "from target_model.data_preprocessing.preprocess_credit import preprocess_credit,tabinfo_credit\n",
    "from target_model.data_preprocessing.preprocess_purchase import preprocess_purchase,tabinfo_purchase\n",
    "from target_model.data_preprocessing.preprocess_Iris import preprocess_Iris, tabinfo_Iris\n",
    "from target_model.data_preprocessing.dataset import get_one_data\n",
    "\n",
    "# utils\n",
    "from  ppsplit.utils.utils import create_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "        'device':torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        # 'device':torch.device(\"cpu\"),\n",
    "        'dataset':'CIFAR10',\n",
    "        # 'dataset':'bank',\n",
    "        # 'dataset':'credit',\n",
    "        # 'dataset':'purchase',\n",
    "        # 'dataset':'Iris',\n",
    "        # 'result_dir': '20240702-FIL/',\n",
    "        'result_dir': '20240702-effectiveInfo/',\n",
    "        'oneData_bs': 500,\n",
    "        'test_bs': 1,\n",
    "        'train_bs': 1,\n",
    "        'noise_scale': 0, # 防护措施\n",
    "        'split_layer': 2,\n",
    "        # 'test_num': 'invdFIL', # MI, invdFIL, distCor, ULoss, \n",
    "        # 'test_num': 'effectiveInfo1.1'\n",
    "        # 'test_num': 'effectEntropy1.1'\n",
    "        'test_num': 'effectFisher1.2'\n",
    "        }\n",
    "print(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num layers: 3\n",
      "Split layer: 2\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): GELU(approximate='none')\n",
      "  (maxpool): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU(approximate='none')\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU(approximate='none')\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU(approximate='none')\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU(approximate='none')\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU(approximate='none')\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU(approximate='none')\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU(approximate='none')\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): GELU(approximate='none')\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (layers): ModuleList(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): GELU(approximate='none')\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# resnet18\n",
    "net = resnet18(pretrained=False, split_layer=2, bottleneck_dim=-1, num_classes=10, activation='gelu', pooling='avg')\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InversionNet(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): ConvTranspose2d(128, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# decoder\n",
    "# inet = InversionNet(in_c=64, upconv_channels=[(128, 'same'), (3, 'same')], last_activation=None)\n",
    "inet = InversionNet(last_activation=None)\n",
    "print(inet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['model.conv1.weight', 'model.bn1.weight', 'model.bn1.bias', 'model.bn1.running_mean', 'model.bn1.running_var', 'model.bn1.num_batches_tracked', 'model.layer1.0.conv1.weight', 'model.layer1.0.bn1.weight', 'model.layer1.0.bn1.bias', 'model.layer1.0.bn1.running_mean', 'model.layer1.0.bn1.running_var', 'model.layer1.0.bn1.num_batches_tracked', 'model.layer1.0.conv2.weight', 'model.layer1.0.bn2.weight', 'model.layer1.0.bn2.bias', 'model.layer1.0.bn2.running_mean', 'model.layer1.0.bn2.running_var', 'model.layer1.0.bn2.num_batches_tracked', 'model.layer1.1.conv1.weight', 'model.layer1.1.bn1.weight', 'model.layer1.1.bn1.bias', 'model.layer1.1.bn1.running_mean', 'model.layer1.1.bn1.running_var', 'model.layer1.1.bn1.num_batches_tracked', 'model.layer1.1.conv2.weight', 'model.layer1.1.bn2.weight', 'model.layer1.1.bn2.bias', 'model.layer1.1.bn2.running_mean', 'model.layer1.1.bn2.running_var', 'model.layer1.1.bn2.num_batches_tracked', 'model.layer2.0.conv1.weight', 'model.layer2.0.bn1.weight', 'model.layer2.0.bn1.bias', 'model.layer2.0.bn1.running_mean', 'model.layer2.0.bn1.running_var', 'model.layer2.0.bn1.num_batches_tracked', 'model.layer2.0.conv2.weight', 'model.layer2.0.bn2.weight', 'model.layer2.0.bn2.bias', 'model.layer2.0.bn2.running_mean', 'model.layer2.0.bn2.running_var', 'model.layer2.0.bn2.num_batches_tracked', 'model.layer2.0.downsample.0.weight', 'model.layer2.0.downsample.1.weight', 'model.layer2.0.downsample.1.bias', 'model.layer2.0.downsample.1.running_mean', 'model.layer2.0.downsample.1.running_var', 'model.layer2.0.downsample.1.num_batches_tracked', 'model.layer2.1.conv1.weight', 'model.layer2.1.bn1.weight', 'model.layer2.1.bn1.bias', 'model.layer2.1.bn1.running_mean', 'model.layer2.1.bn1.running_var', 'model.layer2.1.bn1.num_batches_tracked', 'model.layer2.1.conv2.weight', 'model.layer2.1.bn2.weight', 'model.layer2.1.bn2.bias', 'model.layer2.1.bn2.running_mean', 'model.layer2.1.bn2.running_var', 'model.layer2.1.bn2.num_batches_tracked', 'model.layer3.0.conv1.weight', 'model.layer3.0.bn1.weight', 'model.layer3.0.bn1.bias', 'model.layer3.0.bn1.running_mean', 'model.layer3.0.bn1.running_var', 'model.layer3.0.bn1.num_batches_tracked', 'model.layer3.0.conv2.weight', 'model.layer3.0.bn2.weight', 'model.layer3.0.bn2.bias', 'model.layer3.0.bn2.running_mean', 'model.layer3.0.bn2.running_var', 'model.layer3.0.bn2.num_batches_tracked', 'model.layer3.0.downsample.0.weight', 'model.layer3.0.downsample.1.weight', 'model.layer3.0.downsample.1.bias', 'model.layer3.0.downsample.1.running_mean', 'model.layer3.0.downsample.1.running_var', 'model.layer3.0.downsample.1.num_batches_tracked', 'model.layer3.1.conv1.weight', 'model.layer3.1.bn1.weight', 'model.layer3.1.bn1.bias', 'model.layer3.1.bn1.running_mean', 'model.layer3.1.bn1.running_var', 'model.layer3.1.bn1.num_batches_tracked', 'model.layer3.1.conv2.weight', 'model.layer3.1.bn2.weight', 'model.layer3.1.bn2.bias', 'model.layer3.1.bn2.running_mean', 'model.layer3.1.bn2.running_var', 'model.layer3.1.bn2.num_batches_tracked', 'model.layer4.0.conv1.weight', 'model.layer4.0.bn1.weight', 'model.layer4.0.bn1.bias', 'model.layer4.0.bn1.running_mean', 'model.layer4.0.bn1.running_var', 'model.layer4.0.bn1.num_batches_tracked', 'model.layer4.0.conv2.weight', 'model.layer4.0.bn2.weight', 'model.layer4.0.bn2.bias', 'model.layer4.0.bn2.running_mean', 'model.layer4.0.bn2.running_var', 'model.layer4.0.bn2.num_batches_tracked', 'model.layer4.0.downsample.0.weight', 'model.layer4.0.downsample.1.weight', 'model.layer4.0.downsample.1.bias', 'model.layer4.0.downsample.1.running_mean', 'model.layer4.0.downsample.1.running_var', 'model.layer4.0.downsample.1.num_batches_tracked', 'model.layer4.1.conv1.weight', 'model.layer4.1.bn1.weight', 'model.layer4.1.bn1.bias', 'model.layer4.1.bn1.running_mean', 'model.layer4.1.bn1.running_var', 'model.layer4.1.bn1.num_batches_tracked', 'model.layer4.1.conv2.weight', 'model.layer4.1.bn2.weight', 'model.layer4.1.bn2.bias', 'model.layer4.1.bn2.running_mean', 'model.layer4.1.bn2.running_var', 'model.layer4.1.bn2.num_batches_tracked', 'model.fc.weight', 'model.fc.bias', 'model.layers.0.weight', 'model.layers.1.weight', 'model.layers.1.bias', 'model.layers.1.running_mean', 'model.layers.1.running_var', 'model.layers.1.num_batches_tracked', 'model.layers.4.conv1.weight', 'model.layers.4.bn1.weight', 'model.layers.4.bn1.bias', 'model.layers.4.bn1.running_mean', 'model.layers.4.bn1.running_var', 'model.layers.4.bn1.num_batches_tracked', 'model.layers.4.conv2.weight', 'model.layers.4.bn2.weight', 'model.layers.4.bn2.bias', 'model.layers.4.bn2.running_mean', 'model.layers.4.bn2.running_var', 'model.layers.4.bn2.num_batches_tracked', 'model.layers.5.conv1.weight', 'model.layers.5.bn1.weight', 'model.layers.5.bn1.bias', 'model.layers.5.bn1.running_mean', 'model.layers.5.bn1.running_var', 'model.layers.5.bn1.num_batches_tracked', 'model.layers.5.conv2.weight', 'model.layers.5.bn2.weight', 'model.layers.5.bn2.bias', 'model.layers.5.bn2.running_mean', 'model.layers.5.bn2.running_var', 'model.layers.5.bn2.num_batches_tracked', 'model.layers.6.conv1.weight', 'model.layers.6.bn1.weight', 'model.layers.6.bn1.bias', 'model.layers.6.bn1.running_mean', 'model.layers.6.bn1.running_var', 'model.layers.6.bn1.num_batches_tracked', 'model.layers.6.conv2.weight', 'model.layers.6.bn2.weight', 'model.layers.6.bn2.bias', 'model.layers.6.bn2.running_mean', 'model.layers.6.bn2.running_var', 'model.layers.6.bn2.num_batches_tracked', 'model.layers.6.downsample.0.weight', 'model.layers.6.downsample.1.weight', 'model.layers.6.downsample.1.bias', 'model.layers.6.downsample.1.running_mean', 'model.layers.6.downsample.1.running_var', 'model.layers.6.downsample.1.num_batches_tracked', 'model.layers.7.conv1.weight', 'model.layers.7.bn1.weight', 'model.layers.7.bn1.bias', 'model.layers.7.bn1.running_mean', 'model.layers.7.bn1.running_var', 'model.layers.7.bn1.num_batches_tracked', 'model.layers.7.conv2.weight', 'model.layers.7.bn2.weight', 'model.layers.7.bn2.bias', 'model.layers.7.bn2.running_mean', 'model.layers.7.bn2.running_var', 'model.layers.7.bn2.num_batches_tracked', 'model.layers.8.conv1.weight', 'model.layers.8.bn1.weight', 'model.layers.8.bn1.bias', 'model.layers.8.bn1.running_mean', 'model.layers.8.bn1.running_var', 'model.layers.8.bn1.num_batches_tracked', 'model.layers.8.conv2.weight', 'model.layers.8.bn2.weight', 'model.layers.8.bn2.bias', 'model.layers.8.bn2.running_mean', 'model.layers.8.bn2.running_var', 'model.layers.8.bn2.num_batches_tracked', 'model.layers.8.downsample.0.weight', 'model.layers.8.downsample.1.weight', 'model.layers.8.downsample.1.bias', 'model.layers.8.downsample.1.running_mean', 'model.layers.8.downsample.1.running_var', 'model.layers.8.downsample.1.num_batches_tracked', 'model.layers.9.conv1.weight', 'model.layers.9.bn1.weight', 'model.layers.9.bn1.bias', 'model.layers.9.bn1.running_mean', 'model.layers.9.bn1.running_var', 'model.layers.9.bn1.num_batches_tracked', 'model.layers.9.conv2.weight', 'model.layers.9.bn2.weight', 'model.layers.9.bn2.bias', 'model.layers.9.bn2.running_mean', 'model.layers.9.bn2.running_var', 'model.layers.9.bn2.num_batches_tracked', 'model.layers.10.conv1.weight', 'model.layers.10.bn1.weight', 'model.layers.10.bn1.bias', 'model.layers.10.bn1.running_mean', 'model.layers.10.bn1.running_var', 'model.layers.10.bn1.num_batches_tracked', 'model.layers.10.conv2.weight', 'model.layers.10.bn2.weight', 'model.layers.10.bn2.bias', 'model.layers.10.bn2.running_mean', 'model.layers.10.bn2.running_var', 'model.layers.10.bn2.num_batches_tracked', 'model.layers.10.downsample.0.weight', 'model.layers.10.downsample.1.weight', 'model.layers.10.downsample.1.bias', 'model.layers.10.downsample.1.running_mean', 'model.layers.10.downsample.1.running_var', 'model.layers.10.downsample.1.num_batches_tracked', 'model.layers.11.conv1.weight', 'model.layers.11.bn1.weight', 'model.layers.11.bn1.bias', 'model.layers.11.bn1.running_mean', 'model.layers.11.bn1.running_var', 'model.layers.11.bn1.num_batches_tracked', 'model.layers.11.conv2.weight', 'model.layers.11.bn2.weight', 'model.layers.11.bn2.bias', 'model.layers.11.bn2.running_mean', 'model.layers.11.bn2.running_var', 'model.layers.11.bn2.num_batches_tracked', 'model.layers.13.weight', 'model.layers.13.bias'])\n"
     ]
    }
   ],
   "source": [
    "# 加载一下模型参数\n",
    "unit_net_route = '/home/dengruijun/data/FinTech/PP-Split/results/trained_models/CIFAR10-models/ResNet18/32bs-ep20-relu-max-adam/resnet18-drj.pth' # VGG5-BN+Tanh # 存储的是模型参数，不包括模型结构\n",
    "pweights = torch.load(unit_net_route)\n",
    "print(pweights.keys())\n",
    "# state_dict = pweights['state_dict']\n",
    "# torch.save(state_dict, unit_net_route)\n",
    "# client_net = resnet18(pretrained=False, split_layer=split_layer, bottleneck_dim=-1, num_classes=10, activation='gelu', pooling='avg')\n",
    "# client_net.load_state_dict(pweights) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
